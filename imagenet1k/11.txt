






<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <script>
    const GTAG_ENABLED =  true ;
    const GTAG_TRACKING_ID = "UA-121182717-1";
    const SENTRY_DSN_FRONTEND = "https://6284059d22664211881c89b4c409c619@o241170.ingest.sentry.io/5629308".trim();
    const GLOBAL_CSRF_TOKEN = 'm9U78S7eePCMaJv5CeiIGIjtSVB74wogvZQikIsoK1EeQngJlO5YTyJlXelorMdE';
    const MEDIA_URL = "https://production-media.paperswithcode.com/";
    const ASSETS_URL = "https://production-assets.paperswithcode.com";
    run_after_frontend_loaded = window.run_after_frontend_loaded || [];
  </script>
  <link rel="preconnect" href="https://production-assets.paperswithcode.com"><link rel="dns-prefetch" href="https://production-assets.paperswithcode.com"><link rel="preload" as="font" type="font/woff2" href="https://production-assets.paperswithcode.com/perf/fonts/65e877e527022735c1a1.woff2" crossorigin><link rel="preload" as="font" type="font/woff2" href="https://production-assets.paperswithcode.com/perf/fonts/917632e36982ca7933c8.woff2" crossorigin><link rel="preload" as="font" type="font/woff2" href="https://production-assets.paperswithcode.com/perf/fonts/f1405bd8a987c2ea8a67.woff2" crossorigin><script>(()=>{if(GTAG_ENABLED){const t=document.createElement("script");function n(){window.dataLayer.push(arguments)}t.src=`https://www.googletagmanager.com/gtag/js?id=${GTAG_TRACKING_ID}`,document.head.appendChild(t),window.dataLayer=window.dataLayer||[],window.gtag=n,n("js",new Date),n("config",GTAG_TRACKING_ID),window.captureOutboundLink=function(t){n("event","click",{event_category:"outbound",event_label:t})}}else window.captureOutboundLink=function(n){document.location=n}})();</script><link rel="preload" as="script" href="https://production-assets.paperswithcode.com/perf/766.4af6b88b.js"><link rel="preload" as="script" href="https://production-assets.paperswithcode.com/perf/2.6da00df7.js"><link rel="preload" as="script" href="https://production-assets.paperswithcode.com/perf/351.a22a9607.js"><link rel="preload" as="script" href="https://production-assets.paperswithcode.com/perf/452.d3ecdfa4.js"><link rel="preload" as="style" href="https://production-assets.paperswithcode.com/perf/553.4050647d.css"><link rel="preload" as="script" href="https://production-assets.paperswithcode.com/perf/553.357efc0e.js"><link rel="preload" as="style" href="https://production-assets.paperswithcode.com/perf/918.c41196c3.css"><link rel="preload" as="style" href="https://production-assets.paperswithcode.com/perf/sota.table.fe0fcc15.css"><link rel="preload" as="script" href="https://production-assets.paperswithcode.com/perf/sota.table.040f2c99.js"><link rel="stylesheet" href="https://production-assets.paperswithcode.com/perf/553.4050647d.css"><link rel="stylesheet" href="https://production-assets.paperswithcode.com/perf/918.c41196c3.css"><link rel="stylesheet" href="https://production-assets.paperswithcode.com/perf/sota.table.fe0fcc15.css">
  
    




  <!-- Metadata -->
  <title>ImageNet Benchmark (Image Classification) | Papers With Code</title>
  <meta name="description" content="The current state-of-the-art on ImageNet is CoCa (finetuned). See a full comparison of 787 papers with code." />
  


  <!-- Open Graph protocol metadata -->
  <meta property="og:title" content="Papers with Code - ImageNet Benchmark (Image Classification)">
  <meta property="og:description" content="The current state-of-the-art on ImageNet is CoCa (finetuned). See a full comparison of 787 papers with code.">
  
  <meta property="og:image" content="https://production-media.paperswithcode.com/sota-thumbs/image-classification-on-imagenet-large_2040e90d.png">
  
  <meta property="og:url" content="https://paperswithcode.com/sota/image-classification-on-imagenet">
  


  <!-- Twitter metadata -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@paperswithcode">
  <meta name="twitter:title" content="Papers with Code - ImageNet Benchmark (Image Classification)">
  <meta name="twitter:description" content="The current state-of-the-art on ImageNet is CoCa (finetuned). See a full comparison of 787 papers with code.">
  <meta name="twitter:creator" content="@paperswithcode">
  <meta name="twitter:url" content="https://paperswithcode.com/sota/image-classification-on-imagenet">
  <meta name="twitter:domain" content="paperswithcode.com">


<!-- JSON LD -->

<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@graph": {
        "@type": "ItemList",
        "name": "ImageNet Benchmark (Image Classification)",
        "description": "The current state-of-the-art on ImageNet is CoCa (finetuned). See a full comparison of 787 papers with code.",
        "url": "https://paperswithcode.com/sota/image-classification-on-imagenet",
        "image": "https://production-media.paperswithcode.com/sota-thumbs/image-classification-on-imagenet-large_2040e90d.png"
    }
}</script>

  
  <meta name="theme-color" content="#fff"/>
  <link rel="manifest" href="https://production-assets.paperswithcode.com/static/manifest.web.json">

  
    

</head>
<body>




    

<nav class="navbar navbar-expand-lg navbar-light header">
  <a class="navbar-brand" href="/">
    
      <span class=" icon-wrapper" data-name="pwc"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path d="M104 104V56H16v400h88v-48H64V104zm304-48v48h40v304h-40v48h88V56z"/></svg></span>
    
  </a>

  <div class="navbar-mobile-twitter d-lg-none">
    <a rel="noreferrer" href="https://twitter.com/paperswithcode">
      <span class=" icon-wrapper icon-fa icon-fa-brands" data-name="twitter"><svg viewBox="0 0 512.001 515.25" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 152.016c.326 4.548.326 9.097.326 13.645 0 138.72-105.583 298.558-298.559 298.558C101.685 464.22 46.457 447 0 417.114c8.447.973 16.568 1.298 25.34 1.298 49.054 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.113-72.772 6.499.975 12.996 1.624 19.819 1.624 9.42 0 18.843-1.3 27.613-3.573-48.08-9.747-84.142-51.98-84.142-102.984v-1.3c13.968 7.798 30.213 12.67 47.43 13.32-28.263-18.843-46.78-51.006-46.78-87.391 0-19.492 5.196-37.36 14.294-52.954 51.654 63.674 129.3 105.258 216.364 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.827 46.782-104.934 104.934-104.934 30.214 0 57.502 12.67 76.671 33.136 23.715-4.548 46.455-13.319 66.599-25.34-7.798 24.367-24.366 44.834-46.132 57.828 21.117-2.274 41.584-8.122 60.426-16.244-14.292 20.791-32.161 39.309-52.628 54.253z"/></svg></span>
    </a>
  </div>
  <button
    class="navbar-toggler"
    type="button"
    data-toggle="collapse"
    data-bs-toggle="collapse"
    data-target="#top-menu"
    data-bs-target="#top-menu"
    aria-controls="top-menu"
    aria-expanded="false"
    aria-label="Toggle navigation"
  >
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="top-menu">
    <ul class="navbar-nav mr-auto navbar-nav__left light-header">
      <li class="nav-item header-search">
        <form action="/search" method="get" id="id_global_search_form" autocomplete="off">
          <input type="text" name="q_meta" style="display:none" id="q_meta" />
          <input type="hidden" name="q_type" id="q_type" />
          <input id="id_global_search_input" autocomplete="off" value="" name='q' class="global-search" type="search" placeholder='Search'/>
          <button type="submit" class="icon"><span class=" icon-wrapper icon-fa icon-fa-light" data-name="search"><svg viewBox="0 0 512.025 520.146" xmlns="http://www.w3.org/2000/svg"><path d="M508.5 482.6c4.7 4.7 4.7 12.3 0 17l-9.9 9.9c-4.7 4.7-12.3 4.7-17 0l-129-129c-2.2-2.3-3.5-5.3-3.5-8.5v-10.2C312 396 262.5 417 208 417 93.1 417 0 323.9 0 209S93.1 1 208 1s208 93.1 208 208c0 54.5-21 104-55.3 141.1H371c3.2 0 6.2 1.2 8.5 3.5zM208 385c97.3 0 176-78.7 176-176S305.3 33 208 33 32 111.7 32 209s78.7 176 176 176z"/></svg></span></button>
        </form>
      </li>

      
        
        
        
        <li class="nav-item">
          <a class="nav-link" href="/sota">
            Browse State-of-the-Art
          </a>
        </li>

        
          <li class="nav-item">
            <a class="nav-link" href="/datasets"> Datasets </a>
          </li>
        

        
            <li class="nav-item">
              <a class="nav-link" href="/methods">Methods</a>
            </li>
        

        <li class="nav-item dropdown">
          <a
            class="nav-link dropdown-toggle"
            role="button"
            id="navbarDropdownRepro"
            data-toggle="dropdown"
            data-bs-toggle="dropdown"
            aria-haspopup="true"
            aria-expanded="false"
          >
            More
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownRepro">
        
            <a class="dropdown-item" href="/newsletter">Newsletter</a>
            <a class="dropdown-item" href="/rc2022">RC2022</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="/about">About</a>
            <a class="dropdown-item" href="/trends">Trends</a>
              
                  <a class="dropdown-item" href="https://portal.paperswithcode.com/">
                      Portals
                  </a>
              
              
                  <a class="dropdown-item" href="/libraries"> Libraries </a>
              
          </div>
        </li>

          



      
    </ul>

    <ul class="navbar-nav ml-auto navbar-nav__right navbar-subscribe justify-content-center align-items-center">
      

      <li class="nav-item">
        <a class="nav-link" rel="noreferrer" href="https://twitter.com/paperswithcode">
          <span class="nav-link-social-icon icon-wrapper icon-fa icon-fa-brands" data-name="twitter"><svg viewBox="0 0 512.001 515.25" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 152.016c.326 4.548.326 9.097.326 13.645 0 138.72-105.583 298.558-298.559 298.558C101.685 464.22 46.457 447 0 417.114c8.447.973 16.568 1.298 25.34 1.298 49.054 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.113-72.772 6.499.975 12.996 1.624 19.819 1.624 9.42 0 18.843-1.3 27.613-3.573-48.08-9.747-84.142-51.98-84.142-102.984v-1.3c13.968 7.798 30.213 12.67 47.43 13.32-28.263-18.843-46.78-51.006-46.78-87.391 0-19.492 5.196-37.36 14.294-52.954 51.654 63.674 129.3 105.258 216.364 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.827 46.782-104.934 104.934-104.934 30.214 0 57.502 12.67 76.671 33.136 23.715-4.548 46.455-13.319 66.599-25.34-7.798 24.367-24.366 44.834-46.132 57.828 21.117-2.274 41.584-8.122 60.426-16.244-14.292 20.791-32.161 39.309-52.628 54.253z"/></svg></span>
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" rel="noreferrer" href="https://join.slack.com/t/paperswithcode/shared_invite/zt-18dc477yc-IQNl9TIJo35XxWuw0b4m_w">
          <span class="nav-link-social-icon nav-link-social-icon-slack icon-wrapper" data-name="slack"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 270 270"><path d="M99.4 151.2c0 7.1-5.8 12.9-12.9 12.9-7.1 0-12.9-5.8-12.9-12.9 0-7.1 5.8-12.9 12.9-12.9h12.9v12.9zm6.5 0c0-7.1 5.8-12.9 12.9-12.9s12.9 5.8 12.9 12.9v32.3c0 7.1-5.8 12.9-12.9 12.9s-12.9-5.8-12.9-12.9v-32.3z" fill="#e01e5a"/><path d="M118.8 99.4c-7.1 0-12.9-5.8-12.9-12.9 0-7.1 5.8-12.9 12.9-12.9s12.9 5.8 12.9 12.9v12.9h-12.9zm0 6.5c7.1 0 12.9 5.8 12.9 12.9s-5.8 12.9-12.9 12.9H86.5c-7.1 0-12.9-5.8-12.9-12.9s5.8-12.9 12.9-12.9h32.3z" fill="#36c5f0"/><path d="M170.6 118.8c0-7.1 5.8-12.9 12.9-12.9 7.1 0 12.9 5.8 12.9 12.9s-5.8 12.9-12.9 12.9h-12.9v-12.9zm-6.5 0c0 7.1-5.8 12.9-12.9 12.9-7.1 0-12.9-5.8-12.9-12.9V86.5c0-7.1 5.8-12.9 12.9-12.9 7.1 0 12.9 5.8 12.9 12.9v32.3z" fill="#2eb67d"/><path d="M151.2 170.6c7.1 0 12.9 5.8 12.9 12.9 0 7.1-5.8 12.9-12.9 12.9-7.1 0-12.9-5.8-12.9-12.9v-12.9h12.9zm0-6.5c-7.1 0-12.9-5.8-12.9-12.9 0-7.1 5.8-12.9 12.9-12.9h32.3c7.1 0 12.9 5.8 12.9 12.9 0 7.1-5.8 12.9-12.9 12.9h-32.3z" fill="#ecb22e"/></svg></span>
        </a>
      </li>

      
        <li class="nav-item">
          <a id="signin-link" class="nav-link" href="/accounts/login?next=/sota/image-classification-on-imagenet">Sign In</a>
        </li>
      
    </ul>
  </div>
</nav>





<!-- Page modals -->
<div class="modal fade" id="emailModal" tabindex="-1" role="dialog" aria-labelledby="emailModalLabel" aria-hidden="true">
  <div class="modal-dialog" role="document">
    <div class="modal-content">
      <div class="modal-header">
        <h3 class="modal-title" id="emailModalLabel">Subscribe to the PwC Newsletter</h3>
        <button type="button" class="close" data-dismiss="modal" data-bs-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <form action="" method="post">
        <div class="modal-body">
          <div class="modal-body-info-text">
            Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.<br/><br/>
            <a href="/newsletter">Read previous issues</a>
          </div>

          <input type="hidden" name="csrfmiddlewaretoken" value="m9U78S7eePCMaJv5CeiIGIjtSVB74wogvZQikIsoK1EeQngJlO5YTyJlXelorMdE">
          <input placeholder="Enter your email" type="email" class="form-control pwc-email" name="address" id="id_address" max_length="100" required>
        </div>
        <div class="modal-footer">
          <button type="submit" class="btn btn-primary">Subscribe</button>
        </div>
      </form>
    </div>
  </div>
</div>

<!-- Login -->
<div class="modal fade" id="loginModal" tabindex="-1" role="dialog" aria-labelledby="loginModalLabel" aria-hidden="true">
  <div class="modal-dialog" role="document">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title" id="loginModalLabel">Join the community</h5>
        <button type="button" class="close btn-close" data-dismiss="modal" data-bs-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="login-modal-message">
        You need to <a href="/accounts/login?next=/sota/image-classification-on-imagenet">log in</a> to edit.<br/>
        You can <a href="/accounts/register?next=/sota/image-classification-on-imagenet">create a new account</a> if you don't have one.<br/><br/>
        Or, discuss a change on <a href="https://join.slack.com/t/paperswithcode/shared_invite/zt-18dc477yc-IQNl9TIJo35XxWuw0b4m_w"><i class="fab fa-slack"></i> Slack</a>.
      </div>
    </div>
  </div>
</div>






<div class="container content content-buffer ">

  
    
    
    
    
    


    <div class="leaderboard-header">
        <a href="/task/image-classification">
                    <span class="badge badge-primary">
                         
                             <img src="https://production-media.paperswithcode.com/thumbnails/task/task-0000000951-52325f45_O0tAMly.jpg">
                         
                    <span>Image Classification</span>
                    </span>
        </a>
    </div>

    <div id="sota-page">
        <div class="text-center">
            <img src="https://production-assets.paperswithcode.com/perf/images/spin-1s-32px-ed14c515.gif">
        </div>
    </div>

    <link href="https://production-assets.paperswithcode.com/static/fonts/font-awesome/css/all.min.css" rel="stylesheet" />

    <script type="application/javascript">
        const CSRF_TOKEN = "m9U78S7eePCMaJv5CeiIGIjtSVB74wogvZQikIsoK1EeQngJlO5YTyJlXelorMdE";
        const USER_IS_AUTHENTICATED = false;
        const LOGIN_REQUIRED = true;

    </script>

    <script
  type="module"
  src="https://unpkg.com/ionicons@5.1.2/dist/ionicons/ionicons.esm.js"
></script>
<script
  nomodule=""
  src="https://unpkg.com/ionicons@5.1.2/dist/ionicons/ionicons.js"
></script>
    
    

    <!-- Start SOTA Table Generation -->
    <script id="evaluation-chart-data" type="application/json">{"all": {"yAxis": {"title": "Top 1 Accuracy", "includeZero": false, "gridColor": "#ddd", "valueFormatString": "# %", "minimum": 0.14704, "maximum": 0.9793600000000001}, "data": {"trend": {"name": "State-of-the-art methods", "type": "line", "showInLegend": true, "markerSize": 10, "toolTipContent": "{name}: {y}", "color": "#21ccc7", "dataPoints": [{"x": "2012-12-01", "y": 0.633, "name": "AlexNet", "nameShort": "AlexNet", "nameDetails": null, "paperSlug": "imagenet-classification-with-deep", "usesAdditionalData": false}, {"x": "2013-11-12", "y": 0.64, "name": "ZFNet ", "nameShort": "ZFNet ", "nameDetails": "ensemble, 6 convnets", "paperSlug": "visualizing-and-understanding-convolutional", "usesAdditionalData": false}, {"x": "2013-12-19", "y": 0.6629999999999999, "name": "Five Base + Five HiRes", "nameShort": "Five Base + Five HiRes", "nameDetails": null, "paperSlug": "some-improvements-on-deep-convolutional", "usesAdditionalData": false}, {"x": "2014-06-18", "y": 0.7031999999999999, "name": "MSRA", "nameShort": "MSRA", "nameDetails": null, "paperSlug": "spatial-pyramid-pooling-in-deep-convolutional", "usesAdditionalData": false}, {"x": "2014-09-04", "y": 0.745, "name": "VGG-19", "nameShort": "VGG-19", "nameDetails": null, "paperSlug": "very-deep-convolutional-networks-for-large", "usesAdditionalData": false}, {"x": "2015-02-11", "y": 0.748, "name": "Inception V2", "nameShort": "Inception V2", "nameDetails": null, "paperSlug": "batch-normalization-accelerating-deep-network", "usesAdditionalData": false}, {"x": "2015-12-02", "y": 0.7879999999999999, "name": "Inception V3", "nameShort": "Inception V3", "nameDetails": null, "paperSlug": "rethinking-the-inception-architecture-for", "usesAdditionalData": false}, {"x": "2016-02-23", "y": 0.8009999999999999, "name": "Inception ResNet V2", "nameShort": "Inception ResNet V2", "nameDetails": null, "paperSlug": "inception-v4-inception-resnet-and-the-impact", "usesAdditionalData": false}, {"x": "2016-11-16", "y": 0.809, "name": "ResNeXt-101  64x4", "nameShort": "ResNeXt-101  64x4", "nameDetails": null, "paperSlug": "aggregated-residual-transformations-for-deep", "usesAdditionalData": false}, {"x": "2017-07-21", "y": 0.8270000000000001, "name": "NASNET-A", "nameShort": "NASNET-A", "nameDetails": "6", "paperSlug": "learning-transferable-architectures-for", "usesAdditionalData": false}, {"x": "2017-12-02", "y": 0.8290000000000001, "name": "PNASNet-5", "nameShort": "PNASNet-5", "nameDetails": null, "paperSlug": "progressive-neural-architecture-search", "usesAdditionalData": false}, {"x": "2018-02-05", "y": 0.8390000000000001, "name": "AmoebaNet-A", "nameShort": "AmoebaNet-A", "nameDetails": null, "paperSlug": "regularized-evolution-for-image-classifier", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.8540000000000001, "name": "ResNeXt-101 32x48d", "nameShort": "ResNeXt-101 32x48d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.8640000000000001, "name": "FixResNeXt-101 32x48d", "nameShort": "FixResNeXt-101 32x48d", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.8690000000000001, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B7", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.8754000000000001, "name": "BiT-L ", "nameShort": "BiT-L ", "nameDetails": "ResNet", "paperSlug": "large-scale-learning-of-general-visual", "usesAdditionalData": false}, {"x": "2020-01-07", "y": 0.884, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-L2", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.885, "name": "FixEfficientNet-L2", "nameShort": "FixEfficientNet-L2", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-23", "y": 0.902, "name": "Meta Pseudo Labels ", "nameShort": "Meta Pseudo Labels ", "nameDetails": "EfficientNet-L2", "paperSlug": "meta-pseudo-labels", "usesAdditionalData": false}, {"x": "2021-06-08", "y": 0.9045000000000001, "name": "ViT-G/14", "nameShort": "ViT-G/14", "nameDetails": null, "paperSlug": "scaling-vision-transformers", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.9087999999999999, "name": "CoAtNet-7", "nameShort": "CoAtNet-7", "nameDetails": null, "paperSlug": "coatnet-marrying-convolution-and-attention", "usesAdditionalData": false}, {"x": "2022-03-10", "y": 0.9098, "name": "Model soups ", "nameShort": "Model soups ", "nameDetails": "BASIC-L", "paperSlug": "model-soups-averaging-weights-of-multiple", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.91, "name": "CoCa ", "nameShort": "CoCa ", "nameDetails": "finetuned", "paperSlug": "coca-contrastive-captioners-are-image-text", "usesAdditionalData": false}], "yValueFormatString": "#.# %"}, "other": {"name": "Other methods", "type": "scatter", "showInLegend": true, "color": "#ddd", "markerSize": 10, "toolTipContent": "{name}: {y}", "dataPoints": [{"x": "2013-11-12", "y": 0.625, "name": "ZFNet ", "nameShort": "ZFNet ", "nameDetails": "1 convnet, 512,1024,512 maps", "paperSlug": "visualizing-and-understanding-convolutional", "usesAdditionalData": false}, {"x": "2013-12-21", "y": 0.6604000000000001, "name": "OverFeat - 7 accurate models", "nameShort": "OverFeat - 7 accurate models", "nameDetails": null, "paperSlug": "overfeat-integrated-recognition-localization", "usesAdditionalData": false}, {"x": "2014-09-04", "y": 0.7440000000000001, "name": "VGG-16", "nameShort": "VGG-16", "nameDetails": null, "paperSlug": "very-deep-convolutional-networks-for-large", "usesAdditionalData": false}, {"x": "2015-10-31", "y": 0.6829999999999999, "name": "FireCaffe ", "nameShort": "FireCaffe ", "nameDetails": "GoogLeNet", "paperSlug": "firecaffe-near-linear-acceleration-of-deep", "usesAdditionalData": false}, {"x": "2015-10-31", "y": 0.589, "name": "FireCaffe ", "nameShort": "FireCaffe ", "nameDetails": "AlexNet", "paperSlug": "firecaffe-near-linear-acceleration-of-deep", "usesAdditionalData": false}, {"x": "2015-12-10", "y": 0.7857, "name": "ResNet-152", "nameShort": "ResNet-152", "nameDetails": null, "paperSlug": "deep-residual-learning-for-image-recognition", "usesAdditionalData": false}, {"x": "2015-12-10", "y": 0.7825, "name": "ResNet-101", "nameShort": "ResNet-101", "nameDetails": null, "paperSlug": "deep-residual-learning-for-image-recognition", "usesAdditionalData": false}, {"x": "2015-12-10", "y": 0.753, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "deep-residual-learning-for-image-recognition", "usesAdditionalData": false}, {"x": "2016-02-24", "y": 0.604, "name": "SqueezeNet + Simple Bypass", "nameShort": "SqueezeNet + Simple Bypass", "nameDetails": null, "paperSlug": "squeezenet-alexnet-level-accuracy-with-50x", "usesAdditionalData": false}, {"x": "2016-03-16", "y": 0.799, "name": "ResNet-200", "nameShort": "ResNet-200", "nameDetails": null, "paperSlug": "identity-mappings-in-deep-residual-networks", "usesAdditionalData": false}, {"x": "2016-05-23", "y": 0.7809999999999999, "name": "WRN-50-2-bottleneck", "nameShort": "WRN-50-2-bottleneck", "nameDetails": null, "paperSlug": "wide-residual-networks", "usesAdditionalData": false}, {"x": "2016-05-24", "y": 0.7587999999999999, "name": "FractalNet-34", "nameShort": "FractalNet-34", "nameDetails": null, "paperSlug": "fractalnet-ultra-deep-neural-networks-without", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.7785, "name": "DenseNet-264", "nameShort": "DenseNet-264", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.7742, "name": "DenseNet-201", "nameShort": "DenseNet-201", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.762, "name": "DenseNet-169", "nameShort": "DenseNet-169", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.7498, "name": "DenseNet-121", "nameShort": "DenseNet-121", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-10-07", "y": 0.79, "name": "Xception", "nameShort": "Xception", "nameDetails": null, "paperSlug": "xception-deep-learning-with-depthwise", "usesAdditionalData": false}, {"x": "2017-04-17", "y": 0.706, "name": "MobileNet-224 \u00d71.0", "nameShort": "MobileNet-224 \u00d71.0", "nameDetails": null, "paperSlug": "mobilenets-efficient-convolutional-neural", "usesAdditionalData": false}, {"x": "2017-04-23", "y": 0.805, "name": "Attention-92", "nameShort": "Attention-92", "nameDetails": null, "paperSlug": "residual-attention-network-for-image", "usesAdditionalData": false}, {"x": "2017-07-04", "y": 0.7090000000000001, "name": "ShuffleNet", "nameShort": "ShuffleNet", "nameDetails": null, "paperSlug": "shufflenet-an-extremely-efficient", "usesAdditionalData": false}, {"x": "2017-07-10", "y": 0.792, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "JFT-300M Finetuning", "paperSlug": "revisiting-unreasonable-effectiveness-of-data", "usesAdditionalData": false}, {"x": "2018-01-13", "y": 0.747, "name": "MobileNetV2 ", "nameShort": "MobileNetV2 ", "nameDetails": "1.4", "paperSlug": "mobilenetv2-inverted-residuals-and-linear", "usesAdditionalData": false}, {"x": "2018-01-13", "y": 0.72, "name": "MobileNetV2", "nameShort": "MobileNetV2", "nameDetails": null, "paperSlug": "mobilenetv2-inverted-residuals-and-linear", "usesAdditionalData": false}, {"x": "2018-03-14", "y": 0.7894, "name": "ResNet-152 + SWA", "nameShort": "ResNet-152 + SWA", "nameDetails": null, "paperSlug": "averaging-weights-leads-to-wider-optima-and", "usesAdditionalData": false}, {"x": "2018-03-14", "y": 0.7844, "name": "DenseNet-161 + SWA", "nameShort": "DenseNet-161 + SWA", "nameDetails": null, "paperSlug": "averaging-weights-leads-to-wider-optima-and", "usesAdditionalData": false}, {"x": "2018-03-22", "y": 0.7712, "name": "Inception v3", "nameShort": "Inception v3", "nameDetails": null, "paperSlug": "what-do-deep-networks-like-to-see", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.851, "name": "ResNeXt-101 32x32d", "nameShort": "ResNeXt-101 32x32d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.8420000000000001, "name": "ResNeXt-101 32\u00d716d", "nameShort": "ResNeXt-101 32\u00d716d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.8220000000000001, "name": "ResNeXt-101 32x8d", "nameShort": "ResNeXt-101 32x8d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2018-07-09", "y": 0.7574, "name": "CoordConv ResNet-50", "nameShort": "CoordConv ResNet-50", "nameDetails": null, "paperSlug": "an-intriguing-failing-of-convolutional-neural", "usesAdditionalData": false}, {"x": "2018-07-30", "y": 0.754, "name": "ShuffleNet V2", "nameShort": "ShuffleNet V2", "nameDetails": null, "paperSlug": "shufflenet-v2-practical-guidelines-for", "usesAdditionalData": false}, {"x": "2018-07-31", "y": 0.767, "name": "MnasNet-A3", "nameShort": "MnasNet-A3", "nameDetails": null, "paperSlug": "mnasnet-platform-aware-neural-architecture", "usesAdditionalData": false}, {"x": "2018-07-31", "y": 0.7559999999999999, "name": "MnasNet-A2", "nameShort": "MnasNet-A2", "nameDetails": null, "paperSlug": "mnasnet-platform-aware-neural-architecture", "usesAdditionalData": false}, {"x": "2018-07-31", "y": 0.752, "name": "MnasNet-A1", "nameShort": "MnasNet-A1", "nameDetails": null, "paperSlug": "mnasnet-platform-aware-neural-architecture", "usesAdditionalData": false}, {"x": "2018-10-30", "y": 0.7835, "name": "ResNet-50 + DropBlock ", "nameShort": "ResNet-50 + DropBlock ", "nameDetails": "0.9 kp, 0.1 label smoothing", "paperSlug": "dropblock-a-regularization-method-for", "usesAdditionalData": false}, {"x": "2018-11-16", "y": 0.8440000000000001, "name": "GPIPE", "nameShort": "GPIPE", "nameDetails": null, "paperSlug": "gpipe-efficient-training-of-giant-neural", "usesAdditionalData": false}, {"x": "2018-11-28", "y": 0.7490000000000001, "name": "ESPNetv2", "nameShort": "ESPNetv2", "nameDetails": null, "paperSlug": "espnetv2-a-light-weight-power-efficient-and", "usesAdditionalData": false}, {"x": "2018-12-02", "y": 0.746, "name": "Proxyless", "nameShort": "Proxyless", "nameDetails": null, "paperSlug": "proxylessnas-direct-neural-architecture", "usesAdditionalData": false}, {"x": "2018-12-04", "y": 0.7716, "name": "ResNet-50-D", "nameShort": "ResNet-50-D", "nameDetails": null, "paperSlug": "bag-of-tricks-for-image-classification-with", "usesAdditionalData": false}, {"x": "2018-12-09", "y": 0.7490000000000001, "name": "FBNet-C", "nameShort": "FBNet-C", "nameDetails": null, "paperSlug": "fbnet-hardware-aware-efficient-convnet-design", "usesAdditionalData": false}, {"x": "2019-02-01", "y": 0.8432, "name": "ColorNet ", "nameShort": "ColorNet ", "nameDetails": "RHYLH with Conv Layer", "paperSlug": "colornet-investigating-the-importance-of", "usesAdditionalData": false}, {"x": "2019-02-01", "y": 0.8234999999999999, "name": "ColorNet", "nameShort": "ColorNet", "nameDetails": null, "paperSlug": "colornet-investigating-the-importance-of", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.836, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "500px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.8320000000000001, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "450px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.831, "name": "MultiGrain SENet154 ", "nameShort": "MultiGrain SENet154 ", "nameDetails": "450px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.83, "name": "MultiGrain SENet154 ", "nameShort": "MultiGrain SENet154 ", "nameDetails": "400px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.8270000000000001, "name": "MultiGrain SENet154 ", "nameShort": "MultiGrain SENet154 ", "nameDetails": "500px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.826, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "400px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.813, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "300px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.794, "name": "MultiGrain R50-AA-500", "nameShort": "MultiGrain R50-AA-500", "nameDetails": null, "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.782, "name": "MultiGrain R50-AA-224", "nameShort": "MultiGrain R50-AA-224", "nameDetails": null, "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.7509999999999999, "name": "MultiGrain NASNet-A-Mobile ", "nameShort": "MultiGrain NASNet-A-Mobile ", "nameDetails": "350px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.6829000000000001, "name": "Graph-RISE ", "nameShort": "Graph-RISE ", "nameDetails": "40M", "paperSlug": "graph-rise-graph-regularized-image-semantic", "usesAdditionalData": false}, {"x": "2019-03-15", "y": 0.7981, "name": "SKNet-101", "nameShort": "SKNet-101", "nameDetails": null, "paperSlug": "selective-kernel-networks", "usesAdditionalData": false}, {"x": "2019-03-26", "y": 0.7847, "name": "SRM-ResNet-101", "nameShort": "SRM-ResNet-101", "nameDetails": null, "paperSlug": "srm-a-style-based-recalibration-module-for", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.8123, "name": "Res2Net-101", "nameShort": "Res2Net-101", "nameDetails": null, "paperSlug": "res2net-a-new-multi-scale-backbone", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.8009999999999999, "name": "RandWire-WS", "nameShort": "RandWire-WS", "nameDetails": null, "paperSlug": "exploring-randomly-wired-neural-networks-for", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.7859, "name": "Res2Net-50-299", "nameShort": "Res2Net-50-299", "nameDetails": null, "paperSlug": "res2net-a-new-multi-scale-backbone", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.747, "name": "RandWire-WS ", "nameShort": "RandWire-WS ", "nameDetails": "small", "paperSlug": "exploring-randomly-wired-neural-networks-for", "usesAdditionalData": false}, {"x": "2019-04-05", "y": 0.7495999999999999, "name": "Single-Path NAS", "nameShort": "Single-Path NAS", "nameDetails": null, "paperSlug": "single-path-nas-designing-hardware-efficient", "usesAdditionalData": false}, {"x": "2019-04-07", "y": 0.775, "name": "ACNet ", "nameShort": "ACNet ", "nameDetails": "ResNet-50", "paperSlug": "adaptively-connected-neural-networks", "usesAdditionalData": false}, {"x": "2019-04-10", "y": 0.8290000000000001, "name": "Oct-ResNet-152 ", "nameShort": "Oct-ResNet-152 ", "nameDetails": "SE", "paperSlug": "drop-an-octave-reducing-spatial-redundancy-in", "usesAdditionalData": false}, {"x": "2019-04-10", "y": 0.7829999999999999, "name": "EfficientNet-B0 ", "nameShort": "EfficientNet-B0 ", "nameDetails": "CondConv", "paperSlug": "soft-conditional-computation", "usesAdditionalData": false}, {"x": "2019-04-20", "y": 0.7938, "name": "ScaleNet-152", "nameShort": "ScaleNet-152", "nameDetails": null, "paperSlug": "190409460", "usesAdditionalData": false}, {"x": "2019-04-20", "y": 0.7903, "name": "ScaleNet-101", "nameShort": "ScaleNet-101", "nameDetails": null, "paperSlug": "190409460", "usesAdditionalData": false}, {"x": "2019-04-20", "y": 0.778, "name": "ScaleNet-50", "nameShort": "ScaleNet-50", "nameDetails": null, "paperSlug": "190409460", "usesAdditionalData": false}, {"x": "2019-04-22", "y": 0.7909999999999999, "name": "AA-ResNet-152", "nameShort": "AA-ResNet-152", "nameDetails": null, "paperSlug": "190409925", "usesAdditionalData": false}, {"x": "2019-04-25", "y": 0.757, "name": "LR-Net-26", "nameShort": "LR-Net-26", "nameDetails": null, "paperSlug": "190411491", "usesAdditionalData": false}, {"x": "2019-04-29", "y": 0.7904000000000001, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "UDA", "paperSlug": "unsupervised-data-augmentation-1", "usesAdditionalData": false}, {"x": "2019-05-01", "y": 0.8059999999999999, "name": "ResNet-200 ", "nameShort": "ResNet-200 ", "nameDetails": "Fast AA", "paperSlug": "fast-autoaugment", "usesAdditionalData": false}, {"x": "2019-05-01", "y": 0.7759999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "Fast AA", "paperSlug": "fast-autoaugment", "usesAdditionalData": false}, {"x": "2019-05-02", "y": 0.848, "name": "ResNeXt-101 32x16d ", "nameShort": "ResNeXt-101 32x16d ", "nameDetails": "semi-weakly sup.", "paperSlug": "billion-scale-semi-supervised-learning-for", "usesAdditionalData": false}, {"x": "2019-05-02", "y": 0.843, "name": "ResNeXt-101 32x8d ", "nameShort": "ResNeXt-101 32x8d ", "nameDetails": "semi-weakly sup.", "paperSlug": "billion-scale-semi-supervised-learning-for", "usesAdditionalData": false}, {"x": "2019-05-02", "y": 0.8340000000000001, "name": "ResNeXt-101 32x4d ", "nameShort": "ResNeXt-101 32x4d ", "nameDetails": "semi-weakly sup.", "paperSlug": "billion-scale-semi-supervised-learning-for", "usesAdditionalData": false}, {"x": "2019-05-06", "y": 0.752, "name": "MobileNet V3-Large 1.0", "nameShort": "MobileNet V3-Large 1.0", "nameDetails": null, "paperSlug": "searching-for-mobilenetv3", "usesAdditionalData": false}, {"x": "2019-05-13", "y": 0.8053, "name": "ResNeXt-101 ", "nameShort": "ResNeXt-101 ", "nameDetails": "CutMix", "paperSlug": "cutmix-regularization-strategy-to-train", "usesAdditionalData": false}, {"x": "2019-05-13", "y": 0.784, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "CutMix", "paperSlug": "cutmix-regularization-strategy-to-train", "usesAdditionalData": false}, {"x": "2019-05-23", "y": 0.78798, "name": "SGE-ResNet101", "nameShort": "SGE-ResNet101", "nameDetails": null, "paperSlug": "spatial-group-wise-enhance-improving-semantic", "usesAdditionalData": false}, {"x": "2019-05-23", "y": 0.7758400000000001, "name": "SGE-ResNet50", "nameShort": "SGE-ResNet50", "nameDetails": null, "paperSlug": "spatial-group-wise-enhance-improving-semantic", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.8440000000000001, "name": "EfficientNet-B7", "nameShort": "EfficientNet-B7", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.84, "name": "EfficientNet-B6", "nameShort": "EfficientNet-B6", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.833, "name": "EfficientNet-B5", "nameShort": "EfficientNet-B5", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.826, "name": "EfficientNet-B4", "nameShort": "EfficientNet-B4", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.8109999999999999, "name": "EfficientNet-B3", "nameShort": "EfficientNet-B3", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.7979999999999999, "name": "EfficientNet-B2", "nameShort": "EfficientNet-B2", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.7879999999999999, "name": "EfficientNet-B1", "nameShort": "EfficientNet-B1", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.763, "name": "EfficientNet-B0", "nameShort": "EfficientNet-B0", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-06-08", "y": 0.7509999999999999, "name": "DiCENet", "nameShort": "DiCENet", "nameDetails": null, "paperSlug": "dicenet-dimension-wise-convolutions-for", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.8370000000000001, "name": "FixPNASNet-5", "nameShort": "FixPNASNet-5", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.825, "name": "FixResNet-50 Billion-scale@224", "nameShort": "FixResNet-50 Billion-scale@224", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.7979999999999999, "name": "FixResNet-50 CutMix", "nameShort": "FixResNet-50 CutMix", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.7909999999999999, "name": "FixResNet-50", "nameShort": "FixResNet-50", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-23", "y": 0.759, "name": "DenseNAS-A", "nameShort": "DenseNAS-A", "nameDetails": null, "paperSlug": "densely-connected-search-space-for-more", "usesAdditionalData": false}, {"x": "2019-07-03", "y": 0.7534000000000001, "name": "FairNAS-A", "nameShort": "FairNAS-A", "nameDetails": null, "paperSlug": "fairnas-rethinking-evaluation-fairness-of", "usesAdditionalData": false}, {"x": "2019-07-03", "y": 0.7509999999999999, "name": "FairNAS-B", "nameShort": "FairNAS-B", "nameDetails": null, "paperSlug": "fairnas-rethinking-evaluation-fairness-of", "usesAdditionalData": false}, {"x": "2019-07-03", "y": 0.7469, "name": "FairNAS-C", "nameShort": "FairNAS-C", "nameDetails": null, "paperSlug": "fairnas-rethinking-evaluation-fairness-of", "usesAdditionalData": false}, {"x": "2019-07-22", "y": 0.789, "name": "MixNet-L", "nameShort": "MixNet-L", "nameDetails": null, "paperSlug": "mixnet-mixed-depthwise-convolutional-kernels", "usesAdditionalData": false}, {"x": "2019-07-22", "y": 0.77, "name": "MixNet-M", "nameShort": "MixNet-M", "nameDetails": null, "paperSlug": "mixnet-mixed-depthwise-convolutional-kernels", "usesAdditionalData": false}, {"x": "2019-07-22", "y": 0.758, "name": "MixNet-S", "nameShort": "MixNet-S", "nameDetails": null, "paperSlug": "mixnet-mixed-depthwise-convolutional-kernels", "usesAdditionalData": false}, {"x": "2019-07-23", "y": 0.7256, "name": "MobileNet-224 ", "nameShort": "MobileNet-224 ", "nameDetails": "CGD", "paperSlug": "compact-global-descriptor-for-neural-networks", "usesAdditionalData": false}, {"x": "2019-08-04", "y": 0.8187000000000001, "name": "AOGNet-40M-AN", "nameShort": "AOGNet-40M-AN", "nameDetails": null, "paperSlug": "attentive-normalization", "usesAdditionalData": false}, {"x": "2019-08-04", "y": 0.759, "name": "MoGA-A", "nameShort": "MoGA-A", "nameDetails": null, "paperSlug": "moga-searching-beyond-mobilenetv3", "usesAdditionalData": false}, {"x": "2019-08-12", "y": 0.7933, "name": "LIP-ResNet-101", "nameShort": "LIP-ResNet-101", "nameDetails": null, "paperSlug": "lip-local-importance-based-pooling", "usesAdditionalData": false}, {"x": "2019-08-12", "y": 0.7815000000000001, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "LIP Bottleneck-256", "paperSlug": "lip-local-importance-based-pooling", "usesAdditionalData": false}, {"x": "2019-08-12", "y": 0.7664, "name": "LIP-DenseNet-BC-121", "nameShort": "LIP-DenseNet-BC-121", "nameDetails": null, "paperSlug": "lip-local-importance-based-pooling", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.823, "name": "SCARLET-A4", "nameShort": "SCARLET-A4", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.769, "name": "SCARLET-A", "nameShort": "SCARLET-A", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.763, "name": "SCARLET-B", "nameShort": "SCARLET-B", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.7559999999999999, "name": "SCARLET-C", "nameShort": "SCARLET-C", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-23", "y": 0.7979999999999999, "name": "CSPResNeXt-50 + Mish", "nameShort": "CSPResNeXt-50 + Mish", "nameDetails": null, "paperSlug": "mish-a-self-regularized-non-monotonic-neural", "usesAdditionalData": false}, {"x": "2019-08-26", "y": 0.805, "name": "HCGNet-C", "nameShort": "HCGNet-C", "nameDetails": null, "paperSlug": "gated-convolutional-networks-with-hybrid", "usesAdditionalData": false}, {"x": "2019-08-26", "y": 0.785, "name": "HCGNet-B", "nameShort": "HCGNet-B", "nameDetails": null, "paperSlug": "gated-convolutional-networks-with-hybrid", "usesAdditionalData": false}, {"x": "2019-09-26", "y": 0.626, "name": "BBG ", "nameShort": "BBG ", "nameDetails": "ResNet-34", "paperSlug": "balanced-binary-neural-networks-with-gated", "usesAdditionalData": false}, {"x": "2019-09-26", "y": 0.594, "name": "BBG ", "nameShort": "BBG ", "nameDetails": "ResNet-18", "paperSlug": "balanced-binary-neural-networks-with-gated", "usesAdditionalData": false}, {"x": "2019-09-30", "y": 0.8540000000000001, "name": "EfficientNet-B8 ", "nameShort": "EfficientNet-B8 ", "nameDetails": "RandAugment", "paperSlug": "randaugment-practical-data-augmentation-with", "usesAdditionalData": false}, {"x": "2019-09-30", "y": 0.85, "name": "EfficientNet-B7 ", "nameShort": "EfficientNet-B7 ", "nameDetails": "RandAugment", "paperSlug": "randaugment-practical-data-augmentation-with", "usesAdditionalData": false}, {"x": "2019-10-07", "y": 0.785, "name": "ResNet-50-DW ", "nameShort": "ResNet-50-DW ", "nameDetails": "Deformable Kernels", "paperSlug": "deformable-kernels-adapting-effective", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7892, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "ResNet-152", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7865000000000001, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "ResNet-101", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7748, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "ResNet-50", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7256, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "MobileNetV2", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-09", "y": 0.721, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "on-the-adequacy-of-untuned-warmup-for", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.8640000000000001, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B6", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.861, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B5", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.853, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B4", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.841, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B3", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.8240000000000001, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B2", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.815, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B1", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.7879999999999999, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B0", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.855, "name": "AdvProp ", "nameShort": "AdvProp ", "nameDetails": "EfficientNet-B8", "paperSlug": "adversarial-examples-improve-image", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.852, "name": "AdvProp ", "nameShort": "AdvProp ", "nameDetails": "EfficientNet-B7", "paperSlug": "adversarial-examples-improve-image", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.7895, "name": "InceptionV3 ", "nameShort": "InceptionV3 ", "nameDetails": "FRN layer", "paperSlug": "filter-response-normalization-layer", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.7720999999999999, "name": "ResnetV2 50 ", "nameShort": "ResnetV2 50 ", "nameDetails": "FRN layer", "paperSlug": "filter-response-normalization-layer", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.7979999999999999, "name": "CSPResNeXt-50 ", "nameShort": "CSPResNeXt-50 ", "nameDetails": "Mish+Aug", "paperSlug": "cspnet-a-new-backbone-that-can-enhance", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.757, "name": "GhostNet \u00d71.3", "nameShort": "GhostNet \u00d71.3", "nameDetails": null, "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.75, "name": "Ghost-ResNet-50 ", "nameShort": "Ghost-ResNet-50 ", "nameDetails": "s=2", "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.741, "name": "Ghost-ResNet-50 ", "nameShort": "Ghost-ResNet-50 ", "nameDetails": "s=4", "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.7390000000000001, "name": "GhostNet \u00d71.0", "nameShort": "GhostNet \u00d71.0", "nameDetails": null, "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.662, "name": "GhostNet \u00d70.5", "nameShort": "GhostNet \u00d70.5", "nameDetails": null, "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-29", "y": 0.733, "name": "Wide ResNet-50 ", "nameShort": "Wide ResNet-50 ", "nameDetails": "edge-popup", "paperSlug": "whats-hidden-in-a-randomly-weighted-neural", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.7440000000000001, "name": "DY-MobileNetV2 \u00d71.0", "nameShort": "DY-MobileNetV2 \u00d71.0", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.728, "name": "DY-MobileNetV2 \u00d70.75", "nameShort": "DY-MobileNetV2 \u00d70.75", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.727, "name": "DY-ResNet-18", "nameShort": "DY-ResNet-18", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.6970000000000001, "name": "DY-MobileNetV3-Small", "nameShort": "DY-MobileNetV3-Small", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.6940000000000001, "name": "DY-MobileNetV2 \u00d70.5", "nameShort": "DY-MobileNetV2 \u00d70.5", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.677, "name": "DY-ResNet-10", "nameShort": "DY-ResNet-10", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.649, "name": "DY-MobileNetV2 \u00d70.35", "nameShort": "DY-MobileNetV2 \u00d70.35", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-10", "y": 0.79, "name": "SpineNet-143", "nameShort": "SpineNet-143", "nameDetails": null, "paperSlug": "spinenet-learning-scale-permuted-backbone-for", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.8539, "name": "BiT-M ", "nameShort": "BiT-M ", "nameDetails": "ResNet", "paperSlug": "large-scale-learning-of-general-visual", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.8131999999999999, "name": "ResNet-200 ", "nameShort": "ResNet-200 ", "nameDetails": "Adversarial Autoaugment", "paperSlug": "adversarial-autoaugment-1", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.794, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "Adversarial Autoaugment", "paperSlug": "adversarial-autoaugment-1", "usesAdditionalData": false}, {"x": "2020-01-17", "y": 0.8420000000000001, "name": "Assemble-ResNet152", "nameShort": "Assemble-ResNet152", "nameDetails": null, "paperSlug": "compounding-the-performance-improvements-of", "usesAdditionalData": false}, {"x": "2020-01-18", "y": 0.8284999999999999, "name": "Harm-SE-RNX-101 64x4d ", "nameShort": "Harm-SE-RNX-101 64x4d ", "nameDetails": "320x320, Mean-Max Pooling", "paperSlug": "harmonic-convolutional-networks-based-on", "usesAdditionalData": false}, {"x": "2020-02-20", "y": 0.858, "name": "Fix-EfficientNet-B8 ", "nameShort": "Fix-EfficientNet-B8 ", "nameDetails": "MaxUp + CutMix", "paperSlug": "maxup-a-simple-way-to-improve-generalization", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.871, "name": "FixEfficientNet-B7", "nameShort": "FixEfficientNet-B7", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.867, "name": "FixEfficientNet-B6", "nameShort": "FixEfficientNet-B6", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.8640000000000001, "name": "FixEfficientNet-B5", "nameShort": "FixEfficientNet-B5", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.8590000000000001, "name": "FixEfficientNet-B4", "nameShort": "FixEfficientNet-B4", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.857, "name": "FixEfficientNet-B8", "nameShort": "FixEfficientNet-B8", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.85, "name": "FixEfficientNet-B3", "nameShort": "FixEfficientNet-B3", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.84, "name": "FixEfficientNetB4", "nameShort": "FixEfficientNetB4", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.836, "name": "FixEfficientNet-B2", "nameShort": "FixEfficientNet-B2", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.826, "name": "FixEfficientNet-B1", "nameShort": "FixEfficientNet-B1", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.802, "name": "FixEfficientNet-B0", "nameShort": "FixEfficientNet-B0", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-23", "y": 0.9, "name": "Meta Pseudo Labels ", "nameShort": "Meta Pseudo Labels ", "nameDetails": "EfficientNet-B6-Wide", "paperSlug": "meta-pseudo-labels", "usesAdditionalData": false}, {"x": "2020-03-23", "y": 0.8320000000000001, "name": "Meta Pseudo Labels ", "nameShort": "Meta Pseudo Labels ", "nameDetails": "ResNet-50", "paperSlug": "meta-pseudo-labels", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.858, "name": "KDforAA ", "nameShort": "KDforAA ", "nameDetails": "EfficientNet-B8", "paperSlug": "circumventing-outliers-of-autoaugment-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.855, "name": "KDforAA ", "nameShort": "KDforAA ", "nameDetails": "EfficientNet-B7", "paperSlug": "circumventing-outliers-of-autoaugment-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.7709999999999999, "name": "GreedyNAS-A", "nameShort": "GreedyNAS-A", "nameDetails": null, "paperSlug": "greedynas-towards-fast-one-shot-nas-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.768, "name": "GreedyNAS-B", "nameShort": "GreedyNAS-B", "nameDetails": null, "paperSlug": "greedynas-towards-fast-one-shot-nas-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.762, "name": "GreedyNAS-C", "nameShort": "GreedyNAS-C", "nameDetails": null, "paperSlug": "greedynas-towards-fast-one-shot-nas-with", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.843, "name": "TResNet-XL", "nameShort": "TResNet-XL", "nameDetails": null, "paperSlug": "tresnet-high-performance-gpu-dedicated", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.799, "name": "RegNetY-8.0GF", "nameShort": "RegNetY-8.0GF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.794, "name": "RegNetY-4.0GF", "nameShort": "RegNetY-4.0GF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.78, "name": "RegNetY-1.6GF", "nameShort": "RegNetY-1.6GF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.763, "name": "RegNetY-800MF", "nameShort": "RegNetY-800MF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.755, "name": "RegNetY-600MF", "nameShort": "RegNetY-600MF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.741, "name": "RegNetY-400MF", "nameShort": "RegNetY-400MF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.7659999999999999, "name": "MUXNet-l", "nameShort": "MUXNet-l", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.753, "name": "MUXNet-m", "nameShort": "MUXNet-m", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.716, "name": "MUXNet-s", "nameShort": "MUXNet-s", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.667, "name": "MUXNet-xs", "nameShort": "MUXNet-xs", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.845, "name": "ResNeSt-269", "nameShort": "ResNeSt-269", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.8390000000000001, "name": "ResNeSt-200", "nameShort": "ResNeSt-200", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.83, "name": "ResNeSt-101", "nameShort": "ResNeSt-101", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.8112999999999999, "name": "ResNeSt-50", "nameShort": "ResNeSt-50", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.8064, "name": "ResNeSt-50-fast", "nameShort": "ResNeSt-50-fast", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-23", "y": 0.8079999999999999, "name": "ResNet-200 ", "nameShort": "ResNet-200 ", "nameDetails": "Supervised Contrastive", "paperSlug": "supervised-contrastive-learning", "usesAdditionalData": false}, {"x": "2020-05-12", "y": 0.805, "name": "NAT-M4", "nameShort": "NAT-M4", "nameDetails": null, "paperSlug": "neural-architecture-transfer", "usesAdditionalData": false}, {"x": "2020-06-15", "y": 0.792, "name": "Multiscale DEQ ", "nameShort": "Multiscale DEQ ", "nameDetails": "MDEQ-XL", "paperSlug": "multiscale-deep-equilibrium-models", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.851, "name": "ResNet200_vd_26w_4s_ssld", "nameShort": "ResNet200_vd_26w_4s_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.84, "name": "Fix_ResNet50_vd_ssld", "nameShort": "Fix_ResNet50_vd_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.83, "name": "ResNet50_vd_ssld", "nameShort": "ResNet50_vd_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.79, "name": "MobileNetV3_large_x1_0_ssld", "nameShort": "MobileNetV3_large_x1_0_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-20", "y": 0.8149, "name": "PyConvResNet-101", "nameShort": "PyConvResNet-101", "nameDetails": null, "paperSlug": "pyramidal-convolution-rethinking", "usesAdditionalData": false}, {"x": "2020-06-20", "y": 0.7717, "name": "Prodpoly", "nameShort": "Prodpoly", "nameDetails": null, "paperSlug": "deep-polynomial-neural-networks", "usesAdditionalData": false}, {"x": "2020-06-22", "y": 0.7924, "name": "PS-KD ", "nameShort": "PS-KD ", "nameDetails": "ResNet-152 + CutMix", "paperSlug": "self-knowledge-distillation-a-simple-way-for", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.828, "name": "ReXNet_3.0", "nameShort": "ReXNet_3.0", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.816, "name": "ReXNet_2.0", "nameShort": "ReXNet_2.0", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.8029999999999999, "name": "ReXNet_1.5", "nameShort": "ReXNet_1.5", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.795, "name": "ReXNet_1.3", "nameShort": "ReXNet_1.3", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.779, "name": "ReXNet_1.0", "nameShort": "ReXNet_1.0", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.772, "name": "ReXNet_0.9", "nameShort": "ReXNet_0.9", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.746, "name": "ReXNet_0.6", "nameShort": "ReXNet_0.6", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-09-10", "y": 0.7197, "name": "Ours", "nameShort": "Ours", "nameDetails": null, "paperSlug": "quantnet-learning-to-quantize-by-learning", "usesAdditionalData": false}, {"x": "2020-09-15", "y": 0.7876000000000001, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "puzzle-mix-exploiting-saliency-and-local-1", "usesAdditionalData": false}, {"x": "2020-09-17", "y": 0.8172, "name": "MEAL V2 ", "nameShort": "MEAL V2 ", "nameDetails": "ResNet-50", "paperSlug": "meal-v2-boosting-vanilla-resnet-50-to-80-top", "usesAdditionalData": false}, {"x": "2020-09-17", "y": 0.8067, "name": "MEAL V2 ", "nameShort": "MEAL V2 ", "nameDetails": "ResNet-50", "paperSlug": "meal-v2-boosting-vanilla-resnet-50-to-80-top", "usesAdditionalData": false}, {"x": "2020-09-17", "y": 0.7319, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "MEAL V2", "paperSlug": "meal-v2-boosting-vanilla-resnet-50-to-80-top", "usesAdditionalData": false}, {"x": "2020-09-29", "y": 0.8022, "name": "iAFF-ResNeXt-50-32x4d", "nameShort": "iAFF-ResNeXt-50-32x4d", "nameDetails": null, "paperSlug": "attentional-feature-fusion", "usesAdditionalData": false}, {"x": "2020-10-03", "y": 0.8861, "name": "EfficientNet-L2-475 ", "nameShort": "EfficientNet-L2-475 ", "nameDetails": "SAM", "paperSlug": "sharpness-aware-minimization-for-efficiently-1", "usesAdditionalData": false}, {"x": "2020-10-03", "y": 0.816, "name": "ResNet-152 ", "nameShort": "ResNet-152 ", "nameDetails": "SAM", "paperSlug": "sharpness-aware-minimization-for-efficiently-1", "usesAdditionalData": false}, {"x": "2020-10-04", "y": 0.812, "name": "ResNeXt-101 ", "nameShort": "ResNeXt-101 ", "nameDetails": "Debiased+CutMix", "paperSlug": "shape-texture-debiased-neural-network-1", "usesAdditionalData": false}, {"x": "2020-10-22", "y": 0.8855, "name": "ViT-H/14", "nameShort": "ViT-H/14", "nameDetails": null, "paperSlug": "an-image-is-worth-16x16-words-transformers-1", "usesAdditionalData": false}, {"x": "2020-10-22", "y": 0.8776, "name": "ViT-L/16", "nameShort": "ViT-L/16", "nameDetails": null, "paperSlug": "an-image-is-worth-16x16-words-transformers-1", "usesAdditionalData": false}, {"x": "2020-10-22", "y": 0.8522, "name": "ViT-B/16", "nameShort": "ViT-B/16", "nameDetails": null, "paperSlug": "an-image-is-worth-16x16-words-transformers-1", "usesAdditionalData": false}, {"x": "2020-10-28", "y": 0.794, "name": "TinyNet ", "nameShort": "TinyNet ", "nameDetails": "GhostNet-A", "paperSlug": "model-rubik-s-cube-twisting-resolution-depth", "usesAdditionalData": false}, {"x": "2020-10-28", "y": 0.777, "name": "TinyNet-A + RA", "nameShort": "TinyNet-A + RA", "nameDetails": null, "paperSlug": "model-rubik-s-cube-twisting-resolution-depth", "usesAdditionalData": false}, {"x": "2020-11-03", "y": 0.7670999999999999, "name": "Perona Malik ", "nameShort": "Perona Malik ", "nameDetails": "Perona and Malik, 1990", "paperSlug": "learning-visual-representations-for-transfer-1", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7959999999999999, "name": "Grafit ", "nameShort": "Grafit ", "nameDetails": "ResNet-50", "paperSlug": "grafit-learning-fine-grained-image", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7171, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "PAD-L2 w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7156, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "FT w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7137, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "KD w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7108, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "L2 w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7093, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "CRD w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7051999999999999, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "tf-KD w/ ResNet-18 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7009000000000001, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "SSKD w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-30", "y": 0.836, "name": "SE-ResNeXt-101, 64x4d, S=2", "nameShort": "SE-ResNeXt-101, 64x4d, S=2", "nameDetails": "320px", "paperSlug": "splitnet-divide-and-co-training", "usesAdditionalData": false}, {"x": "2020-11-30", "y": 0.8334, "name": "SE-ResNeXt-101, 64x4d, S=2", "nameShort": "SE-ResNeXt-101, 64x4d, S=2", "nameDetails": "416px", "paperSlug": "splitnet-divide-and-co-training", "usesAdditionalData": false}, {"x": "2020-11-30", "y": 0.8212999999999999, "name": "ResNeXt-101, 64x4d, S=2", "nameShort": "ResNeXt-101, 64x4d, S=2", "nameDetails": "224px", "paperSlug": "splitnet-divide-and-co-training", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.852, "name": "DeiT-B 384", "nameShort": "DeiT-B 384", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.8420000000000001, "name": "DeiT-B", "nameShort": "DeiT-B", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.826, "name": "DeiT-B", "nameShort": "DeiT-B", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.7659999999999999, "name": "DeiT-B", "nameShort": "DeiT-B", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2021-01-05", "y": 0.8029999999999999, "name": "ResNet-50+AutoDropout+RandAugment", "nameShort": "ResNet-50+AutoDropout+RandAugment", "nameDetails": null, "paperSlug": "autodropout-learning-dropout-patterns-to", "usesAdditionalData": false}, {"x": "2021-01-05", "y": 0.787, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "autodropout-learning-dropout-patterns-to", "usesAdditionalData": false}, {"x": "2021-01-05", "y": 0.775, "name": "EfficientNet-B0", "nameShort": "EfficientNet-B0", "nameDetails": null, "paperSlug": "autodropout-learning-dropout-patterns-to", "usesAdditionalData": false}, {"x": "2021-01-07", "y": 0.77, "name": "SSAL-Resnet50", "nameShort": "SSAL-Resnet50", "nameDetails": null, "paperSlug": "contextual-classification-using-self", "usesAdditionalData": false}, {"x": "2021-01-11", "y": 0.7878000000000001, "name": "RepVGG-B2", "nameShort": "RepVGG-B2", "nameDetails": null, "paperSlug": "repvgg-making-vgg-style-convnets-great-again", "usesAdditionalData": false}, {"x": "2021-01-11", "y": 0.785, "name": "RepVGG-B2g4", "nameShort": "RepVGG-B2g4", "nameDetails": null, "paperSlug": "repvgg-making-vgg-style-convnets-great-again", "usesAdditionalData": false}, {"x": "2021-01-13", "y": 0.784, "name": "ReXNet_1.0-relabel", "nameShort": "ReXNet_1.0-relabel", "nameDetails": null, "paperSlug": "re-labeling-imagenet-from-single-to-multi", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.847, "name": "BoTNet T7", "nameShort": "BoTNet T7", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8420000000000001, "name": "BoTNet T7-320", "nameShort": "BoTNet T7-320", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.84, "name": "BoTNet T6", "nameShort": "BoTNet T6", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.838, "name": "SENet-350", "nameShort": "SENet-350", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.835, "name": "BoTNet T5", "nameShort": "BoTNet T5", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.828, "name": "BoTNet T4", "nameShort": "BoTNet T4", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8220000000000001, "name": "SENet-152", "nameShort": "SENet-152", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8170000000000001, "name": "BoTNet T3", "nameShort": "BoTNet T3", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8140000000000001, "name": "SENet-101", "nameShort": "SENet-101", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8, "name": "ResNet-101", "nameShort": "ResNet-101", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.794, "name": "SENet-50", "nameShort": "SENet-50", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.7879999999999999, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.833, "name": "T2T-ViT-14|384", "nameShort": "T2T-ViT-14|384", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.826, "name": "T2T-ViTt-24", "nameShort": "T2T-ViTt-24", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.823, "name": "T2T-ViT-24", "nameShort": "T2T-ViT-24", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.8220000000000001, "name": "T2T-ViTt-19", "nameShort": "T2T-ViTt-19", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.8190000000000001, "name": "T2T-ViT-19", "nameShort": "T2T-ViT-19", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.815, "name": "T2T-ViT-14", "nameShort": "T2T-ViT-14", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-02-01", "y": 0.83, "name": "ZenNAS ", "nameShort": "ZenNAS ", "nameDetails": "0.8ms", "paperSlug": "zen-nas-a-zero-shot-nas-for-high-performance", "usesAdditionalData": false}, {"x": "2021-02-01", "y": 0.78, "name": "ZenNet-400M-SE", "nameShort": "ZenNet-400M-SE", "nameDetails": null, "paperSlug": "zen-nas-a-zero-shot-nas-for-high-performance", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.892, "name": "NFNet-F4+", "nameShort": "NFNet-F4+", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.8864, "name": "ALIGN ", "nameShort": "ALIGN ", "nameDetails": "EfficientNet-L2", "paperSlug": "scaling-up-visual-and-vision-language", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.865, "name": "NFNet-F6 w/ SAM", "nameShort": "NFNet-F6 w/ SAM", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.863, "name": "NFNet-F5 w/ SAM", "nameShort": "NFNet-F5 w/ SAM", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.86, "name": "NFNet-F5", "nameShort": "NFNet-F5", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.8590000000000001, "name": "NFNet-F4", "nameShort": "NFNet-F4", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.857, "name": "NFNet-F3", "nameShort": "NFNet-F3", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.851, "name": "NFNet-F2", "nameShort": "NFNet-F2", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.847, "name": "NFNet-F1", "nameShort": "NFNet-F1", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.836, "name": "NFNet-F0", "nameShort": "NFNet-F0", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-12", "y": 0.7592, "name": "ResNet-50 MLPerf v0.7 - 2512 steps", "nameShort": "ResNet-50 MLPerf v0.7 - 2512 steps", "nameDetails": null, "paperSlug": "a-large-batch-optimizer-reality-check", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.8079999999999999, "name": "AlphaNet-A6", "nameShort": "AlphaNet-A6", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.8029999999999999, "name": "AlphaNet-A5", "nameShort": "AlphaNet-A5", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.8, "name": "AlphaNet-A4", "nameShort": "AlphaNet-A4", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.794, "name": "AlphaNet-A3", "nameShort": "AlphaNet-A3", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.7909999999999999, "name": "AlphaNet-A2", "nameShort": "AlphaNet-A2", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.789, "name": "AlphaNet-A1", "nameShort": "AlphaNet-A1", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.778, "name": "AlphaNet-A0", "nameShort": "AlphaNet-A0", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-17", "y": 0.843, "name": "LambdaResNet200", "nameShort": "LambdaResNet200", "nameDetails": null, "paperSlug": "lambdanetworks-modeling-long-range-1", "usesAdditionalData": false}, {"x": "2021-02-17", "y": 0.84, "name": "LambdaResNet152", "nameShort": "LambdaResNet152", "nameDetails": null, "paperSlug": "lambdanetworks-modeling-long-range-1", "usesAdditionalData": false}, {"x": "2021-02-17", "y": 0.809, "name": "CentroidViT-S ", "nameShort": "CentroidViT-S ", "nameDetails": "arXiv, 2021-02", "paperSlug": "centroid-transformers-learning-to-abstract", "usesAdditionalData": false}, {"x": "2021-02-27", "y": 0.8390000000000001, "name": "TNT-B", "nameShort": "TNT-B", "nameDetails": null, "paperSlug": "transformer-in-transformer", "usesAdditionalData": false}, {"x": "2021-03-04", "y": 0.78, "name": "Perceiver ", "nameShort": "Perceiver ", "nameDetails": "FF", "paperSlug": "perceiver-general-perception-with-iterative", "usesAdditionalData": false}, {"x": "2021-03-04", "y": 0.764, "name": "Perceiver", "nameShort": "Perceiver", "nameDetails": null, "paperSlug": "perceiver-general-perception-with-iterative", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.7929999999999999, "name": "RedNet-152", "nameShort": "RedNet-152", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.7909999999999999, "name": "RedNet-101", "nameShort": "RedNet-101", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.784, "name": "RedNet-50", "nameShort": "RedNet-50", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.7759999999999999, "name": "RedNet-38", "nameShort": "RedNet-38", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.759, "name": "RedNet-26", "nameShort": "RedNet-26", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-13", "y": 0.8440000000000001, "name": "ResNet-RS-50 ", "nameShort": "ResNet-RS-50 ", "nameDetails": "160 image res", "paperSlug": "revisiting-resnets-improved-training-and", "usesAdditionalData": false}, {"x": "2021-03-13", "y": 0.838, "name": "ResNet-RS-270 ", "nameShort": "ResNet-RS-270 ", "nameDetails": "256 image res", "paperSlug": "revisiting-resnets-improved-training-and", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.825, "name": "ConViT-B+", "nameShort": "ConViT-B+", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.8240000000000001, "name": "ConViT-B", "nameShort": "ConViT-B", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.8220000000000001, "name": "ConViT-S+", "nameShort": "ConViT-S+", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.813, "name": "ConViT-S", "nameShort": "ConViT-S", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.78, "name": "HVT-S-1", "nameShort": "HVT-S-1", "nameDetails": null, "paperSlug": "scalable-visual-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.767, "name": "ConViT-Ti+", "nameShort": "ConViT-Ti+", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.731, "name": "ConViT-Ti", "nameShort": "ConViT-Ti", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.6964, "name": "HVT-Ti-1", "nameShort": "HVT-Ti-1", "nameDetails": null, "paperSlug": "scalable-visual-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.833, "name": "CeiT-S ", "nameShort": "CeiT-S ", "nameDetails": "384 finetune res", "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.831, "name": "DeepVit-L* ", "nameShort": "DeepVit-L* ", "nameDetails": "DeiT training recipe", "paperSlug": "deepvit-towards-deeper-vision-transformer", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.8220000000000001, "name": "DeepVit-L", "nameShort": "DeepVit-L", "nameDetails": null, "paperSlug": "deepvit-towards-deeper-vision-transformer", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.82, "name": "CeiT-S", "nameShort": "CeiT-S", "nameDetails": null, "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.7879999999999999, "name": "CeiT-T ", "nameShort": "CeiT-T ", "nameDetails": "384 finetune res", "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.764, "name": "CeiT-T", "nameShort": "CeiT-T", "nameDetails": null, "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-23", "y": 0.855, "name": "HaloNet4 ", "nameShort": "HaloNet4 ", "nameDetails": "base 128, Conv-12", "paperSlug": "scaling-local-self-attention-for-parameter", "usesAdditionalData": false}, {"x": "2021-03-23", "y": 0.8220000000000001, "name": "BossNet-T1", "nameShort": "BossNet-T1", "nameDetails": null, "paperSlug": "bossnas-exploring-hybrid-cnn-transformers", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.8098000000000001, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.7925, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.7609999999999999, "name": "ResNet-34 ", "nameShort": "ResNet-34 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.7204999999999999, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-25", "y": 0.873, "name": "Swin-L ", "nameShort": "Swin-L ", "nameDetails": "384 res, ImageNet-22k pretrain", "paperSlug": "swin-transformer-hierarchical-vision", "usesAdditionalData": false}, {"x": "2021-03-25", "y": 0.8640000000000001, "name": "Swin-B ", "nameShort": "Swin-B ", "nameDetails": "384 res, ImageNet-22k pretrain", "paperSlug": "swin-transformer-hierarchical-vision", "usesAdditionalData": false}, {"x": "2021-03-25", "y": 0.813, "name": "Swin-T", "nameShort": "Swin-T", "nameDetails": null, "paperSlug": "swin-transformer-hierarchical-vision", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.828, "name": "CrossViT-18+", "nameShort": "CrossViT-18+", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.825, "name": "CrossViT-18", "nameShort": "CrossViT-18", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.823, "name": "CrossViT-15+", "nameShort": "CrossViT-15+", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.815, "name": "CrossViT-15", "nameShort": "CrossViT-15", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8490000000000001, "name": "CvT-21 ", "nameShort": "CvT-21 ", "nameDetails": "384 res, ImageNet-22k pretrain", "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.833, "name": "CvT-21 ", "nameShort": "CvT-21 ", "nameDetails": "384 res", "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.833, "name": "ViL-Medium-D", "nameShort": "ViL-Medium-D", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8320000000000001, "name": "ViL-Base-D", "nameShort": "ViL-Base-D", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.83, "name": "CvT-13 ", "nameShort": "CvT-13 ", "nameDetails": "384 res", "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8290000000000001, "name": "ViL-Medium-W", "nameShort": "ViL-Medium-W", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.825, "name": "CvT-21", "nameShort": "CvT-21", "nameDetails": null, "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8220000000000001, "name": "CvT-13-NAS", "nameShort": "CvT-13-NAS", "nameDetails": null, "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.82, "name": "ViL-Small", "nameShort": "ViL-Small", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8190000000000001, "name": "ViL-Base-W", "nameShort": "ViL-Base-W", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.816, "name": "CvT-13", "nameShort": "CvT-13", "nameDetails": null, "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.767, "name": "ViL-Tiny-RPB", "nameShort": "ViL-Tiny-RPB", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.84, "name": "PiT-B", "nameShort": "PiT-B", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.8190000000000001, "name": "PiT-S", "nameShort": "PiT-S", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.7909999999999999, "name": "PiT-XS", "nameShort": "PiT-XS", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.746, "name": "PiT-Ti", "nameShort": "PiT-Ti", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.865, "name": "CaiT-M-48-448", "nameShort": "CaiT-M-48-448", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.863, "name": "CAIT-M36-448", "nameShort": "CAIT-M36-448", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.861, "name": "CAIT-M-36", "nameShort": "CAIT-M-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.858, "name": "CAIT-M-24", "nameShort": "CAIT-M-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.8540000000000001, "name": "CAIT-S-36", "nameShort": "CAIT-S-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.853, "name": "CAIT-S-48", "nameShort": "CAIT-S-48", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.851, "name": "CAIT-S-24", "nameShort": "CAIT-S-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.848, "name": "CAIT-XS-36", "nameShort": "CAIT-XS-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.841, "name": "CAIT-XS-24", "nameShort": "CAIT-XS-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.8220000000000001, "name": "CAIT-XXS-36", "nameShort": "CAIT-XXS-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.809, "name": "CAIT-XXS-24", "nameShort": "CAIT-XXS-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.868, "name": "EfficientNetV2-L ", "nameShort": "EfficientNetV2-L ", "nameDetails": "21k", "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.861, "name": "EfficientNetV2-M ", "nameShort": "EfficientNetV2-M ", "nameDetails": "21k", "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.857, "name": "EfficientNetV2-L", "nameShort": "EfficientNetV2-L", "nameDetails": null, "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.851, "name": "EfficientNetV2-M", "nameShort": "EfficientNetV2-M", "nameDetails": null, "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.85, "name": "EfficientNetV2-S ", "nameShort": "EfficientNetV2-S ", "nameDetails": "21k", "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.8390000000000001, "name": "EfficientNetV2-S", "nameShort": "EfficientNetV2-S", "nameDetails": null, "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.825, "name": "LeViT-384", "nameShort": "LeViT-384", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.816, "name": "LeViT-256", "nameShort": "LeViT-256", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.8, "name": "LeViT-192", "nameShort": "LeViT-192", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.7959999999999999, "name": "LeViT-128", "nameShort": "LeViT-128", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.757, "name": "LeViT-128S", "nameShort": "LeViT-128S", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8271, "name": "CCT-14/7x2 | 384", "nameShort": "CCT-14/7x2 | 384", "nameDetails": null, "paperSlug": "escaping-the-big-data-paradigm-with-compact", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8134, "name": "CCT-14/7x2", "nameShort": "CCT-14/7x2", "nameDetails": null, "paperSlug": "escaping-the-big-data-paradigm-with-compact", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8079999999999999, "name": "LocalViT-S", "nameShort": "LocalViT-S", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8028, "name": "CCT-16/7x2", "nameShort": "CCT-16/7x2", "nameDetails": null, "paperSlug": "escaping-the-big-data-paradigm-with-compact", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.782, "name": "LocalViT-PVT", "nameShort": "LocalViT-PVT", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.759, "name": "LocalViT-TNT", "nameShort": "LocalViT-TNT", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.748, "name": "LocalViT-T", "nameShort": "LocalViT-T", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.725, "name": "LocalViT-T2T", "nameShort": "LocalViT-T2T", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-15", "y": 0.754, "name": "AsymmNet-Large \u00d71.0", "nameShort": "AsymmNet-Large \u00d71.0", "nameDetails": null, "paperSlug": "asymmnet-towards-ultralight-convolution", "usesAdditionalData": false}, {"x": "2021-04-15", "y": 0.6920000000000001, "name": "AsymmNet-Large \u00d70.5", "nameShort": "AsymmNet-Large \u00d70.5", "nameDetails": null, "paperSlug": "asymmnet-towards-ultralight-convolution", "usesAdditionalData": false}, {"x": "2021-04-15", "y": 0.684, "name": "AsymmNet-Small \u00d71.0", "nameShort": "AsymmNet-Small \u00d71.0", "nameDetails": null, "paperSlug": "asymmnet-towards-ultralight-convolution", "usesAdditionalData": false}, {"x": "2021-04-16", "y": 0.716, "name": "PDC", "nameShort": "PDC", "nameDetails": null, "paperSlug": "polynomial-networks-in-deep-classifiers", "usesAdditionalData": false}, {"x": "2021-04-16", "y": 0.68, "name": "ReActNet-A ", "nameShort": "ReActNet-A ", "nameDetails": "BN-Free", "paperSlug": "bnn-bn-training-binary-neural-networks", "usesAdditionalData": false}, {"x": "2021-04-20", "y": 0.82, "name": "DIFFQ ", "nameShort": "DIFFQ ", "nameDetails": "\u03bb=1e\u22122", "paperSlug": "differentiable-model-compression-via-pseudo", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.8640000000000001, "name": "LV-ViT-L", "nameShort": "LV-ViT-L", "nameDetails": null, "paperSlug": "token-labeling-training-a-85-5-top-1-accuracy", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.848, "name": "MViT-B-24", "nameShort": "MViT-B-24", "nameDetails": null, "paperSlug": "multiscale-vision-transformers", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.841, "name": "LV-ViT-M", "nameShort": "LV-ViT-M", "nameDetails": null, "paperSlug": "token-labeling-training-a-85-5-top-1-accuracy", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.833, "name": "LV-ViT-S", "nameShort": "LV-ViT-S", "nameDetails": null, "paperSlug": "token-labeling-training-a-85-5-top-1-accuracy", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.83, "name": "MViT-B-16", "nameShort": "MViT-B-16", "nameDetails": null, "paperSlug": "multiscale-vision-transformers", "usesAdditionalData": false}, {"x": "2021-04-26", "y": 0.8220000000000001, "name": "Visformer-S", "nameShort": "Visformer-S", "nameDetails": null, "paperSlug": "visformer-the-vision-friendly-transformer", "usesAdditionalData": false}, {"x": "2021-04-26", "y": 0.7859999999999999, "name": "Visformer-Ti", "nameShort": "Visformer-Ti", "nameDetails": null, "paperSlug": "visformer-the-vision-friendly-transformer", "usesAdditionalData": false}, {"x": "2021-04-28", "y": 0.8370000000000001, "name": "Twins-SVT-L", "nameShort": "Twins-SVT-L", "nameDetails": null, "paperSlug": "twins-revisiting-spatial-attention-design-in", "usesAdditionalData": false}, {"x": "2021-04-28", "y": 0.755, "name": "PAWS ", "nameShort": "PAWS ", "nameDetails": "ResNet-50, 10% labels", "paperSlug": "semi-supervised-learning-of-visual-features", "usesAdditionalData": false}, {"x": "2021-04-28", "y": 0.665, "name": "PAWS ", "nameShort": "PAWS ", "nameDetails": "ResNet-50, 1% labels", "paperSlug": "semi-supervised-learning-of-visual-features", "usesAdditionalData": false}, {"x": "2021-05-04", "y": 0.8794, "name": "Mixer-H/14 ", "nameShort": "Mixer-H/14 ", "nameDetails": "JFT-300M pre-train", "paperSlug": "mlp-mixer-an-all-mlp-architecture-for-vision", "usesAdditionalData": false}, {"x": "2021-05-04", "y": 0.853, "name": "ViT-L/16 Dosovitskiy et al. ", "nameShort": "ViT-L/16 Dosovitskiy et al. ", "nameDetails": "2021", "paperSlug": "mlp-mixer-an-all-mlp-architecture-for-vision", "usesAdditionalData": false}, {"x": "2021-05-04", "y": 0.7644, "name": "Mixer-B/16", "nameShort": "Mixer-B/16", "nameDetails": null, "paperSlug": "mlp-mixer-an-all-mlp-architecture-for-vision", "usesAdditionalData": false}, {"x": "2021-05-05", "y": 0.8170000000000001, "name": "T2T-ViT-14", "nameShort": "T2T-ViT-14", "nameDetails": null, "paperSlug": "beyond-self-attention-external-attention", "usesAdditionalData": false}, {"x": "2021-05-05", "y": 0.7859999999999999, "name": "RepMLP-Res50", "nameShort": "RepMLP-Res50", "nameDetails": null, "paperSlug": "repmlp-re-parameterizing-convolutions-into", "usesAdditionalData": false}, {"x": "2021-05-06", "y": 0.7490000000000001, "name": "FF", "nameShort": "FF", "nameDetails": null, "paperSlug": "do-you-even-need-attention-a-stack-of-feed", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.836, "name": "ResMLP-B24/8", "nameShort": "ResMLP-B24/8", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.8079999999999999, "name": "ResMLP-S24", "nameShort": "ResMLP-S24", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.8, "name": "BasisNet-MV3", "nameShort": "BasisNet-MV3", "nameDetails": null, "paperSlug": "basisnet-two-stage-model-synthesis-for-1", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.797, "name": "ResMLP-36", "nameShort": "ResMLP-36", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.794, "name": "ResMLP-24", "nameShort": "ResMLP-24", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.7859999999999999, "name": "ResMLP-12 ", "nameShort": "ResMLP-12 ", "nameDetails": "distilled, class-MLP", "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.778, "name": "ResMLP-S12", "nameShort": "ResMLP-S12", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-09", "y": 0.841, "name": "Conformer-B", "nameShort": "Conformer-B", "nameDetails": null, "paperSlug": "conformer-local-features-coupling-global", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.8270000000000001, "name": "RVT-B*", "nameShort": "RVT-B*", "nameDetails": null, "paperSlug": "rethinking-the-design-principles-of-robust", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.8190000000000001, "name": "RVT-S*", "nameShort": "RVT-S*", "nameDetails": null, "paperSlug": "rethinking-the-design-principles-of-robust", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.816, "name": "gMLP-B", "nameShort": "gMLP-B", "nameDetails": null, "paperSlug": "pay-attention-to-mlps", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.792, "name": "RVT-Ti*", "nameShort": "RVT-Ti*", "nameDetails": null, "paperSlug": "rethinking-the-design-principles-of-robust", "usesAdditionalData": false}, {"x": "2021-05-19", "y": 0.6859999999999999, "name": "Heteroscedastic ", "nameShort": "Heteroscedastic ", "nameDetails": "InceptionResNet-v2", "paperSlug": "correlated-input-dependent-label-noise-in", "usesAdditionalData": false}, {"x": "2021-05-26", "y": 0.838, "name": "Transformer local-attention ", "nameShort": "Transformer local-attention ", "nameDetails": "NesT-B", "paperSlug": "aggregating-nested-transformers", "usesAdditionalData": false}, {"x": "2021-05-26", "y": 0.833, "name": "Transformer local-attention ", "nameShort": "Transformer local-attention ", "nameDetails": "NesT-S", "paperSlug": "aggregating-nested-transformers", "usesAdditionalData": false}, {"x": "2021-05-26", "y": 0.815, "name": "Transformer local-attention ", "nameShort": "Transformer local-attention ", "nameDetails": "NesT-T", "paperSlug": "aggregating-nested-transformers", "usesAdditionalData": false}, {"x": "2021-05-27", "y": 0.8678, "name": "NFNet-F5 w/ SAM w/ augmult=16", "nameShort": "NFNet-F5 w/ SAM w/ augmult=16", "nameDetails": null, "paperSlug": "drawing-multiple-augmentation-samples-per", "usesAdditionalData": false}, {"x": "2021-05-28", "y": 0.836, "name": "ResT-Large", "nameShort": "ResT-Large", "nameDetails": null, "paperSlug": "rest-an-efficient-transformer-for-visual", "usesAdditionalData": false}, {"x": "2021-05-28", "y": 0.7959999999999999, "name": "ResT-Small", "nameShort": "ResT-Small", "nameDetails": null, "paperSlug": "rest-an-efficient-transformer-for-visual", "usesAdditionalData": false}, {"x": "2021-05-31", "y": 0.8043, "name": "DVT ", "nameShort": "DVT ", "nameDetails": "T2T-ViT-12", "paperSlug": "not-all-images-are-worth-16x16-words-dynamic", "usesAdditionalData": false}, {"x": "2021-05-31", "y": 0.7974, "name": "DVT ", "nameShort": "DVT ", "nameDetails": "T2T-ViT-10", "paperSlug": "not-all-images-are-worth-16x16-words-dynamic", "usesAdditionalData": false}, {"x": "2021-05-31", "y": 0.7848, "name": "DVT ", "nameShort": "DVT ", "nameDetails": "T2T-ViT-7", "paperSlug": "not-all-images-are-worth-16x16-words-dynamic", "usesAdditionalData": false}, {"x": "2021-06-02", "y": 0.8270000000000001, "name": "Container Container", "nameShort": "Container Container", "nameDetails": null, "paperSlug": "container-context-aggregation-network", "usesAdditionalData": false}, {"x": "2021-06-02", "y": 0.82, "name": "Container-Light", "nameShort": "Container-Light", "nameDetails": null, "paperSlug": "container-context-aggregation-network", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.8390000000000001, "name": "DynamicViT-LV-M/0.8", "nameShort": "DynamicViT-LV-M/0.8", "nameDetails": null, "paperSlug": "dynamicvit-efficient-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.8109999999999999, "name": "ResNet-152x2-SAM", "nameShort": "ResNet-152x2-SAM", "nameDetails": null, "paperSlug": "when-vision-transformers-outperform-resnets", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.799, "name": "ViT-B/16-SAM", "nameShort": "ViT-B/16-SAM", "nameDetails": null, "paperSlug": "when-vision-transformers-outperform-resnets", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.79, "name": "Mixer-B/8-SAM", "nameShort": "Mixer-B/8-SAM", "nameDetails": null, "paperSlug": "when-vision-transformers-outperform-resnets", "usesAdditionalData": false}, {"x": "2021-06-04", "y": 0.7659999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "X-volution, stage3", "paperSlug": "x-volution-on-the-unification-of-convolution", "usesAdditionalData": false}, {"x": "2021-06-04", "y": 0.75, "name": "ResNet-34 ", "nameShort": "ResNet-34 ", "nameDetails": "X-volution, stage3", "paperSlug": "x-volution-on-the-unification-of-convolution", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.8603000000000001, "name": "Refiner-ViT-L", "nameShort": "Refiner-ViT-L", "nameDetails": null, "paperSlug": "refiner-refining-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.836, "name": "ViTAE-B-Stage", "nameShort": "ViTAE-B-Stage", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.8220000000000001, "name": "ViTAE-S-Stage", "nameShort": "ViTAE-S-Stage", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.81, "name": "ViTAE-13M", "nameShort": "ViTAE-13M", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.779, "name": "ViTAE-6M", "nameShort": "ViTAE-6M", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.768, "name": "ViTAE-T-Stage", "nameShort": "ViTAE-T-Stage", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.753, "name": "ViTAE-T", "nameShort": "ViTAE-T", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.9045000000000001, "name": "CoAtNet-6", "nameShort": "CoAtNet-6", "nameDetails": null, "paperSlug": "coatnet-marrying-convolution-and-attention", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.8852, "name": "CoAtNet-3 @384", "nameShort": "CoAtNet-3 @384", "nameDetails": null, "paperSlug": "coatnet-marrying-convolution-and-attention", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.828, "name": "FunMatch - T384+224 ", "nameShort": "FunMatch - T384+224 ", "nameDetails": "ResNet-50", "paperSlug": "knowledge-distillation-a-good-teacher-is", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8835999999999999, "name": "V-MoE-H/14 ", "nameShort": "V-MoE-H/14 ", "nameDetails": "Every-2", "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8823000000000001, "name": "V-MoE-H/14 ", "nameShort": "V-MoE-H/14 ", "nameDetails": "Last-5", "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8808, "name": "VIT-H/14", "nameShort": "VIT-H/14", "nameDetails": null, "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8741, "name": "V-MoE-L/16 ", "nameShort": "V-MoE-L/16 ", "nameDetails": "Every-2", "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-15", "y": 0.8859999999999999, "name": "BEiT-L ", "nameShort": "BEiT-L ", "nameDetails": "ViT; ImageNet-22K pretrain", "paperSlug": "beit-bert-pre-training-of-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-15", "y": 0.863, "name": "BEiT-L ", "nameShort": "BEiT-L ", "nameDetails": "ViT; ImageNet 1k pretrain", "paperSlug": "beit-bert-pre-training-of-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.86, "name": "XCiT-L24", "nameShort": "XCiT-L24", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.858, "name": "XCiT-M24", "nameShort": "XCiT-M24", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.856, "name": "XCiT-S24", "nameShort": "XCiT-S24", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.851, "name": "XCiT-S12", "nameShort": "XCiT-S12", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-21", "y": 0.8887, "name": "TokenLearner L/8 ", "nameShort": "TokenLearner L/8 ", "nameDetails": "24+11", "paperSlug": "tokenlearner-what-can-8-learned-tokens-do-for", "usesAdditionalData": false}, {"x": "2021-06-21", "y": 0.8706999999999999, "name": "16-TokenLearner B/16 ", "nameShort": "16-TokenLearner B/16 ", "nameDetails": "21", "paperSlug": "tokenlearner-what-can-8-learned-tokens-do-for", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.871, "name": "VOLO-D5", "nameShort": "VOLO-D5", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.868, "name": "VOLO-D4", "nameShort": "VOLO-D4", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.863, "name": "VOLO-D3", "nameShort": "VOLO-D3", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.86, "name": "VOLO-D2", "nameShort": "VOLO-D2", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.852, "name": "VOLO-D1", "nameShort": "VOLO-D1", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.838, "name": "PVTv2-B4", "nameShort": "PVTv2-B4", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.8320000000000001, "name": "PVTv2-B3", "nameShort": "PVTv2-B3", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.82, "name": "PVTv2-B2", "nameShort": "PVTv2-B2", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.787, "name": "PVTv2-B1", "nameShort": "PVTv2-B1", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.705, "name": "PVTv2-B0", "nameShort": "PVTv2-B0", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.875, "name": "CSWin-L ", "nameShort": "CSWin-L ", "nameDetails": "384 res,ImageNet-22k pretrain", "paperSlug": "cswin-transformer-a-general-vision", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.8290000000000001, "name": "GFNet-H-B", "nameShort": "GFNet-H-B", "nameDetails": null, "paperSlug": "global-filter-networks-for-image", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.8240000000000001, "name": "AutoFormer-base", "nameShort": "AutoFormer-base", "nameDetails": null, "paperSlug": "autoformer-searching-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.8170000000000001, "name": "AutoFormer-small", "nameShort": "AutoFormer-small", "nameDetails": null, "paperSlug": "autoformer-searching-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.747, "name": "AutoFormer-tiny", "nameShort": "AutoFormer-tiny", "nameDetails": null, "paperSlug": "autoformer-searching-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-07-07", "y": 0.823, "name": "GLiT-Bases", "nameShort": "GLiT-Bases", "nameDetails": null, "paperSlug": "glit-neural-architecture-search-for-global", "usesAdditionalData": false}, {"x": "2021-07-07", "y": 0.805, "name": "GLiT-Smalls", "nameShort": "GLiT-Smalls", "nameDetails": null, "paperSlug": "glit-neural-architecture-search-for-global", "usesAdditionalData": false}, {"x": "2021-07-07", "y": 0.763, "name": "GLiT-Tinys", "nameShort": "GLiT-Tinys", "nameDetails": null, "paperSlug": "glit-neural-architecture-search-for-global", "usesAdditionalData": false}, {"x": "2021-07-08", "y": 0.815, "name": "CoE-Large + CondConv", "nameShort": "CoE-Large + CondConv", "nameDetails": null, "paperSlug": "collaboration-of-experts-achieving-80-top-1", "usesAdditionalData": false}, {"x": "2021-07-08", "y": 0.807, "name": "CoE-Large", "nameShort": "CoE-Large", "nameDetails": null, "paperSlug": "collaboration-of-experts-achieving-80-top-1", "usesAdditionalData": false}, {"x": "2021-07-08", "y": 0.8, "name": "CoE-Small + CondConv + PWLU", "nameShort": "CoE-Small + CondConv + PWLU", "nameDetails": null, "paperSlug": "collaboration-of-experts-achieving-80-top-1", "usesAdditionalData": false}, {"x": "2021-07-13", "y": 0.8420000000000001, "name": "ViP-B|384", "nameShort": "ViP-B|384", "nameDetails": null, "paperSlug": "visual-parser-representing-part-whole", "usesAdditionalData": false}, {"x": "2021-07-21", "y": 0.8320000000000001, "name": "CycleMLP-B5", "nameShort": "CycleMLP-B5", "nameDetails": null, "paperSlug": "cyclemlp-a-mlp-like-architecture-for-dense", "usesAdditionalData": false}, {"x": "2021-07-23", "y": 0.7709999999999999, "name": "SkipblockNet-L", "nameShort": "SkipblockNet-L", "nameDetails": null, "paperSlug": "bias-loss-for-mobile-neural-networks", "usesAdditionalData": false}, {"x": "2021-07-23", "y": 0.762, "name": "SkipblockNet-M", "nameShort": "SkipblockNet-M", "nameDetails": null, "paperSlug": "bias-loss-for-mobile-neural-networks", "usesAdditionalData": false}, {"x": "2021-07-25", "y": 0.8009000000000001, "name": "WideNet-H", "nameShort": "WideNet-H", "nameDetails": null, "paperSlug": "go-wider-instead-of-deeper", "usesAdditionalData": false}, {"x": "2021-07-25", "y": 0.7948999999999999, "name": "WideNet-L", "nameShort": "WideNet-L", "nameDetails": null, "paperSlug": "go-wider-instead-of-deeper", "usesAdditionalData": false}, {"x": "2021-07-25", "y": 0.7754000000000001, "name": "WideNet-B", "nameShort": "WideNet-B", "nameDetails": null, "paperSlug": "go-wider-instead-of-deeper", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.846, "name": "SE-CoTNetD-152", "nameShort": "SE-CoTNetD-152", "nameDetails": null, "paperSlug": "contextual-transformer-networks-for-visual", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.8320000000000001, "name": "SE-CoTNetD-101", "nameShort": "SE-CoTNetD-101", "nameDetails": null, "paperSlug": "contextual-transformer-networks-for-visual", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.818, "name": "ResNet-200", "nameShort": "ResNet-200", "nameDetails": null, "paperSlug": "parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.816, "name": "SE-CoTNetD-50", "nameShort": "SE-CoTNetD-50", "nameDetails": null, "paperSlug": "contextual-transformer-networks-for-visual", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.813, "name": "ResNet-152", "nameShort": "ResNet-152", "nameDetails": null, "paperSlug": "parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.809, "name": "ResNet-101", "nameShort": "ResNet-101", "nameDetails": null, "paperSlug": "parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.8240000000000001, "name": "DeiT-B with iRPE-K", "nameShort": "DeiT-B with iRPE-K", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.8140000000000001, "name": "DeiT-S with iRPE-QKV", "nameShort": "DeiT-S with iRPE-QKV", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.8109999999999999, "name": "DeiT-S with iRPE-QK", "nameShort": "DeiT-S with iRPE-QK", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.809, "name": "DeiT-S with iRPE-K", "nameShort": "DeiT-S with iRPE-K", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.737, "name": "DeiT-Ti with iRPE-K", "nameShort": "DeiT-Ti with iRPE-K", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-08-03", "y": 0.8220000000000001, "name": "Evo-LeViT-384*", "nameShort": "Evo-LeViT-384*", "nameDetails": null, "paperSlug": "evo-vit-slow-fast-token-evolution-for-dynamic", "usesAdditionalData": false}, {"x": "2021-08-17", "y": 0.7903, "name": "Co-ResNet-152", "nameShort": "Co-ResNet-152", "nameDetails": null, "paperSlug": "contextual-convolutional-neural-networks", "usesAdditionalData": false}, {"x": "2021-09-09", "y": 0.802, "name": "ConvMLP-L", "nameShort": "ConvMLP-L", "nameDetails": null, "paperSlug": "convmlp-hierarchical-convolutional-mlps-for", "usesAdditionalData": false}, {"x": "2021-09-09", "y": 0.79, "name": "ConvMLP-M", "nameShort": "ConvMLP-M", "nameDetails": null, "paperSlug": "convmlp-hierarchical-convolutional-mlps-for", "usesAdditionalData": false}, {"x": "2021-09-09", "y": 0.768, "name": "ConvMLP-S", "nameShort": "ConvMLP-S", "nameDetails": null, "paperSlug": "convmlp-hierarchical-convolutional-mlps-for", "usesAdditionalData": false}, {"x": "2021-09-12", "y": 0.8340000000000001, "name": "sMLPNet-B ", "nameShort": "sMLPNet-B ", "nameDetails": "ImageNet-1k", "paperSlug": "sparse-mlp-for-image-recognition-is-self", "usesAdditionalData": false}, {"x": "2021-09-12", "y": 0.831, "name": "sMLPNet-S ", "nameShort": "sMLPNet-S ", "nameDetails": "ImageNet-1k", "paperSlug": "sparse-mlp-for-image-recognition-is-self", "usesAdditionalData": false}, {"x": "2021-09-12", "y": 0.8190000000000001, "name": "sMLPNet-T ", "nameShort": "sMLPNet-T ", "nameDetails": "ImageNet-1k", "paperSlug": "sparse-mlp-for-image-recognition-is-self", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.8290000000000001, "name": "NASViT ", "nameShort": "NASViT ", "nameDetails": "supernet", "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.8220000000000001, "name": "ConvMixer-1536/20", "nameShort": "ConvMixer-1536/20", "nameDetails": null, "paperSlug": "patches-are-all-you-need-1", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.818, "name": "NASViT-A5", "nameShort": "NASViT-A5", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.8140000000000001, "name": "NASViT-A4", "nameShort": "NASViT-A4", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.81, "name": "NASViT-A3", "nameShort": "NASViT-A3", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.805, "name": "NASViT-A2", "nameShort": "NASViT-A2", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.797, "name": "NASViT-A1", "nameShort": "NASViT-A1", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.782, "name": "NASViT-A0", "nameShort": "NASViT-A0", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.8240000000000001, "name": "ResNet-152 ", "nameShort": "ResNet-152 ", "nameDetails": "A2 + reg", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.818, "name": "ResNet-152 ", "nameShort": "ResNet-152 ", "nameDetails": "A2", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.804, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "T2", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.804, "name": "ResNet50 ", "nameShort": "ResNet50 ", "nameDetails": "A1", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.7809999999999999, "name": "ResNet50 ", "nameShort": "ResNet50 ", "nameDetails": "A3", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-05", "y": 0.784, "name": "MobileViT-S", "nameShort": "MobileViT-S", "nameDetails": null, "paperSlug": "mobilevit-light-weight-general-purpose-and", "usesAdditionalData": false}, {"x": "2021-10-05", "y": 0.748, "name": "MobileViT-XS", "nameShort": "MobileViT-XS", "nameDetails": null, "paperSlug": "mobilevit-light-weight-general-purpose-and", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.852, "name": "UniNet-B5", "nameShort": "UniNet-B5", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.8420000000000001, "name": "UniNet-B4", "nameShort": "UniNet-B4", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.8270000000000001, "name": "UniNet-B2", "nameShort": "UniNet-B2", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.804, "name": "UniNet-B1", "nameShort": "UniNet-B1", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.7909999999999999, "name": "UniNet-B0", "nameShort": "UniNet-B0", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-18", "y": 0.828, "name": "HRFormer-B", "nameShort": "HRFormer-B", "nameDetails": null, "paperSlug": "hrformer-high-resolution-transformer-for", "usesAdditionalData": false}, {"x": "2021-10-18", "y": 0.785, "name": "HRFormer-T", "nameShort": "HRFormer-T", "nameDetails": null, "paperSlug": "hrformer-high-resolution-transformer-for", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.848, "name": "SReT-B ", "nameShort": "SReT-B ", "nameDetails": "384 res, ImageNet-1K only", "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.843, "name": "SReT-S ", "nameShort": "SReT-S ", "nameDetails": "512 res, ImageNet-1K only", "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.838, "name": "SReT-S ", "nameShort": "SReT-S ", "nameDetails": "384 res, ImageNet-1K only", "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.7759999999999999, "name": "SReT-T", "nameShort": "SReT-T", "nameDetails": null, "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.74, "name": "SReT-ExT", "nameShort": "SReT-ExT", "nameDetails": null, "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.878, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-H, 448", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.8690000000000001, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-H", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.8590000000000001, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-L", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.836, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-L", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-18", "y": 0.9017000000000001, "name": "SwinV2-G", "nameShort": "SwinV2-G", "nameDetails": null, "paperSlug": "swin-transformer-v2-scaling-up-capacity-and", "usesAdditionalData": false}, {"x": "2021-11-18", "y": 0.871, "name": "SwinV2-B", "nameShort": "SwinV2-B", "nameDetails": null, "paperSlug": "swin-transformer-v2-scaling-up-capacity-and", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.841, "name": "FBNetV5-F-CLS", "nameShort": "FBNetV5-F-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.826, "name": "FBNetV5-C-CLS", "nameShort": "FBNetV5-C-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.818, "name": "FBNetV5", "nameShort": "FBNetV5", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.8170000000000001, "name": "FBNetV5-A-CLS", "nameShort": "FBNetV5-A-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.784, "name": "FBNetV5-AC-CLS", "nameShort": "FBNetV5-AC-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.772, "name": "FBNetV5-AR-CLS", "nameShort": "FBNetV5-AR-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-20", "y": 0.8506999999999999, "name": "DiscreteViT", "nameShort": "DiscreteViT", "nameDetails": null, "paperSlug": "discrete-representations-strengthen-vision-1", "usesAdditionalData": false}, {"x": "2021-11-22", "y": 0.9005, "name": "Florence-CoSwin-H", "nameShort": "Florence-CoSwin-H", "nameDetails": null, "paperSlug": "florence-a-new-foundation-model-for-computer", "usesAdditionalData": false}, {"x": "2021-11-22", "y": 0.825, "name": "MetaFormer PoolFormer-M48", "nameShort": "MetaFormer PoolFormer-M48", "nameDetails": null, "paperSlug": "metaformer-is-actually-what-you-need-for", "usesAdditionalData": false}, {"x": "2021-11-24", "y": 0.883, "name": "PeCo ", "nameShort": "PeCo ", "nameDetails": "ViT-H, 448", "paperSlug": "peco-perceptual-codebook-for-bert-pre", "usesAdditionalData": false}, {"x": "2021-11-24", "y": 0.875, "name": "PeCo ", "nameShort": "PeCo ", "nameDetails": "ViT-H, 224", "paperSlug": "peco-perceptual-codebook-for-bert-pre", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.8108, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.7940999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.7635, "name": "ResNet-34 ", "nameShort": "ResNet-34 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.7232999999999999, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.888, "name": "MViTv2-H ", "nameShort": "MViTv2-H ", "nameDetails": "512 res, ImageNet-21k pretrain", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.884, "name": "MViTv2-L ", "nameShort": "MViTv2-L ", "nameDetails": "384 res, ImageNet-21k pretrain", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.88, "name": "MViTv2-H ", "nameShort": "MViTv2-H ", "nameDetails": "mageNet-21k pretrain", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.863, "name": "MViTv2-L ", "nameShort": "MViTv2-L ", "nameDetails": "384 res", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.823, "name": "MViTv2-T", "nameShort": "MViTv2-T", "nameDetails": null, "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.8190000000000001, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "224 res, Fast Knowledge Distillation", "paperSlug": "a-fast-knowledge-distillation-framework-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.8009999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "224 res, Fast Knowledge Distillation", "paperSlug": "a-fast-knowledge-distillation-framework-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.787, "name": "SReT-LT ", "nameShort": "SReT-LT ", "nameDetails": "Fast Knowledge Distillation", "paperSlug": "a-fast-knowledge-distillation-framework-for", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.8370000000000001, "name": "QnA-ViT-Base", "nameShort": "QnA-ViT-Base", "nameDetails": null, "paperSlug": "learned-queries-for-efficient-local-attention", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.8320000000000001, "name": "QnA-ViT-Small", "nameShort": "QnA-ViT-Small", "nameDetails": null, "paperSlug": "learned-queries-for-efficient-local-attention", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.818, "name": "RepMLPNet-L256", "nameShort": "RepMLPNet-L256", "nameDetails": null, "paperSlug": "repmlpnet-hierarchical-vision-mlp-with-re", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.8170000000000001, "name": "QnA-ViT-Tiny", "nameShort": "QnA-ViT-Tiny", "nameDetails": null, "paperSlug": "learned-queries-for-efficient-local-attention", "usesAdditionalData": false}, {"x": "2021-12-23", "y": 0.872, "name": "ELSA-VOLO-D5 ", "nameShort": "ELSA-VOLO-D5 ", "nameDetails": "512*512", "paperSlug": "elsa-enhanced-local-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-12-23", "y": 0.847, "name": "ELSA-VOLO-D1", "nameShort": "ELSA-VOLO-D1", "nameDetails": null, "paperSlug": "elsa-enhanced-local-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-12-23", "y": 0.8270000000000001, "name": "ELSA-Swin-T", "nameShort": "ELSA-Swin-T", "nameDetails": null, "paperSlug": "elsa-enhanced-local-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.871, "name": "PatchConvNet-L120-21k-384", "nameShort": "PatchConvNet-L120-21k-384", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.865, "name": "PatchConvNet-B60-21k-384", "nameShort": "PatchConvNet-B60-21k-384", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.8540000000000001, "name": "PatchConvNet-S60-21k-512", "nameShort": "PatchConvNet-S60-21k-512", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.841, "name": "PatchConvNet-B120", "nameShort": "PatchConvNet-B120", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.835, "name": "PatchConvNet-B60", "nameShort": "PatchConvNet-B60", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.8320000000000001, "name": "PatchConvNet-S120", "nameShort": "PatchConvNet-S120", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.821, "name": "PatchConvNet-S60", "nameShort": "PatchConvNet-S60", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2022-01-03", "y": 0.848, "name": "DAT-B ", "nameShort": "DAT-B ", "nameDetails": "384 res, IN-1K only", "paperSlug": "vision-transformer-with-deformable-attention", "usesAdditionalData": false}, {"x": "2022-01-03", "y": 0.8370000000000001, "name": "DAT-S", "nameShort": "DAT-S", "nameDetails": null, "paperSlug": "vision-transformer-with-deformable-attention", "usesAdditionalData": false}, {"x": "2022-01-03", "y": 0.82, "name": "DAT-T", "nameShort": "DAT-T", "nameDetails": null, "paperSlug": "vision-transformer-with-deformable-attention", "usesAdditionalData": false}, {"x": "2022-01-10", "y": 0.878, "name": "ConvNeXt-XL ", "nameShort": "ConvNeXt-XL ", "nameDetails": "ImageNet-22k", "paperSlug": "a-convnet-for-the-2020s", "usesAdditionalData": false}, {"x": "2022-01-10", "y": 0.855, "name": "ConvNeXt-L ", "nameShort": "ConvNeXt-L ", "nameDetails": "384 res", "paperSlug": "a-convnet-for-the-2020s", "usesAdditionalData": false}, {"x": "2022-01-10", "y": 0.821, "name": "ConvNeXt-T", "nameShort": "ConvNeXt-T", "nameDetails": null, "paperSlug": "a-convnet-for-the-2020s", "usesAdditionalData": false}, {"x": "2022-01-20", "y": 0.8859999999999999, "name": "SWAG ", "nameShort": "SWAG ", "nameDetails": "ViT H/14", "paperSlug": "revisiting-weakly-supervised-pre-training-of", "usesAdditionalData": false}, {"x": "2022-01-20", "y": 0.86, "name": "Omnivore ", "nameShort": "Omnivore ", "nameDetails": "Swin-L", "paperSlug": "omnivore-a-single-model-for-many-visual", "usesAdditionalData": false}, {"x": "2022-01-20", "y": 0.853, "name": "Omnivore ", "nameShort": "Omnivore ", "nameDetails": "Swin-B", "paperSlug": "omnivore-a-single-model-for-many-visual", "usesAdditionalData": false}, {"x": "2022-01-24", "y": 0.863, "name": "UniFormer-L ", "nameShort": "UniFormer-L ", "nameDetails": "384 res", "paperSlug": "uniformer-unifying-convolution-and-self", "usesAdditionalData": false}, {"x": "2022-01-24", "y": 0.856, "name": "UniFormer-L", "nameShort": "UniFormer-L", "nameDetails": null, "paperSlug": "uniformer-unifying-convolution-and-self", "usesAdditionalData": false}, {"x": "2022-01-24", "y": 0.8340000000000001, "name": "UniFormer-S", "nameShort": "UniFormer-S", "nameDetails": null, "paperSlug": "uniformer-unifying-convolution-and-self", "usesAdditionalData": false}, {"x": "2022-01-26", "y": 0.833, "name": "Shift-B", "nameShort": "Shift-B", "nameDetails": null, "paperSlug": "when-shift-operation-meets-vision-transformer", "usesAdditionalData": false}, {"x": "2022-01-26", "y": 0.828, "name": "Shift-S", "nameShort": "Shift-S", "nameDetails": null, "paperSlug": "when-shift-operation-meets-vision-transformer", "usesAdditionalData": false}, {"x": "2022-01-26", "y": 0.8170000000000001, "name": "Shift-T", "nameShort": "Shift-T", "nameDetails": null, "paperSlug": "when-shift-operation-meets-vision-transformer", "usesAdditionalData": false}, {"x": "2022-02-07", "y": 0.866, "name": "data2vec", "nameShort": "data2vec", "nameDetails": null, "paperSlug": "data2vec-a-general-framework-for-self-1", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.865, "name": "MKD ViT-L", "nameShort": "MKD ViT-L", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.858, "name": "SEER ", "nameShort": "SEER ", "nameDetails": "RG-10B", "paperSlug": "vision-models-are-more-robust-and-fair-when", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.851, "name": "MKD ViT-B", "nameShort": "MKD ViT-B", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.831, "name": "MKD ViT-S", "nameShort": "MKD ViT-S", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.7709999999999999, "name": "MKD ViT-T", "nameShort": "MKD ViT-T", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.878, "name": "VAN-B6 ", "nameShort": "VAN-B6 ", "nameDetails": "22K, 384res", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.87, "name": "VAN-B5 ", "nameShort": "VAN-B5 ", "nameDetails": "22K, 384res", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.866, "name": "VAN-B4 ", "nameShort": "VAN-B4 ", "nameDetails": "22K, 384res", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.863, "name": "VAN-B5 ", "nameShort": "VAN-B5 ", "nameDetails": "22K", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.857, "name": "VAN-B4 ", "nameShort": "VAN-B4 ", "nameDetails": "22K", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.828, "name": "VAN-B2", "nameShort": "VAN-B2", "nameDetails": null, "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.8109999999999999, "name": "VAN-B1", "nameShort": "VAN-B1", "nameDetails": null, "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.754, "name": "VAN-B0", "nameShort": "VAN-B0", "nameDetails": null, "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-21", "y": 0.885, "name": "ViTAE-H + MAE ", "nameShort": "ViTAE-H + MAE ", "nameDetails": "448", "paperSlug": "vitaev2-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2022-03-08", "y": 0.7863, "name": "EdgeFormer-S", "nameShort": "EdgeFormer-S", "nameDetails": null, "paperSlug": "edgeformer-improving-light-weight-convnets-by", "usesAdditionalData": false}, {"x": "2022-03-10", "y": 0.9094, "name": "Model soups ", "nameShort": "Model soups ", "nameDetails": "ViT-G/14", "paperSlug": "model-soups-averaging-weights-of-multiple", "usesAdditionalData": false}, {"x": "2022-03-11", "y": 0.836, "name": "ActiveMLP-L", "nameShort": "ActiveMLP-L", "nameDetails": null, "paperSlug": "activemlp-an-mlp-like-architecture-with", "usesAdditionalData": false}, {"x": "2022-03-11", "y": 0.82, "name": "ActiveMLP-T", "nameShort": "ActiveMLP-T", "nameDetails": null, "paperSlug": "activemlp-an-mlp-like-architecture-with", "usesAdditionalData": false}, {"x": "2022-03-13", "y": 0.878, "name": "RepLKNet-XL", "nameShort": "RepLKNet-XL", "nameDetails": null, "paperSlug": "scaling-up-your-kernels-to-31x31-revisiting", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.855, "name": "ViT-L@384 ", "nameShort": "ViT-L@384 ", "nameDetails": "attn finetune", "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.843, "name": "ViT-B@384 ", "nameShort": "ViT-B@384 ", "nameDetails": "attn finetune", "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.841, "name": "ViT-B-36x1", "nameShort": "ViT-B-36x1", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.841, "name": "ViT-B-18x2", "nameShort": "ViT-B-18x2", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.8340000000000001, "name": "ViT-B ", "nameShort": "ViT-B ", "nameDetails": "hMLP + BeiT", "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.826, "name": "ViT-S-24x2", "nameShort": "ViT-S-24x2", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.823, "name": "ViT-S-48x1", "nameShort": "ViT-S-48x1", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8197, "name": "Swin-T ", "nameShort": "Swin-T ", "nameDetails": "SAMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8192, "name": "Swin-T ", "nameShort": "Swin-T ", "nameDetails": "AutoMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8116, "name": "Swin-T ", "nameShort": "Swin-T ", "nameDetails": "PuzzleMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8112, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "SAMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8090999999999999, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "AutoMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8025, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "PuzzleMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.7875, "name": "SAMix+DM ", "nameShort": "SAMix+DM ", "nameDetails": "ResNet-50 RSB A3", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.7862, "name": "AutoMix+DM ", "nameShort": "AutoMix+DM ", "nameDetails": "ResNet-50 RSB A3", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.7815000000000001, "name": "PuzzleMix+DM ", "nameShort": "PuzzleMix+DM ", "nameDetails": "ResNet-50 RSB A3", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.2229, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "PuzzleMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.21850000000000003, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "AutoMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.2164, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "SAMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-29", "y": 0.7613, "name": "ELP ", "nameShort": "ELP ", "nameDetails": "naive ResNet50", "paperSlug": "a-simple-episodic-linear-probe-improves", "usesAdditionalData": false}, {"x": "2022-04-03", "y": 0.873, "name": "VOLO-D5+HAT", "nameShort": "VOLO-D5+HAT", "nameDetails": null, "paperSlug": "improving-vision-transformers-by-revisiting", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8953, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "512res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8941, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "384res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8941, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "512res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8912, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "384res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8881999999999999, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "512res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.887, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "512res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8869, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "384res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8851, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "384res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8845999999999999, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "512res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8837999999999999, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "512res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8832, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "384res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8824, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "384res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.867, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "512res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8640000000000001, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "384res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8634000000000001, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "384res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8619, "name": "MaxViT-S ", "nameShort": "MaxViT-S ", "nameDetails": "512res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8572, "name": "MaxViT-T", "nameShort": "MaxViT-T", "nameDetails": "512res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8523999999999999, "name": "MaxViT-T ", "nameShort": "MaxViT-T ", "nameDetails": "384res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8495, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "224res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8445, "name": "MaxViT-S ", "nameShort": "MaxViT-S ", "nameDetails": "224res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8362, "name": "MaxViT-T ", "nameShort": "MaxViT-T ", "nameDetails": "224res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.904, "name": "DaViT-G", "nameShort": "DaViT-G", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.902, "name": "DaViT-H", "nameShort": "DaViT-H", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.875, "name": "DaViT-L ", "nameShort": "DaViT-L ", "nameDetails": "ImageNet-22k", "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.8690000000000001, "name": "DaViT-B ", "nameShort": "DaViT-B ", "nameDetails": "ImageNet-22k", "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.846, "name": "DaViT-B", "nameShort": "DaViT-B", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.8420000000000001, "name": "DaViT-S", "nameShort": "DaViT-S", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.828, "name": "DaViT-T", "nameShort": "DaViT-T", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.867, "name": "ViT-B @384 ", "nameShort": "ViT-B @384 ", "nameDetails": "DeiT III, 21k", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.858, "name": "ViT-L @384 ", "nameShort": "ViT-L @384 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.857, "name": "ViT-B @224 ", "nameShort": "ViT-B @224 ", "nameDetails": "DeiT III, 21k", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.855, "name": "Mini-Swin-B@384", "nameShort": "Mini-Swin-B@384", "nameDetails": null, "paperSlug": "minivit-compressing-vision-transformers-with", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.852, "name": "ViT-H @224 ", "nameShort": "ViT-H @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.85, "name": "ViT-B @384 ", "nameShort": "ViT-B @384 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8490000000000001, "name": "ViT-L @224 ", "nameShort": "ViT-L @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.843, "name": "NAT-Base", "nameShort": "NAT-Base", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.838, "name": "ViT-B @224 ", "nameShort": "ViT-B @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8370000000000001, "name": "NAT-Small", "nameShort": "NAT-Small", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8340000000000001, "name": "ViT-S @384 ", "nameShort": "ViT-S @384 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8320000000000001, "name": "NAT-Tiny", "nameShort": "NAT-Tiny", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.831, "name": "ViT-S @224 ", "nameShort": "ViT-S @224 ", "nameDetails": "DeiT III, 21k", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.818, "name": "NAT-Mini", "nameShort": "NAT-Mini", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8140000000000001, "name": "ViT-S @224 ", "nameShort": "ViT-S @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-26", "y": 0.872, "name": "EfficientNetV2 ", "nameShort": "EfficientNetV2 ", "nameDetails": "PolyLoss", "paperSlug": "polyloss-a-polynomial-expansion-perspective-1", "usesAdditionalData": false}, {"x": "2022-04-26", "y": 0.871, "name": "FAN-L-Hybrid++", "nameShort": "FAN-L-Hybrid++", "nameDetails": null, "paperSlug": "understanding-the-robustness-in-vision", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.9059999999999999, "name": "CoCa ", "nameShort": "CoCa ", "nameDetails": "frozen", "paperSlug": "coca-contrastive-captioners-are-image-text", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.846, "name": "Sequencer2D-L\u2191392", "nameShort": "Sequencer2D-L\u2191392", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.8340000000000001, "name": "Sequencer2D-L", "nameShort": "Sequencer2D-L", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.828, "name": "Sequencer2D-M", "nameShort": "Sequencer2D-M", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.823, "name": "Sequencer2D-S", "nameShort": "Sequencer2D-S", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8661, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:ViT+D:EffNet-B7", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8645999999999999, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:ViT+D:VOLO-D3", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8642, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:ConvNeXt-L+D:EffNet-B7", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8528, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:D1+D:D5", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8388, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:B4+D:B7", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-21", "y": 0.871, "name": "Bamboo ", "nameShort": "Bamboo ", "nameDetails": "Bamboo-H", "paperSlug": "deeper-vs-wider-a-revisit-of-transformer", "usesAdditionalData": false}, {"x": "2022-05-21", "y": 0.863, "name": "Bamboo ", "nameShort": "Bamboo ", "nameDetails": "Bamboo-L", "paperSlug": "deeper-vs-wider-a-revisit-of-transformer", "usesAdditionalData": false}, {"x": "2022-05-21", "y": 0.8420000000000001, "name": "Bamboo ", "nameShort": "Bamboo ", "nameDetails": "Bamboo-B", "paperSlug": "deeper-vs-wider-a-revisit-of-transformer", "usesAdditionalData": false}, {"x": "2022-05-25", "y": 0.8674, "name": "\u00b52Net ", "nameShort": "\u00b52Net ", "nameDetails": "ViT-L/16", "paperSlug": "an-evolutionary-approach-to-dynamic", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.851, "name": "MixMIM-B", "nameShort": "MixMIM-B", "nameDetails": null, "paperSlug": "mixmim-mixed-and-masked-image-modeling-for", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.847, "name": "LITv2-B|384", "nameShort": "LITv2-B|384", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8367, "name": "TransBoost-ViT-S", "nameShort": "TransBoost-ViT-S", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.836, "name": "LITv2-B", "nameShort": "LITv2-B", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.833, "name": "LITv2-M", "nameShort": "LITv2-M", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8245999999999999, "name": "TransBoost-ConvNext-T", "nameShort": "TransBoost-ConvNext-T", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8216, "name": "TransBoost-Swin-T", "nameShort": "TransBoost-Swin-T", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.82, "name": "LITv2-S", "nameShort": "LITv2-S", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8115000000000001, "name": "TransBoost-ResNet50-StrikesBack", "nameShort": "TransBoost-ResNet50-StrikesBack", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8064, "name": "TransBoost-ResNet152", "nameShort": "TransBoost-ResNet152", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7986, "name": "TransBoost-ResNet101", "nameShort": "TransBoost-ResNet101", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7903, "name": "TransBoost-ResNet50", "nameShort": "TransBoost-ResNet50", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7859999999999999, "name": "TransBoost-EfficientNetB0", "nameShort": "TransBoost-EfficientNetB0", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7681, "name": "TransBoost-MobileNetV3-L", "nameShort": "TransBoost-MobileNetV3-L", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.767, "name": "TransBoost-ResNet34", "nameShort": "TransBoost-ResNet34", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7336, "name": "TransBoost-ResNet18", "nameShort": "TransBoost-ResNet18", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-27", "y": 0.89, "name": "FD ", "nameShort": "FD ", "nameDetails": "CLIP ViT-L-336", "paperSlug": "contrastive-learning-rivals-masked-image", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.8370000000000001, "name": "Pyramid ViG-B", "nameShort": "Pyramid ViG-B", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.831, "name": "Pyramid ViG-M", "nameShort": "Pyramid ViG-M", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.821, "name": "Pyramid ViG-S", "nameShort": "Pyramid ViG-S", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.782, "name": "Pyramid ViG-Ti", "nameShort": "Pyramid ViG-Ti", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.845, "name": "GC ViT-B", "nameShort": "GC ViT-B", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.84, "name": "GC ViT-S", "nameShort": "GC ViT-S", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.8340000000000001, "name": "GC ViT-T", "nameShort": "GC ViT-T", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.82, "name": "GC ViT-XT", "nameShort": "GC ViT-XT", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.7979999999999999, "name": "GC ViT-XXT", "nameShort": "GC ViT-XXT", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-21", "y": 0.794, "name": "EdgeNeXt-S", "nameShort": "EdgeNeXt-S", "nameDetails": null, "paperSlug": "edgenext-efficiently-amalgamated-cnn", "usesAdditionalData": false}, {"x": "2022-06-21", "y": 0.7120000000000001, "name": "EdgeNeXt-XXS", "nameShort": "EdgeNeXt-XXS", "nameDetails": null, "paperSlug": "edgenext-efficiently-amalgamated-cnn", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.8420000000000001, "name": "RevBiFPN-S6", "nameShort": "RevBiFPN-S6", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.8370000000000001, "name": "RevBiFPN-S5", "nameShort": "RevBiFPN-S5", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.83, "name": "RevBiFPN-S4", "nameShort": "RevBiFPN-S4", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.8109999999999999, "name": "RevBiFPN-S3", "nameShort": "RevBiFPN-S3", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.79, "name": "RevBiFPN-S2", "nameShort": "RevBiFPN-S2", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.759, "name": "RevBiFPN-S1", "nameShort": "RevBiFPN-S1", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.728, "name": "RevBiFPN-S0", "nameShort": "RevBiFPN-S0", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-07-11", "y": 0.855, "name": "Wave-ViT-L", "nameShort": "Wave-ViT-L", "nameDetails": null, "paperSlug": "wave-vit-unifying-wavelet-and-transformers", "usesAdditionalData": false}, {"x": "2022-07-11", "y": 0.848, "name": "Wave-ViT-B", "nameShort": "Wave-ViT-B", "nameDetails": null, "paperSlug": "wave-vit-unifying-wavelet-and-transformers", "usesAdditionalData": false}, {"x": "2022-07-11", "y": 0.8390000000000001, "name": "Wave-ViT-S", "nameShort": "Wave-ViT-S", "nameDetails": null, "paperSlug": "wave-vit-unifying-wavelet-and-transformers", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.8740000000000001, "name": "UniNet-B6", "nameShort": "UniNet-B6", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with-1", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.87, "name": "UniNet-B5", "nameShort": "UniNet-B5", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with-1", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.847, "name": "Next-ViT-L @384", "nameShort": "Next-ViT-L @384", "nameDetails": null, "paperSlug": "next-vit-next-generation-vision-transformer", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.8320000000000001, "name": "Next-ViT-B", "nameShort": "Next-ViT-B", "nameDetails": null, "paperSlug": "next-vit-next-generation-vision-transformer", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.825, "name": "Next-ViT-S", "nameShort": "Next-ViT-S", "nameDetails": null, "paperSlug": "next-vit-next-generation-vision-transformer", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.8079999999999999, "name": "UniNet-B0", "nameShort": "UniNet-B0", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with-1", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.865, "name": "TinyViT-21M-512-distill ", "nameShort": "TinyViT-21M-512-distill ", "nameDetails": "512 res, 21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.862, "name": "TinyViT-21M-384-distill ", "nameShort": "TinyViT-21M-384-distill ", "nameDetails": "384 res, 21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.848, "name": "TinyViT-21M-distill ", "nameShort": "TinyViT-21M-distill ", "nameDetails": "21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.8320000000000001, "name": "TinyViT-11M-distill ", "nameShort": "TinyViT-11M-distill ", "nameDetails": "21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.831, "name": "TinyViT-21M", "nameShort": "TinyViT-21M", "nameDetails": null, "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.815, "name": "TinyViT-11M", "nameShort": "TinyViT-11M", "nameDetails": null, "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.807, "name": "TinyViT-5M-distill ", "nameShort": "TinyViT-5M-distill ", "nameDetails": "21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.7909999999999999, "name": "TinyViT-5M", "nameShort": "TinyViT-5M", "nameDetails": null, "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-28", "y": 0.877, "name": "HorNet-L ", "nameShort": "HorNet-L ", "nameDetails": "GF", "paperSlug": "hornet-efficient-high-order-spatial", "usesAdditionalData": false}, {"x": "2022-08-24", "y": 0.8301000000000001, "name": "gSwin-S", "nameShort": "gSwin-S", "nameDetails": null, "paperSlug": "gswin-gated-mlp-vision-model-with", "usesAdditionalData": false}, {"x": "2022-08-24", "y": 0.8170999999999999, "name": "gSwin-T", "nameShort": "gSwin-T", "nameDetails": null, "paperSlug": "gswin-gated-mlp-vision-model-with", "usesAdditionalData": false}, {"x": "2022-08-24", "y": 0.8031999999999999, "name": "gSwin-VT", "nameShort": "gSwin-VT", "nameDetails": null, "paperSlug": "gswin-gated-mlp-vision-model-with", "usesAdditionalData": false}, {"x": "2022-09-08", "y": 0.882, "name": "dBOT ViT-H ", "nameShort": "dBOT ViT-H ", "nameDetails": "CLIP as Teacher", "paperSlug": "exploring-target-representations-for-masked", "usesAdditionalData": false}, {"x": "2022-09-08", "y": 0.878, "name": "dBOT ViT-L ", "nameShort": "dBOT ViT-L ", "nameDetails": "CLIP as Teacher", "paperSlug": "exploring-target-representations-for-masked", "usesAdditionalData": false}, {"x": "2022-09-08", "y": 0.857, "name": "dBOT ViT-B ", "nameShort": "dBOT ViT-B ", "nameDetails": "CLIP as Teacher", "paperSlug": "exploring-target-representations-for-masked", "usesAdditionalData": false}, {"x": "2022-09-14", "y": 0.909, "name": "ViT-e", "nameShort": "ViT-e", "nameDetails": null, "paperSlug": "pali-a-jointly-scaled-multilingual-language", "usesAdditionalData": false}, {"x": "2022-09-16", "y": 0.8702, "name": "MAE+DAT ", "nameShort": "MAE+DAT ", "nameDetails": "ViT-H", "paperSlug": "enhance-the-visual-representation-via", "usesAdditionalData": false}, {"x": "2022-09-21", "y": 0.8240000000000001, "name": "Mega", "nameShort": "Mega", "nameDetails": null, "paperSlug": "mega-moving-average-equipped-gated-attention", "usesAdditionalData": false}, {"x": "2022-09-26", "y": 0.8601000000000001, "name": "GPaCo ", "nameShort": "GPaCo ", "nameDetails": "ViT-L", "paperSlug": "generalized-parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2022-09-26", "y": 0.84, "name": "GPaCo ", "nameShort": "GPaCo ", "nameDetails": "Vit-B", "paperSlug": "generalized-parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2022-09-26", "y": 0.797, "name": "GPaCo ", "nameShort": "GPaCo ", "nameDetails": "ResNet-50", "paperSlug": "generalized-parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8740000000000001, "name": "DiNAT_s-Large ", "nameShort": "DiNAT_s-Large ", "nameDetails": "384res; Pretrained on IN22K@224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8731, "name": "DiNAT-Large ", "nameShort": "DiNAT-Large ", "nameDetails": "11x11ks; 384res; Pretrained on IN22K@224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8718, "name": "DiNAT-Large ", "nameShort": "DiNAT-Large ", "nameDetails": "384x384; Pretrained on ImageNet-22K @ 224x224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.865, "name": "DiNAT_s-Large ", "nameShort": "DiNAT_s-Large ", "nameDetails": "224x224; Pretrained on ImageNet-22K @ 224x224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8440000000000001, "name": "DiNAT-Base", "nameShort": "DiNAT-Base", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.838, "name": "DiNAT-Small", "nameShort": "DiNAT-Small", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8270000000000001, "name": "DiNAT-Tiny", "nameShort": "DiNAT-Tiny", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.818, "name": "DiNAT-Mini", "nameShort": "DiNAT-Mini", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7929999999999999, "name": "MobileViTv3-S", "nameShort": "MobileViTv3-S", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7864, "name": "MobileViTv3-1.0", "nameShort": "MobileViTv3-1.0", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.767, "name": "MobileViTv3-XS", "nameShort": "MobileViTv3-XS", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7655, "name": "MobileViTv3-0.75", "nameShort": "MobileViTv3-0.75", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7232999999999999, "name": "MobileViTv3-0.5", "nameShort": "MobileViTv3-0.5", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7098, "name": "MobileViTv3-XXS", "nameShort": "MobileViTv3-XXS", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-10-04", "y": 0.8909999999999999, "name": "MOAT-4 22K+1K", "nameShort": "MOAT-4 22K+1K", "nameDetails": null, "paperSlug": "moat-alternating-mobile-convolution-and", "usesAdditionalData": false}, {"x": "2022-10-04", "y": 0.867, "name": "MOAT-3 1K only", "nameShort": "MOAT-3 1K only", "nameDetails": null, "paperSlug": "moat-alternating-mobile-convolution-and", "usesAdditionalData": false}, {"x": "2022-10-04", "y": 0.833, "name": "MOAT-0 1K only", "nameShort": "MOAT-0 1K only", "nameDetails": null, "paperSlug": "moat-alternating-mobile-convolution-and", "usesAdditionalData": false}, {"x": "2022-10-13", "y": 0.677, "name": "WaveMixLite-256/24", "nameShort": "WaveMixLite-256/24", "nameDetails": null, "paperSlug": "wavemix-lite-a-resource-efficient-neural-1", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.846, "name": "MogaNet-L", "nameShort": "MogaNet-L", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.8420000000000001, "name": "MogaNet-B", "nameShort": "MogaNet-B", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.8340000000000001, "name": "MogaNet-S", "nameShort": "MogaNet-S", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.8, "name": "MogaNet-T ", "nameShort": "MogaNet-T ", "nameDetails": "256res", "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.79, "name": "MogaNet-T", "nameShort": "MogaNet-T", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.765, "name": "MogaNet-XT", "nameShort": "MogaNet-XT", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-10", "y": 0.892, "name": "InternImage-DCNv3-H", "nameShort": "InternImage-DCNv3-H", "nameDetails": null, "paperSlug": "internimage-exploring-large-scale-vision", "usesAdditionalData": false}], "yValueFormatString": "#.# %"}}}, "uses_additional_data": {"yAxis": {"title": "Top 1 Accuracy", "includeZero": false, "gridColor": "#ddd", "valueFormatString": "# %"}, "data": {"trend": {"name": "State-of-the-art methods", "type": "line", "showInLegend": true, "markerSize": 10, "toolTipContent": "{name}: {y}", "color": "#21ccc7", "dataPoints": [], "yValueFormatString": "#.# %"}, "other": {"name": "Other methods", "type": "scatter", "showInLegend": true, "color": "#ddd", "markerSize": 10, "toolTipContent": "{name}: {y}", "dataPoints": [], "yValueFormatString": "#.# %"}}}, "no_additional_data": {"yAxis": {"title": "Top 1 Accuracy", "includeZero": false, "gridColor": "#ddd", "valueFormatString": "# %", "minimum": 0.14704, "maximum": 0.9793600000000001}, "data": {"trend": {"name": "State-of-the-art methods", "type": "line", "showInLegend": true, "markerSize": 10, "toolTipContent": "{name}: {y}", "color": "#21ccc7", "dataPoints": [{"x": "2012-12-01", "y": 0.633, "name": "AlexNet", "nameShort": "AlexNet", "nameDetails": null, "paperSlug": "imagenet-classification-with-deep", "usesAdditionalData": false}, {"x": "2013-11-12", "y": 0.64, "name": "ZFNet ", "nameShort": "ZFNet ", "nameDetails": "ensemble, 6 convnets", "paperSlug": "visualizing-and-understanding-convolutional", "usesAdditionalData": false}, {"x": "2013-12-19", "y": 0.6629999999999999, "name": "Five Base + Five HiRes", "nameShort": "Five Base + Five HiRes", "nameDetails": null, "paperSlug": "some-improvements-on-deep-convolutional", "usesAdditionalData": false}, {"x": "2014-06-18", "y": 0.7031999999999999, "name": "MSRA", "nameShort": "MSRA", "nameDetails": null, "paperSlug": "spatial-pyramid-pooling-in-deep-convolutional", "usesAdditionalData": false}, {"x": "2014-09-04", "y": 0.745, "name": "VGG-19", "nameShort": "VGG-19", "nameDetails": null, "paperSlug": "very-deep-convolutional-networks-for-large", "usesAdditionalData": false}, {"x": "2015-02-11", "y": 0.748, "name": "Inception V2", "nameShort": "Inception V2", "nameDetails": null, "paperSlug": "batch-normalization-accelerating-deep-network", "usesAdditionalData": false}, {"x": "2015-12-02", "y": 0.7879999999999999, "name": "Inception V3", "nameShort": "Inception V3", "nameDetails": null, "paperSlug": "rethinking-the-inception-architecture-for", "usesAdditionalData": false}, {"x": "2016-02-23", "y": 0.8009999999999999, "name": "Inception ResNet V2", "nameShort": "Inception ResNet V2", "nameDetails": null, "paperSlug": "inception-v4-inception-resnet-and-the-impact", "usesAdditionalData": false}, {"x": "2016-11-16", "y": 0.809, "name": "ResNeXt-101  64x4", "nameShort": "ResNeXt-101  64x4", "nameDetails": null, "paperSlug": "aggregated-residual-transformations-for-deep", "usesAdditionalData": false}, {"x": "2017-07-21", "y": 0.8270000000000001, "name": "NASNET-A", "nameShort": "NASNET-A", "nameDetails": "6", "paperSlug": "learning-transferable-architectures-for", "usesAdditionalData": false}, {"x": "2017-12-02", "y": 0.8290000000000001, "name": "PNASNet-5", "nameShort": "PNASNet-5", "nameDetails": null, "paperSlug": "progressive-neural-architecture-search", "usesAdditionalData": false}, {"x": "2018-02-05", "y": 0.8390000000000001, "name": "AmoebaNet-A", "nameShort": "AmoebaNet-A", "nameDetails": null, "paperSlug": "regularized-evolution-for-image-classifier", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.8540000000000001, "name": "ResNeXt-101 32x48d", "nameShort": "ResNeXt-101 32x48d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.8640000000000001, "name": "FixResNeXt-101 32x48d", "nameShort": "FixResNeXt-101 32x48d", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.8690000000000001, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B7", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.8754000000000001, "name": "BiT-L ", "nameShort": "BiT-L ", "nameDetails": "ResNet", "paperSlug": "large-scale-learning-of-general-visual", "usesAdditionalData": false}, {"x": "2020-01-07", "y": 0.884, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-L2", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.885, "name": "FixEfficientNet-L2", "nameShort": "FixEfficientNet-L2", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-23", "y": 0.902, "name": "Meta Pseudo Labels ", "nameShort": "Meta Pseudo Labels ", "nameDetails": "EfficientNet-L2", "paperSlug": "meta-pseudo-labels", "usesAdditionalData": false}, {"x": "2021-06-08", "y": 0.9045000000000001, "name": "ViT-G/14", "nameShort": "ViT-G/14", "nameDetails": null, "paperSlug": "scaling-vision-transformers", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.9087999999999999, "name": "CoAtNet-7", "nameShort": "CoAtNet-7", "nameDetails": null, "paperSlug": "coatnet-marrying-convolution-and-attention", "usesAdditionalData": false}, {"x": "2022-03-10", "y": 0.9098, "name": "Model soups ", "nameShort": "Model soups ", "nameDetails": "BASIC-L", "paperSlug": "model-soups-averaging-weights-of-multiple", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.91, "name": "CoCa ", "nameShort": "CoCa ", "nameDetails": "finetuned", "paperSlug": "coca-contrastive-captioners-are-image-text", "usesAdditionalData": false}], "yValueFormatString": "#.# %"}, "other": {"name": "Other methods", "type": "scatter", "showInLegend": true, "color": "#ddd", "markerSize": 10, "toolTipContent": "{name}: {y}", "dataPoints": [{"x": "2013-11-12", "y": 0.625, "name": "ZFNet ", "nameShort": "ZFNet ", "nameDetails": "1 convnet, 512,1024,512 maps", "paperSlug": "visualizing-and-understanding-convolutional", "usesAdditionalData": false}, {"x": "2013-12-21", "y": 0.6604000000000001, "name": "OverFeat - 7 accurate models", "nameShort": "OverFeat - 7 accurate models", "nameDetails": null, "paperSlug": "overfeat-integrated-recognition-localization", "usesAdditionalData": false}, {"x": "2014-09-04", "y": 0.7440000000000001, "name": "VGG-16", "nameShort": "VGG-16", "nameDetails": null, "paperSlug": "very-deep-convolutional-networks-for-large", "usesAdditionalData": false}, {"x": "2015-10-31", "y": 0.6829999999999999, "name": "FireCaffe ", "nameShort": "FireCaffe ", "nameDetails": "GoogLeNet", "paperSlug": "firecaffe-near-linear-acceleration-of-deep", "usesAdditionalData": false}, {"x": "2015-10-31", "y": 0.589, "name": "FireCaffe ", "nameShort": "FireCaffe ", "nameDetails": "AlexNet", "paperSlug": "firecaffe-near-linear-acceleration-of-deep", "usesAdditionalData": false}, {"x": "2015-12-10", "y": 0.7857, "name": "ResNet-152", "nameShort": "ResNet-152", "nameDetails": null, "paperSlug": "deep-residual-learning-for-image-recognition", "usesAdditionalData": false}, {"x": "2015-12-10", "y": 0.7825, "name": "ResNet-101", "nameShort": "ResNet-101", "nameDetails": null, "paperSlug": "deep-residual-learning-for-image-recognition", "usesAdditionalData": false}, {"x": "2015-12-10", "y": 0.753, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "deep-residual-learning-for-image-recognition", "usesAdditionalData": false}, {"x": "2016-02-24", "y": 0.604, "name": "SqueezeNet + Simple Bypass", "nameShort": "SqueezeNet + Simple Bypass", "nameDetails": null, "paperSlug": "squeezenet-alexnet-level-accuracy-with-50x", "usesAdditionalData": false}, {"x": "2016-03-16", "y": 0.799, "name": "ResNet-200", "nameShort": "ResNet-200", "nameDetails": null, "paperSlug": "identity-mappings-in-deep-residual-networks", "usesAdditionalData": false}, {"x": "2016-05-23", "y": 0.7809999999999999, "name": "WRN-50-2-bottleneck", "nameShort": "WRN-50-2-bottleneck", "nameDetails": null, "paperSlug": "wide-residual-networks", "usesAdditionalData": false}, {"x": "2016-05-24", "y": 0.7587999999999999, "name": "FractalNet-34", "nameShort": "FractalNet-34", "nameDetails": null, "paperSlug": "fractalnet-ultra-deep-neural-networks-without", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.7785, "name": "DenseNet-264", "nameShort": "DenseNet-264", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.7742, "name": "DenseNet-201", "nameShort": "DenseNet-201", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.762, "name": "DenseNet-169", "nameShort": "DenseNet-169", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-08-25", "y": 0.7498, "name": "DenseNet-121", "nameShort": "DenseNet-121", "nameDetails": null, "paperSlug": "densely-connected-convolutional-networks", "usesAdditionalData": false}, {"x": "2016-10-07", "y": 0.79, "name": "Xception", "nameShort": "Xception", "nameDetails": null, "paperSlug": "xception-deep-learning-with-depthwise", "usesAdditionalData": false}, {"x": "2017-04-17", "y": 0.706, "name": "MobileNet-224 \u00d71.0", "nameShort": "MobileNet-224 \u00d71.0", "nameDetails": null, "paperSlug": "mobilenets-efficient-convolutional-neural", "usesAdditionalData": false}, {"x": "2017-04-23", "y": 0.805, "name": "Attention-92", "nameShort": "Attention-92", "nameDetails": null, "paperSlug": "residual-attention-network-for-image", "usesAdditionalData": false}, {"x": "2017-07-04", "y": 0.7090000000000001, "name": "ShuffleNet", "nameShort": "ShuffleNet", "nameDetails": null, "paperSlug": "shufflenet-an-extremely-efficient", "usesAdditionalData": false}, {"x": "2017-07-10", "y": 0.792, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "JFT-300M Finetuning", "paperSlug": "revisiting-unreasonable-effectiveness-of-data", "usesAdditionalData": false}, {"x": "2018-01-13", "y": 0.747, "name": "MobileNetV2 ", "nameShort": "MobileNetV2 ", "nameDetails": "1.4", "paperSlug": "mobilenetv2-inverted-residuals-and-linear", "usesAdditionalData": false}, {"x": "2018-01-13", "y": 0.72, "name": "MobileNetV2", "nameShort": "MobileNetV2", "nameDetails": null, "paperSlug": "mobilenetv2-inverted-residuals-and-linear", "usesAdditionalData": false}, {"x": "2018-03-14", "y": 0.7894, "name": "ResNet-152 + SWA", "nameShort": "ResNet-152 + SWA", "nameDetails": null, "paperSlug": "averaging-weights-leads-to-wider-optima-and", "usesAdditionalData": false}, {"x": "2018-03-14", "y": 0.7844, "name": "DenseNet-161 + SWA", "nameShort": "DenseNet-161 + SWA", "nameDetails": null, "paperSlug": "averaging-weights-leads-to-wider-optima-and", "usesAdditionalData": false}, {"x": "2018-03-22", "y": 0.7712, "name": "Inception v3", "nameShort": "Inception v3", "nameDetails": null, "paperSlug": "what-do-deep-networks-like-to-see", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.851, "name": "ResNeXt-101 32x32d", "nameShort": "ResNeXt-101 32x32d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.8420000000000001, "name": "ResNeXt-101 32\u00d716d", "nameShort": "ResNeXt-101 32\u00d716d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2018-05-02", "y": 0.8220000000000001, "name": "ResNeXt-101 32x8d", "nameShort": "ResNeXt-101 32x8d", "nameDetails": null, "paperSlug": "exploring-the-limits-of-weakly-supervised", "usesAdditionalData": false}, {"x": "2018-07-09", "y": 0.7574, "name": "CoordConv ResNet-50", "nameShort": "CoordConv ResNet-50", "nameDetails": null, "paperSlug": "an-intriguing-failing-of-convolutional-neural", "usesAdditionalData": false}, {"x": "2018-07-30", "y": 0.754, "name": "ShuffleNet V2", "nameShort": "ShuffleNet V2", "nameDetails": null, "paperSlug": "shufflenet-v2-practical-guidelines-for", "usesAdditionalData": false}, {"x": "2018-07-31", "y": 0.767, "name": "MnasNet-A3", "nameShort": "MnasNet-A3", "nameDetails": null, "paperSlug": "mnasnet-platform-aware-neural-architecture", "usesAdditionalData": false}, {"x": "2018-07-31", "y": 0.7559999999999999, "name": "MnasNet-A2", "nameShort": "MnasNet-A2", "nameDetails": null, "paperSlug": "mnasnet-platform-aware-neural-architecture", "usesAdditionalData": false}, {"x": "2018-07-31", "y": 0.752, "name": "MnasNet-A1", "nameShort": "MnasNet-A1", "nameDetails": null, "paperSlug": "mnasnet-platform-aware-neural-architecture", "usesAdditionalData": false}, {"x": "2018-10-30", "y": 0.7835, "name": "ResNet-50 + DropBlock ", "nameShort": "ResNet-50 + DropBlock ", "nameDetails": "0.9 kp, 0.1 label smoothing", "paperSlug": "dropblock-a-regularization-method-for", "usesAdditionalData": false}, {"x": "2018-11-16", "y": 0.8440000000000001, "name": "GPIPE", "nameShort": "GPIPE", "nameDetails": null, "paperSlug": "gpipe-efficient-training-of-giant-neural", "usesAdditionalData": false}, {"x": "2018-11-28", "y": 0.7490000000000001, "name": "ESPNetv2", "nameShort": "ESPNetv2", "nameDetails": null, "paperSlug": "espnetv2-a-light-weight-power-efficient-and", "usesAdditionalData": false}, {"x": "2018-12-02", "y": 0.746, "name": "Proxyless", "nameShort": "Proxyless", "nameDetails": null, "paperSlug": "proxylessnas-direct-neural-architecture", "usesAdditionalData": false}, {"x": "2018-12-04", "y": 0.7716, "name": "ResNet-50-D", "nameShort": "ResNet-50-D", "nameDetails": null, "paperSlug": "bag-of-tricks-for-image-classification-with", "usesAdditionalData": false}, {"x": "2018-12-09", "y": 0.7490000000000001, "name": "FBNet-C", "nameShort": "FBNet-C", "nameDetails": null, "paperSlug": "fbnet-hardware-aware-efficient-convnet-design", "usesAdditionalData": false}, {"x": "2019-02-01", "y": 0.8432, "name": "ColorNet ", "nameShort": "ColorNet ", "nameDetails": "RHYLH with Conv Layer", "paperSlug": "colornet-investigating-the-importance-of", "usesAdditionalData": false}, {"x": "2019-02-01", "y": 0.8234999999999999, "name": "ColorNet", "nameShort": "ColorNet", "nameDetails": null, "paperSlug": "colornet-investigating-the-importance-of", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.836, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "500px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.8320000000000001, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "450px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.831, "name": "MultiGrain SENet154 ", "nameShort": "MultiGrain SENet154 ", "nameDetails": "450px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.83, "name": "MultiGrain SENet154 ", "nameShort": "MultiGrain SENet154 ", "nameDetails": "400px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.8270000000000001, "name": "MultiGrain SENet154 ", "nameShort": "MultiGrain SENet154 ", "nameDetails": "500px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.826, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "400px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.813, "name": "MultiGrain PNASNet ", "nameShort": "MultiGrain PNASNet ", "nameDetails": "300px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.794, "name": "MultiGrain R50-AA-500", "nameShort": "MultiGrain R50-AA-500", "nameDetails": null, "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.782, "name": "MultiGrain R50-AA-224", "nameShort": "MultiGrain R50-AA-224", "nameDetails": null, "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.7509999999999999, "name": "MultiGrain NASNet-A-Mobile ", "nameShort": "MultiGrain NASNet-A-Mobile ", "nameDetails": "350px", "paperSlug": "multigrain-a-unified-image-embedding-for", "usesAdditionalData": false}, {"x": "2019-02-14", "y": 0.6829000000000001, "name": "Graph-RISE ", "nameShort": "Graph-RISE ", "nameDetails": "40M", "paperSlug": "graph-rise-graph-regularized-image-semantic", "usesAdditionalData": false}, {"x": "2019-03-15", "y": 0.7981, "name": "SKNet-101", "nameShort": "SKNet-101", "nameDetails": null, "paperSlug": "selective-kernel-networks", "usesAdditionalData": false}, {"x": "2019-03-26", "y": 0.7847, "name": "SRM-ResNet-101", "nameShort": "SRM-ResNet-101", "nameDetails": null, "paperSlug": "srm-a-style-based-recalibration-module-for", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.8123, "name": "Res2Net-101", "nameShort": "Res2Net-101", "nameDetails": null, "paperSlug": "res2net-a-new-multi-scale-backbone", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.8009999999999999, "name": "RandWire-WS", "nameShort": "RandWire-WS", "nameDetails": null, "paperSlug": "exploring-randomly-wired-neural-networks-for", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.7859, "name": "Res2Net-50-299", "nameShort": "Res2Net-50-299", "nameDetails": null, "paperSlug": "res2net-a-new-multi-scale-backbone", "usesAdditionalData": false}, {"x": "2019-04-02", "y": 0.747, "name": "RandWire-WS ", "nameShort": "RandWire-WS ", "nameDetails": "small", "paperSlug": "exploring-randomly-wired-neural-networks-for", "usesAdditionalData": false}, {"x": "2019-04-05", "y": 0.7495999999999999, "name": "Single-Path NAS", "nameShort": "Single-Path NAS", "nameDetails": null, "paperSlug": "single-path-nas-designing-hardware-efficient", "usesAdditionalData": false}, {"x": "2019-04-07", "y": 0.775, "name": "ACNet ", "nameShort": "ACNet ", "nameDetails": "ResNet-50", "paperSlug": "adaptively-connected-neural-networks", "usesAdditionalData": false}, {"x": "2019-04-10", "y": 0.8290000000000001, "name": "Oct-ResNet-152 ", "nameShort": "Oct-ResNet-152 ", "nameDetails": "SE", "paperSlug": "drop-an-octave-reducing-spatial-redundancy-in", "usesAdditionalData": false}, {"x": "2019-04-10", "y": 0.7829999999999999, "name": "EfficientNet-B0 ", "nameShort": "EfficientNet-B0 ", "nameDetails": "CondConv", "paperSlug": "soft-conditional-computation", "usesAdditionalData": false}, {"x": "2019-04-20", "y": 0.7938, "name": "ScaleNet-152", "nameShort": "ScaleNet-152", "nameDetails": null, "paperSlug": "190409460", "usesAdditionalData": false}, {"x": "2019-04-20", "y": 0.7903, "name": "ScaleNet-101", "nameShort": "ScaleNet-101", "nameDetails": null, "paperSlug": "190409460", "usesAdditionalData": false}, {"x": "2019-04-20", "y": 0.778, "name": "ScaleNet-50", "nameShort": "ScaleNet-50", "nameDetails": null, "paperSlug": "190409460", "usesAdditionalData": false}, {"x": "2019-04-22", "y": 0.7909999999999999, "name": "AA-ResNet-152", "nameShort": "AA-ResNet-152", "nameDetails": null, "paperSlug": "190409925", "usesAdditionalData": false}, {"x": "2019-04-25", "y": 0.757, "name": "LR-Net-26", "nameShort": "LR-Net-26", "nameDetails": null, "paperSlug": "190411491", "usesAdditionalData": false}, {"x": "2019-04-29", "y": 0.7904000000000001, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "UDA", "paperSlug": "unsupervised-data-augmentation-1", "usesAdditionalData": false}, {"x": "2019-05-01", "y": 0.8059999999999999, "name": "ResNet-200 ", "nameShort": "ResNet-200 ", "nameDetails": "Fast AA", "paperSlug": "fast-autoaugment", "usesAdditionalData": false}, {"x": "2019-05-01", "y": 0.7759999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "Fast AA", "paperSlug": "fast-autoaugment", "usesAdditionalData": false}, {"x": "2019-05-02", "y": 0.848, "name": "ResNeXt-101 32x16d ", "nameShort": "ResNeXt-101 32x16d ", "nameDetails": "semi-weakly sup.", "paperSlug": "billion-scale-semi-supervised-learning-for", "usesAdditionalData": false}, {"x": "2019-05-02", "y": 0.843, "name": "ResNeXt-101 32x8d ", "nameShort": "ResNeXt-101 32x8d ", "nameDetails": "semi-weakly sup.", "paperSlug": "billion-scale-semi-supervised-learning-for", "usesAdditionalData": false}, {"x": "2019-05-02", "y": 0.8340000000000001, "name": "ResNeXt-101 32x4d ", "nameShort": "ResNeXt-101 32x4d ", "nameDetails": "semi-weakly sup.", "paperSlug": "billion-scale-semi-supervised-learning-for", "usesAdditionalData": false}, {"x": "2019-05-06", "y": 0.752, "name": "MobileNet V3-Large 1.0", "nameShort": "MobileNet V3-Large 1.0", "nameDetails": null, "paperSlug": "searching-for-mobilenetv3", "usesAdditionalData": false}, {"x": "2019-05-13", "y": 0.8053, "name": "ResNeXt-101 ", "nameShort": "ResNeXt-101 ", "nameDetails": "CutMix", "paperSlug": "cutmix-regularization-strategy-to-train", "usesAdditionalData": false}, {"x": "2019-05-13", "y": 0.784, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "CutMix", "paperSlug": "cutmix-regularization-strategy-to-train", "usesAdditionalData": false}, {"x": "2019-05-23", "y": 0.78798, "name": "SGE-ResNet101", "nameShort": "SGE-ResNet101", "nameDetails": null, "paperSlug": "spatial-group-wise-enhance-improving-semantic", "usesAdditionalData": false}, {"x": "2019-05-23", "y": 0.7758400000000001, "name": "SGE-ResNet50", "nameShort": "SGE-ResNet50", "nameDetails": null, "paperSlug": "spatial-group-wise-enhance-improving-semantic", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.8440000000000001, "name": "EfficientNet-B7", "nameShort": "EfficientNet-B7", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.84, "name": "EfficientNet-B6", "nameShort": "EfficientNet-B6", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.833, "name": "EfficientNet-B5", "nameShort": "EfficientNet-B5", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.826, "name": "EfficientNet-B4", "nameShort": "EfficientNet-B4", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.8109999999999999, "name": "EfficientNet-B3", "nameShort": "EfficientNet-B3", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.7979999999999999, "name": "EfficientNet-B2", "nameShort": "EfficientNet-B2", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.7879999999999999, "name": "EfficientNet-B1", "nameShort": "EfficientNet-B1", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-05-28", "y": 0.763, "name": "EfficientNet-B0", "nameShort": "EfficientNet-B0", "nameDetails": null, "paperSlug": "efficientnet-rethinking-model-scaling-for", "usesAdditionalData": false}, {"x": "2019-06-08", "y": 0.7509999999999999, "name": "DiCENet", "nameShort": "DiCENet", "nameDetails": null, "paperSlug": "dicenet-dimension-wise-convolutions-for", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.8370000000000001, "name": "FixPNASNet-5", "nameShort": "FixPNASNet-5", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.825, "name": "FixResNet-50 Billion-scale@224", "nameShort": "FixResNet-50 Billion-scale@224", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.7979999999999999, "name": "FixResNet-50 CutMix", "nameShort": "FixResNet-50 CutMix", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-14", "y": 0.7909999999999999, "name": "FixResNet-50", "nameShort": "FixResNet-50", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy", "usesAdditionalData": false}, {"x": "2019-06-23", "y": 0.759, "name": "DenseNAS-A", "nameShort": "DenseNAS-A", "nameDetails": null, "paperSlug": "densely-connected-search-space-for-more", "usesAdditionalData": false}, {"x": "2019-07-03", "y": 0.7534000000000001, "name": "FairNAS-A", "nameShort": "FairNAS-A", "nameDetails": null, "paperSlug": "fairnas-rethinking-evaluation-fairness-of", "usesAdditionalData": false}, {"x": "2019-07-03", "y": 0.7509999999999999, "name": "FairNAS-B", "nameShort": "FairNAS-B", "nameDetails": null, "paperSlug": "fairnas-rethinking-evaluation-fairness-of", "usesAdditionalData": false}, {"x": "2019-07-03", "y": 0.7469, "name": "FairNAS-C", "nameShort": "FairNAS-C", "nameDetails": null, "paperSlug": "fairnas-rethinking-evaluation-fairness-of", "usesAdditionalData": false}, {"x": "2019-07-22", "y": 0.789, "name": "MixNet-L", "nameShort": "MixNet-L", "nameDetails": null, "paperSlug": "mixnet-mixed-depthwise-convolutional-kernels", "usesAdditionalData": false}, {"x": "2019-07-22", "y": 0.77, "name": "MixNet-M", "nameShort": "MixNet-M", "nameDetails": null, "paperSlug": "mixnet-mixed-depthwise-convolutional-kernels", "usesAdditionalData": false}, {"x": "2019-07-22", "y": 0.758, "name": "MixNet-S", "nameShort": "MixNet-S", "nameDetails": null, "paperSlug": "mixnet-mixed-depthwise-convolutional-kernels", "usesAdditionalData": false}, {"x": "2019-07-23", "y": 0.7256, "name": "MobileNet-224 ", "nameShort": "MobileNet-224 ", "nameDetails": "CGD", "paperSlug": "compact-global-descriptor-for-neural-networks", "usesAdditionalData": false}, {"x": "2019-08-04", "y": 0.8187000000000001, "name": "AOGNet-40M-AN", "nameShort": "AOGNet-40M-AN", "nameDetails": null, "paperSlug": "attentive-normalization", "usesAdditionalData": false}, {"x": "2019-08-04", "y": 0.759, "name": "MoGA-A", "nameShort": "MoGA-A", "nameDetails": null, "paperSlug": "moga-searching-beyond-mobilenetv3", "usesAdditionalData": false}, {"x": "2019-08-12", "y": 0.7933, "name": "LIP-ResNet-101", "nameShort": "LIP-ResNet-101", "nameDetails": null, "paperSlug": "lip-local-importance-based-pooling", "usesAdditionalData": false}, {"x": "2019-08-12", "y": 0.7815000000000001, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "LIP Bottleneck-256", "paperSlug": "lip-local-importance-based-pooling", "usesAdditionalData": false}, {"x": "2019-08-12", "y": 0.7664, "name": "LIP-DenseNet-BC-121", "nameShort": "LIP-DenseNet-BC-121", "nameDetails": null, "paperSlug": "lip-local-importance-based-pooling", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.823, "name": "SCARLET-A4", "nameShort": "SCARLET-A4", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.769, "name": "SCARLET-A", "nameShort": "SCARLET-A", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.763, "name": "SCARLET-B", "nameShort": "SCARLET-B", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-16", "y": 0.7559999999999999, "name": "SCARLET-C", "nameShort": "SCARLET-C", "nameDetails": null, "paperSlug": "scarletnas-bridging-the-gap-between", "usesAdditionalData": false}, {"x": "2019-08-23", "y": 0.7979999999999999, "name": "CSPResNeXt-50 + Mish", "nameShort": "CSPResNeXt-50 + Mish", "nameDetails": null, "paperSlug": "mish-a-self-regularized-non-monotonic-neural", "usesAdditionalData": false}, {"x": "2019-08-26", "y": 0.805, "name": "HCGNet-C", "nameShort": "HCGNet-C", "nameDetails": null, "paperSlug": "gated-convolutional-networks-with-hybrid", "usesAdditionalData": false}, {"x": "2019-08-26", "y": 0.785, "name": "HCGNet-B", "nameShort": "HCGNet-B", "nameDetails": null, "paperSlug": "gated-convolutional-networks-with-hybrid", "usesAdditionalData": false}, {"x": "2019-09-26", "y": 0.626, "name": "BBG ", "nameShort": "BBG ", "nameDetails": "ResNet-34", "paperSlug": "balanced-binary-neural-networks-with-gated", "usesAdditionalData": false}, {"x": "2019-09-26", "y": 0.594, "name": "BBG ", "nameShort": "BBG ", "nameDetails": "ResNet-18", "paperSlug": "balanced-binary-neural-networks-with-gated", "usesAdditionalData": false}, {"x": "2019-09-30", "y": 0.8540000000000001, "name": "EfficientNet-B8 ", "nameShort": "EfficientNet-B8 ", "nameDetails": "RandAugment", "paperSlug": "randaugment-practical-data-augmentation-with", "usesAdditionalData": false}, {"x": "2019-09-30", "y": 0.85, "name": "EfficientNet-B7 ", "nameShort": "EfficientNet-B7 ", "nameDetails": "RandAugment", "paperSlug": "randaugment-practical-data-augmentation-with", "usesAdditionalData": false}, {"x": "2019-10-07", "y": 0.785, "name": "ResNet-50-DW ", "nameShort": "ResNet-50-DW ", "nameDetails": "Deformable Kernels", "paperSlug": "deformable-kernels-adapting-effective", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7892, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "ResNet-152", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7865000000000001, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "ResNet-101", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7748, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "ResNet-50", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-08", "y": 0.7256, "name": "ECA-Net ", "nameShort": "ECA-Net ", "nameDetails": "MobileNetV2", "paperSlug": "eca-net-efficient-channel-attention-for-deep", "usesAdditionalData": false}, {"x": "2019-10-09", "y": 0.721, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "on-the-adequacy-of-untuned-warmup-for", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.8640000000000001, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B6", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.861, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B5", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.853, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B4", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.841, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B3", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.8240000000000001, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B2", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.815, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B1", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-11", "y": 0.7879999999999999, "name": "NoisyStudent ", "nameShort": "NoisyStudent ", "nameDetails": "EfficientNet-B0", "paperSlug": "self-training-with-noisy-student-improves", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.855, "name": "AdvProp ", "nameShort": "AdvProp ", "nameDetails": "EfficientNet-B8", "paperSlug": "adversarial-examples-improve-image", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.852, "name": "AdvProp ", "nameShort": "AdvProp ", "nameDetails": "EfficientNet-B7", "paperSlug": "adversarial-examples-improve-image", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.7895, "name": "InceptionV3 ", "nameShort": "InceptionV3 ", "nameDetails": "FRN layer", "paperSlug": "filter-response-normalization-layer", "usesAdditionalData": false}, {"x": "2019-11-21", "y": 0.7720999999999999, "name": "ResnetV2 50 ", "nameShort": "ResnetV2 50 ", "nameDetails": "FRN layer", "paperSlug": "filter-response-normalization-layer", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.7979999999999999, "name": "CSPResNeXt-50 ", "nameShort": "CSPResNeXt-50 ", "nameDetails": "Mish+Aug", "paperSlug": "cspnet-a-new-backbone-that-can-enhance", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.757, "name": "GhostNet \u00d71.3", "nameShort": "GhostNet \u00d71.3", "nameDetails": null, "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.75, "name": "Ghost-ResNet-50 ", "nameShort": "Ghost-ResNet-50 ", "nameDetails": "s=2", "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.741, "name": "Ghost-ResNet-50 ", "nameShort": "Ghost-ResNet-50 ", "nameDetails": "s=4", "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.7390000000000001, "name": "GhostNet \u00d71.0", "nameShort": "GhostNet \u00d71.0", "nameDetails": null, "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-27", "y": 0.662, "name": "GhostNet \u00d70.5", "nameShort": "GhostNet \u00d70.5", "nameDetails": null, "paperSlug": "ghostnet-more-features-from-cheap-operations", "usesAdditionalData": false}, {"x": "2019-11-29", "y": 0.733, "name": "Wide ResNet-50 ", "nameShort": "Wide ResNet-50 ", "nameDetails": "edge-popup", "paperSlug": "whats-hidden-in-a-randomly-weighted-neural", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.7440000000000001, "name": "DY-MobileNetV2 \u00d71.0", "nameShort": "DY-MobileNetV2 \u00d71.0", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.728, "name": "DY-MobileNetV2 \u00d70.75", "nameShort": "DY-MobileNetV2 \u00d70.75", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.727, "name": "DY-ResNet-18", "nameShort": "DY-ResNet-18", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.6970000000000001, "name": "DY-MobileNetV3-Small", "nameShort": "DY-MobileNetV3-Small", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.6940000000000001, "name": "DY-MobileNetV2 \u00d70.5", "nameShort": "DY-MobileNetV2 \u00d70.5", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.677, "name": "DY-ResNet-10", "nameShort": "DY-ResNet-10", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-07", "y": 0.649, "name": "DY-MobileNetV2 \u00d70.35", "nameShort": "DY-MobileNetV2 \u00d70.35", "nameDetails": null, "paperSlug": "dynamic-convolution-attention-over", "usesAdditionalData": false}, {"x": "2019-12-10", "y": 0.79, "name": "SpineNet-143", "nameShort": "SpineNet-143", "nameDetails": null, "paperSlug": "spinenet-learning-scale-permuted-backbone-for", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.8539, "name": "BiT-M ", "nameShort": "BiT-M ", "nameDetails": "ResNet", "paperSlug": "large-scale-learning-of-general-visual", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.8131999999999999, "name": "ResNet-200 ", "nameShort": "ResNet-200 ", "nameDetails": "Adversarial Autoaugment", "paperSlug": "adversarial-autoaugment-1", "usesAdditionalData": false}, {"x": "2019-12-24", "y": 0.794, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "Adversarial Autoaugment", "paperSlug": "adversarial-autoaugment-1", "usesAdditionalData": false}, {"x": "2020-01-17", "y": 0.8420000000000001, "name": "Assemble-ResNet152", "nameShort": "Assemble-ResNet152", "nameDetails": null, "paperSlug": "compounding-the-performance-improvements-of", "usesAdditionalData": false}, {"x": "2020-01-18", "y": 0.8284999999999999, "name": "Harm-SE-RNX-101 64x4d ", "nameShort": "Harm-SE-RNX-101 64x4d ", "nameDetails": "320x320, Mean-Max Pooling", "paperSlug": "harmonic-convolutional-networks-based-on", "usesAdditionalData": false}, {"x": "2020-02-20", "y": 0.858, "name": "Fix-EfficientNet-B8 ", "nameShort": "Fix-EfficientNet-B8 ", "nameDetails": "MaxUp + CutMix", "paperSlug": "maxup-a-simple-way-to-improve-generalization", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.871, "name": "FixEfficientNet-B7", "nameShort": "FixEfficientNet-B7", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.867, "name": "FixEfficientNet-B6", "nameShort": "FixEfficientNet-B6", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.8640000000000001, "name": "FixEfficientNet-B5", "nameShort": "FixEfficientNet-B5", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.8590000000000001, "name": "FixEfficientNet-B4", "nameShort": "FixEfficientNet-B4", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.857, "name": "FixEfficientNet-B8", "nameShort": "FixEfficientNet-B8", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.85, "name": "FixEfficientNet-B3", "nameShort": "FixEfficientNet-B3", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.84, "name": "FixEfficientNetB4", "nameShort": "FixEfficientNetB4", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.836, "name": "FixEfficientNet-B2", "nameShort": "FixEfficientNet-B2", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.826, "name": "FixEfficientNet-B1", "nameShort": "FixEfficientNet-B1", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-18", "y": 0.802, "name": "FixEfficientNet-B0", "nameShort": "FixEfficientNet-B0", "nameDetails": null, "paperSlug": "fixing-the-train-test-resolution-discrepancy-2", "usesAdditionalData": false}, {"x": "2020-03-23", "y": 0.9, "name": "Meta Pseudo Labels ", "nameShort": "Meta Pseudo Labels ", "nameDetails": "EfficientNet-B6-Wide", "paperSlug": "meta-pseudo-labels", "usesAdditionalData": false}, {"x": "2020-03-23", "y": 0.8320000000000001, "name": "Meta Pseudo Labels ", "nameShort": "Meta Pseudo Labels ", "nameDetails": "ResNet-50", "paperSlug": "meta-pseudo-labels", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.858, "name": "KDforAA ", "nameShort": "KDforAA ", "nameDetails": "EfficientNet-B8", "paperSlug": "circumventing-outliers-of-autoaugment-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.855, "name": "KDforAA ", "nameShort": "KDforAA ", "nameDetails": "EfficientNet-B7", "paperSlug": "circumventing-outliers-of-autoaugment-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.7709999999999999, "name": "GreedyNAS-A", "nameShort": "GreedyNAS-A", "nameDetails": null, "paperSlug": "greedynas-towards-fast-one-shot-nas-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.768, "name": "GreedyNAS-B", "nameShort": "GreedyNAS-B", "nameDetails": null, "paperSlug": "greedynas-towards-fast-one-shot-nas-with", "usesAdditionalData": false}, {"x": "2020-03-25", "y": 0.762, "name": "GreedyNAS-C", "nameShort": "GreedyNAS-C", "nameDetails": null, "paperSlug": "greedynas-towards-fast-one-shot-nas-with", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.843, "name": "TResNet-XL", "nameShort": "TResNet-XL", "nameDetails": null, "paperSlug": "tresnet-high-performance-gpu-dedicated", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.799, "name": "RegNetY-8.0GF", "nameShort": "RegNetY-8.0GF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.794, "name": "RegNetY-4.0GF", "nameShort": "RegNetY-4.0GF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.78, "name": "RegNetY-1.6GF", "nameShort": "RegNetY-1.6GF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.763, "name": "RegNetY-800MF", "nameShort": "RegNetY-800MF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.755, "name": "RegNetY-600MF", "nameShort": "RegNetY-600MF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-30", "y": 0.741, "name": "RegNetY-400MF", "nameShort": "RegNetY-400MF", "nameDetails": null, "paperSlug": "designing-network-design-spaces", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.7659999999999999, "name": "MUXNet-l", "nameShort": "MUXNet-l", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.753, "name": "MUXNet-m", "nameShort": "MUXNet-m", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.716, "name": "MUXNet-s", "nameShort": "MUXNet-s", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-03-31", "y": 0.667, "name": "MUXNet-xs", "nameShort": "MUXNet-xs", "nameDetails": null, "paperSlug": "muxconv-information-multiplexing-in", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.845, "name": "ResNeSt-269", "nameShort": "ResNeSt-269", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.8390000000000001, "name": "ResNeSt-200", "nameShort": "ResNeSt-200", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.83, "name": "ResNeSt-101", "nameShort": "ResNeSt-101", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.8112999999999999, "name": "ResNeSt-50", "nameShort": "ResNeSt-50", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-19", "y": 0.8064, "name": "ResNeSt-50-fast", "nameShort": "ResNeSt-50-fast", "nameDetails": null, "paperSlug": "resnest-split-attention-networks", "usesAdditionalData": false}, {"x": "2020-04-23", "y": 0.8079999999999999, "name": "ResNet-200 ", "nameShort": "ResNet-200 ", "nameDetails": "Supervised Contrastive", "paperSlug": "supervised-contrastive-learning", "usesAdditionalData": false}, {"x": "2020-05-12", "y": 0.805, "name": "NAT-M4", "nameShort": "NAT-M4", "nameDetails": null, "paperSlug": "neural-architecture-transfer", "usesAdditionalData": false}, {"x": "2020-06-15", "y": 0.792, "name": "Multiscale DEQ ", "nameShort": "Multiscale DEQ ", "nameDetails": "MDEQ-XL", "paperSlug": "multiscale-deep-equilibrium-models", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.851, "name": "ResNet200_vd_26w_4s_ssld", "nameShort": "ResNet200_vd_26w_4s_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.84, "name": "Fix_ResNet50_vd_ssld", "nameShort": "Fix_ResNet50_vd_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.83, "name": "ResNet50_vd_ssld", "nameShort": "ResNet50_vd_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-18", "y": 0.79, "name": "MobileNetV3_large_x1_0_ssld", "nameShort": "MobileNetV3_large_x1_0_ssld", "nameDetails": null, "paperSlug": "semi-supervised-recognition-under-a-noisy-and", "usesAdditionalData": false}, {"x": "2020-06-20", "y": 0.8149, "name": "PyConvResNet-101", "nameShort": "PyConvResNet-101", "nameDetails": null, "paperSlug": "pyramidal-convolution-rethinking", "usesAdditionalData": false}, {"x": "2020-06-20", "y": 0.7717, "name": "Prodpoly", "nameShort": "Prodpoly", "nameDetails": null, "paperSlug": "deep-polynomial-neural-networks", "usesAdditionalData": false}, {"x": "2020-06-22", "y": 0.7924, "name": "PS-KD ", "nameShort": "PS-KD ", "nameDetails": "ResNet-152 + CutMix", "paperSlug": "self-knowledge-distillation-a-simple-way-for", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.828, "name": "ReXNet_3.0", "nameShort": "ReXNet_3.0", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.816, "name": "ReXNet_2.0", "nameShort": "ReXNet_2.0", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.8029999999999999, "name": "ReXNet_1.5", "nameShort": "ReXNet_1.5", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.795, "name": "ReXNet_1.3", "nameShort": "ReXNet_1.3", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.779, "name": "ReXNet_1.0", "nameShort": "ReXNet_1.0", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.772, "name": "ReXNet_0.9", "nameShort": "ReXNet_0.9", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-07-02", "y": 0.746, "name": "ReXNet_0.6", "nameShort": "ReXNet_0.6", "nameDetails": null, "paperSlug": "rexnet-diminishing-representational", "usesAdditionalData": false}, {"x": "2020-09-10", "y": 0.7197, "name": "Ours", "nameShort": "Ours", "nameDetails": null, "paperSlug": "quantnet-learning-to-quantize-by-learning", "usesAdditionalData": false}, {"x": "2020-09-15", "y": 0.7876000000000001, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "puzzle-mix-exploiting-saliency-and-local-1", "usesAdditionalData": false}, {"x": "2020-09-17", "y": 0.8172, "name": "MEAL V2 ", "nameShort": "MEAL V2 ", "nameDetails": "ResNet-50", "paperSlug": "meal-v2-boosting-vanilla-resnet-50-to-80-top", "usesAdditionalData": false}, {"x": "2020-09-17", "y": 0.8067, "name": "MEAL V2 ", "nameShort": "MEAL V2 ", "nameDetails": "ResNet-50", "paperSlug": "meal-v2-boosting-vanilla-resnet-50-to-80-top", "usesAdditionalData": false}, {"x": "2020-09-17", "y": 0.7319, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "MEAL V2", "paperSlug": "meal-v2-boosting-vanilla-resnet-50-to-80-top", "usesAdditionalData": false}, {"x": "2020-09-29", "y": 0.8022, "name": "iAFF-ResNeXt-50-32x4d", "nameShort": "iAFF-ResNeXt-50-32x4d", "nameDetails": null, "paperSlug": "attentional-feature-fusion", "usesAdditionalData": false}, {"x": "2020-10-03", "y": 0.8861, "name": "EfficientNet-L2-475 ", "nameShort": "EfficientNet-L2-475 ", "nameDetails": "SAM", "paperSlug": "sharpness-aware-minimization-for-efficiently-1", "usesAdditionalData": false}, {"x": "2020-10-03", "y": 0.816, "name": "ResNet-152 ", "nameShort": "ResNet-152 ", "nameDetails": "SAM", "paperSlug": "sharpness-aware-minimization-for-efficiently-1", "usesAdditionalData": false}, {"x": "2020-10-04", "y": 0.812, "name": "ResNeXt-101 ", "nameShort": "ResNeXt-101 ", "nameDetails": "Debiased+CutMix", "paperSlug": "shape-texture-debiased-neural-network-1", "usesAdditionalData": false}, {"x": "2020-10-22", "y": 0.8855, "name": "ViT-H/14", "nameShort": "ViT-H/14", "nameDetails": null, "paperSlug": "an-image-is-worth-16x16-words-transformers-1", "usesAdditionalData": false}, {"x": "2020-10-22", "y": 0.8776, "name": "ViT-L/16", "nameShort": "ViT-L/16", "nameDetails": null, "paperSlug": "an-image-is-worth-16x16-words-transformers-1", "usesAdditionalData": false}, {"x": "2020-10-22", "y": 0.8522, "name": "ViT-B/16", "nameShort": "ViT-B/16", "nameDetails": null, "paperSlug": "an-image-is-worth-16x16-words-transformers-1", "usesAdditionalData": false}, {"x": "2020-10-28", "y": 0.794, "name": "TinyNet ", "nameShort": "TinyNet ", "nameDetails": "GhostNet-A", "paperSlug": "model-rubik-s-cube-twisting-resolution-depth", "usesAdditionalData": false}, {"x": "2020-10-28", "y": 0.777, "name": "TinyNet-A + RA", "nameShort": "TinyNet-A + RA", "nameDetails": null, "paperSlug": "model-rubik-s-cube-twisting-resolution-depth", "usesAdditionalData": false}, {"x": "2020-11-03", "y": 0.7670999999999999, "name": "Perona Malik ", "nameShort": "Perona Malik ", "nameDetails": "Perona and Malik, 1990", "paperSlug": "learning-visual-representations-for-transfer-1", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7959999999999999, "name": "Grafit ", "nameShort": "Grafit ", "nameDetails": "ResNet-50", "paperSlug": "grafit-learning-fine-grained-image", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7171, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "PAD-L2 w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7156, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "FT w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7137, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "KD w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7108, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "L2 w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7093, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "CRD w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7051999999999999, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "tf-KD w/ ResNet-18 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-25", "y": 0.7009000000000001, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "SSKD w/ ResNet-34 teacher", "paperSlug": "torchdistill-a-modular-configuration-driven", "usesAdditionalData": false}, {"x": "2020-11-30", "y": 0.836, "name": "SE-ResNeXt-101, 64x4d, S=2", "nameShort": "SE-ResNeXt-101, 64x4d, S=2", "nameDetails": "320px", "paperSlug": "splitnet-divide-and-co-training", "usesAdditionalData": false}, {"x": "2020-11-30", "y": 0.8334, "name": "SE-ResNeXt-101, 64x4d, S=2", "nameShort": "SE-ResNeXt-101, 64x4d, S=2", "nameDetails": "416px", "paperSlug": "splitnet-divide-and-co-training", "usesAdditionalData": false}, {"x": "2020-11-30", "y": 0.8212999999999999, "name": "ResNeXt-101, 64x4d, S=2", "nameShort": "ResNeXt-101, 64x4d, S=2", "nameDetails": "224px", "paperSlug": "splitnet-divide-and-co-training", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.852, "name": "DeiT-B 384", "nameShort": "DeiT-B 384", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.8420000000000001, "name": "DeiT-B", "nameShort": "DeiT-B", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.826, "name": "DeiT-B", "nameShort": "DeiT-B", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2020-12-23", "y": 0.7659999999999999, "name": "DeiT-B", "nameShort": "DeiT-B", "nameDetails": null, "paperSlug": "training-data-efficient-image-transformers", "usesAdditionalData": false}, {"x": "2021-01-05", "y": 0.8029999999999999, "name": "ResNet-50+AutoDropout+RandAugment", "nameShort": "ResNet-50+AutoDropout+RandAugment", "nameDetails": null, "paperSlug": "autodropout-learning-dropout-patterns-to", "usesAdditionalData": false}, {"x": "2021-01-05", "y": 0.787, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "autodropout-learning-dropout-patterns-to", "usesAdditionalData": false}, {"x": "2021-01-05", "y": 0.775, "name": "EfficientNet-B0", "nameShort": "EfficientNet-B0", "nameDetails": null, "paperSlug": "autodropout-learning-dropout-patterns-to", "usesAdditionalData": false}, {"x": "2021-01-07", "y": 0.77, "name": "SSAL-Resnet50", "nameShort": "SSAL-Resnet50", "nameDetails": null, "paperSlug": "contextual-classification-using-self", "usesAdditionalData": false}, {"x": "2021-01-11", "y": 0.7878000000000001, "name": "RepVGG-B2", "nameShort": "RepVGG-B2", "nameDetails": null, "paperSlug": "repvgg-making-vgg-style-convnets-great-again", "usesAdditionalData": false}, {"x": "2021-01-11", "y": 0.785, "name": "RepVGG-B2g4", "nameShort": "RepVGG-B2g4", "nameDetails": null, "paperSlug": "repvgg-making-vgg-style-convnets-great-again", "usesAdditionalData": false}, {"x": "2021-01-13", "y": 0.784, "name": "ReXNet_1.0-relabel", "nameShort": "ReXNet_1.0-relabel", "nameDetails": null, "paperSlug": "re-labeling-imagenet-from-single-to-multi", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.847, "name": "BoTNet T7", "nameShort": "BoTNet T7", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8420000000000001, "name": "BoTNet T7-320", "nameShort": "BoTNet T7-320", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.84, "name": "BoTNet T6", "nameShort": "BoTNet T6", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.838, "name": "SENet-350", "nameShort": "SENet-350", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.835, "name": "BoTNet T5", "nameShort": "BoTNet T5", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.828, "name": "BoTNet T4", "nameShort": "BoTNet T4", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8220000000000001, "name": "SENet-152", "nameShort": "SENet-152", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8170000000000001, "name": "BoTNet T3", "nameShort": "BoTNet T3", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8140000000000001, "name": "SENet-101", "nameShort": "SENet-101", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.8, "name": "ResNet-101", "nameShort": "ResNet-101", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.794, "name": "SENet-50", "nameShort": "SENet-50", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-27", "y": 0.7879999999999999, "name": "ResNet-50", "nameShort": "ResNet-50", "nameDetails": null, "paperSlug": "bottleneck-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.833, "name": "T2T-ViT-14|384", "nameShort": "T2T-ViT-14|384", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.826, "name": "T2T-ViTt-24", "nameShort": "T2T-ViTt-24", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.823, "name": "T2T-ViT-24", "nameShort": "T2T-ViT-24", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.8220000000000001, "name": "T2T-ViTt-19", "nameShort": "T2T-ViTt-19", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.8190000000000001, "name": "T2T-ViT-19", "nameShort": "T2T-ViT-19", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-01-28", "y": 0.815, "name": "T2T-ViT-14", "nameShort": "T2T-ViT-14", "nameDetails": null, "paperSlug": "tokens-to-token-vit-training-vision", "usesAdditionalData": false}, {"x": "2021-02-01", "y": 0.83, "name": "ZenNAS ", "nameShort": "ZenNAS ", "nameDetails": "0.8ms", "paperSlug": "zen-nas-a-zero-shot-nas-for-high-performance", "usesAdditionalData": false}, {"x": "2021-02-01", "y": 0.78, "name": "ZenNet-400M-SE", "nameShort": "ZenNet-400M-SE", "nameDetails": null, "paperSlug": "zen-nas-a-zero-shot-nas-for-high-performance", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.892, "name": "NFNet-F4+", "nameShort": "NFNet-F4+", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.8864, "name": "ALIGN ", "nameShort": "ALIGN ", "nameDetails": "EfficientNet-L2", "paperSlug": "scaling-up-visual-and-vision-language", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.865, "name": "NFNet-F6 w/ SAM", "nameShort": "NFNet-F6 w/ SAM", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.863, "name": "NFNet-F5 w/ SAM", "nameShort": "NFNet-F5 w/ SAM", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.86, "name": "NFNet-F5", "nameShort": "NFNet-F5", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.8590000000000001, "name": "NFNet-F4", "nameShort": "NFNet-F4", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.857, "name": "NFNet-F3", "nameShort": "NFNet-F3", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.851, "name": "NFNet-F2", "nameShort": "NFNet-F2", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.847, "name": "NFNet-F1", "nameShort": "NFNet-F1", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-11", "y": 0.836, "name": "NFNet-F0", "nameShort": "NFNet-F0", "nameDetails": null, "paperSlug": "high-performance-large-scale-image", "usesAdditionalData": false}, {"x": "2021-02-12", "y": 0.7592, "name": "ResNet-50 MLPerf v0.7 - 2512 steps", "nameShort": "ResNet-50 MLPerf v0.7 - 2512 steps", "nameDetails": null, "paperSlug": "a-large-batch-optimizer-reality-check", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.8079999999999999, "name": "AlphaNet-A6", "nameShort": "AlphaNet-A6", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.8029999999999999, "name": "AlphaNet-A5", "nameShort": "AlphaNet-A5", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.8, "name": "AlphaNet-A4", "nameShort": "AlphaNet-A4", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.794, "name": "AlphaNet-A3", "nameShort": "AlphaNet-A3", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.7909999999999999, "name": "AlphaNet-A2", "nameShort": "AlphaNet-A2", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.789, "name": "AlphaNet-A1", "nameShort": "AlphaNet-A1", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-16", "y": 0.778, "name": "AlphaNet-A0", "nameShort": "AlphaNet-A0", "nameDetails": null, "paperSlug": "alphanet-improved-training-of-supernet-with", "usesAdditionalData": false}, {"x": "2021-02-17", "y": 0.843, "name": "LambdaResNet200", "nameShort": "LambdaResNet200", "nameDetails": null, "paperSlug": "lambdanetworks-modeling-long-range-1", "usesAdditionalData": false}, {"x": "2021-02-17", "y": 0.84, "name": "LambdaResNet152", "nameShort": "LambdaResNet152", "nameDetails": null, "paperSlug": "lambdanetworks-modeling-long-range-1", "usesAdditionalData": false}, {"x": "2021-02-17", "y": 0.809, "name": "CentroidViT-S ", "nameShort": "CentroidViT-S ", "nameDetails": "arXiv, 2021-02", "paperSlug": "centroid-transformers-learning-to-abstract", "usesAdditionalData": false}, {"x": "2021-02-27", "y": 0.8390000000000001, "name": "TNT-B", "nameShort": "TNT-B", "nameDetails": null, "paperSlug": "transformer-in-transformer", "usesAdditionalData": false}, {"x": "2021-03-04", "y": 0.78, "name": "Perceiver ", "nameShort": "Perceiver ", "nameDetails": "FF", "paperSlug": "perceiver-general-perception-with-iterative", "usesAdditionalData": false}, {"x": "2021-03-04", "y": 0.764, "name": "Perceiver", "nameShort": "Perceiver", "nameDetails": null, "paperSlug": "perceiver-general-perception-with-iterative", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.7929999999999999, "name": "RedNet-152", "nameShort": "RedNet-152", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.7909999999999999, "name": "RedNet-101", "nameShort": "RedNet-101", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.784, "name": "RedNet-50", "nameShort": "RedNet-50", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.7759999999999999, "name": "RedNet-38", "nameShort": "RedNet-38", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-10", "y": 0.759, "name": "RedNet-26", "nameShort": "RedNet-26", "nameDetails": null, "paperSlug": "involution-inverting-the-inherence-of", "usesAdditionalData": false}, {"x": "2021-03-13", "y": 0.8440000000000001, "name": "ResNet-RS-50 ", "nameShort": "ResNet-RS-50 ", "nameDetails": "160 image res", "paperSlug": "revisiting-resnets-improved-training-and", "usesAdditionalData": false}, {"x": "2021-03-13", "y": 0.838, "name": "ResNet-RS-270 ", "nameShort": "ResNet-RS-270 ", "nameDetails": "256 image res", "paperSlug": "revisiting-resnets-improved-training-and", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.825, "name": "ConViT-B+", "nameShort": "ConViT-B+", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.8240000000000001, "name": "ConViT-B", "nameShort": "ConViT-B", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.8220000000000001, "name": "ConViT-S+", "nameShort": "ConViT-S+", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.813, "name": "ConViT-S", "nameShort": "ConViT-S", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.78, "name": "HVT-S-1", "nameShort": "HVT-S-1", "nameDetails": null, "paperSlug": "scalable-visual-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.767, "name": "ConViT-Ti+", "nameShort": "ConViT-Ti+", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.731, "name": "ConViT-Ti", "nameShort": "ConViT-Ti", "nameDetails": null, "paperSlug": "convit-improving-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-19", "y": 0.6964, "name": "HVT-Ti-1", "nameShort": "HVT-Ti-1", "nameDetails": null, "paperSlug": "scalable-visual-transformers-with", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.833, "name": "CeiT-S ", "nameShort": "CeiT-S ", "nameDetails": "384 finetune res", "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.831, "name": "DeepVit-L* ", "nameShort": "DeepVit-L* ", "nameDetails": "DeiT training recipe", "paperSlug": "deepvit-towards-deeper-vision-transformer", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.8220000000000001, "name": "DeepVit-L", "nameShort": "DeepVit-L", "nameDetails": null, "paperSlug": "deepvit-towards-deeper-vision-transformer", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.82, "name": "CeiT-S", "nameShort": "CeiT-S", "nameDetails": null, "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.7879999999999999, "name": "CeiT-T ", "nameShort": "CeiT-T ", "nameDetails": "384 finetune res", "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-22", "y": 0.764, "name": "CeiT-T", "nameShort": "CeiT-T", "nameDetails": null, "paperSlug": "incorporating-convolution-designs-into-visual", "usesAdditionalData": false}, {"x": "2021-03-23", "y": 0.855, "name": "HaloNet4 ", "nameShort": "HaloNet4 ", "nameDetails": "base 128, Conv-12", "paperSlug": "scaling-local-self-attention-for-parameter", "usesAdditionalData": false}, {"x": "2021-03-23", "y": 0.8220000000000001, "name": "BossNet-T1", "nameShort": "BossNet-T1", "nameDetails": null, "paperSlug": "bossnas-exploring-hybrid-cnn-transformers", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.8098000000000001, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.7925, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.7609999999999999, "name": "ResNet-34 ", "nameShort": "ResNet-34 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-24", "y": 0.7204999999999999, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "AutoMix", "paperSlug": "automix-unveiling-the-power-of-mixup", "usesAdditionalData": false}, {"x": "2021-03-25", "y": 0.873, "name": "Swin-L ", "nameShort": "Swin-L ", "nameDetails": "384 res, ImageNet-22k pretrain", "paperSlug": "swin-transformer-hierarchical-vision", "usesAdditionalData": false}, {"x": "2021-03-25", "y": 0.8640000000000001, "name": "Swin-B ", "nameShort": "Swin-B ", "nameDetails": "384 res, ImageNet-22k pretrain", "paperSlug": "swin-transformer-hierarchical-vision", "usesAdditionalData": false}, {"x": "2021-03-25", "y": 0.813, "name": "Swin-T", "nameShort": "Swin-T", "nameDetails": null, "paperSlug": "swin-transformer-hierarchical-vision", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.828, "name": "CrossViT-18+", "nameShort": "CrossViT-18+", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.825, "name": "CrossViT-18", "nameShort": "CrossViT-18", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.823, "name": "CrossViT-15+", "nameShort": "CrossViT-15+", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-27", "y": 0.815, "name": "CrossViT-15", "nameShort": "CrossViT-15", "nameDetails": null, "paperSlug": "2103-14899", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8490000000000001, "name": "CvT-21 ", "nameShort": "CvT-21 ", "nameDetails": "384 res, ImageNet-22k pretrain", "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.833, "name": "CvT-21 ", "nameShort": "CvT-21 ", "nameDetails": "384 res", "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.833, "name": "ViL-Medium-D", "nameShort": "ViL-Medium-D", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8320000000000001, "name": "ViL-Base-D", "nameShort": "ViL-Base-D", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.83, "name": "CvT-13 ", "nameShort": "CvT-13 ", "nameDetails": "384 res", "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8290000000000001, "name": "ViL-Medium-W", "nameShort": "ViL-Medium-W", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.825, "name": "CvT-21", "nameShort": "CvT-21", "nameDetails": null, "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8220000000000001, "name": "CvT-13-NAS", "nameShort": "CvT-13-NAS", "nameDetails": null, "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.82, "name": "ViL-Small", "nameShort": "ViL-Small", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.8190000000000001, "name": "ViL-Base-W", "nameShort": "ViL-Base-W", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.816, "name": "CvT-13", "nameShort": "CvT-13", "nameDetails": null, "paperSlug": "cvt-introducing-convolutions-to-vision", "usesAdditionalData": false}, {"x": "2021-03-29", "y": 0.767, "name": "ViL-Tiny-RPB", "nameShort": "ViL-Tiny-RPB", "nameDetails": null, "paperSlug": "2103-15358", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.84, "name": "PiT-B", "nameShort": "PiT-B", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.8190000000000001, "name": "PiT-S", "nameShort": "PiT-S", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.7909999999999999, "name": "PiT-XS", "nameShort": "PiT-XS", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-30", "y": 0.746, "name": "PiT-Ti", "nameShort": "PiT-Ti", "nameDetails": null, "paperSlug": "rethinking-spatial-dimensions-of-vision", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.865, "name": "CaiT-M-48-448", "nameShort": "CaiT-M-48-448", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.863, "name": "CAIT-M36-448", "nameShort": "CAIT-M36-448", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.861, "name": "CAIT-M-36", "nameShort": "CAIT-M-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.858, "name": "CAIT-M-24", "nameShort": "CAIT-M-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.8540000000000001, "name": "CAIT-S-36", "nameShort": "CAIT-S-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.853, "name": "CAIT-S-48", "nameShort": "CAIT-S-48", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.851, "name": "CAIT-S-24", "nameShort": "CAIT-S-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.848, "name": "CAIT-XS-36", "nameShort": "CAIT-XS-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.841, "name": "CAIT-XS-24", "nameShort": "CAIT-XS-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.8220000000000001, "name": "CAIT-XXS-36", "nameShort": "CAIT-XXS-36", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-03-31", "y": 0.809, "name": "CAIT-XXS-24", "nameShort": "CAIT-XXS-24", "nameDetails": null, "paperSlug": "going-deeper-with-image-transformers", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.868, "name": "EfficientNetV2-L ", "nameShort": "EfficientNetV2-L ", "nameDetails": "21k", "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.861, "name": "EfficientNetV2-M ", "nameShort": "EfficientNetV2-M ", "nameDetails": "21k", "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.857, "name": "EfficientNetV2-L", "nameShort": "EfficientNetV2-L", "nameDetails": null, "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.851, "name": "EfficientNetV2-M", "nameShort": "EfficientNetV2-M", "nameDetails": null, "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.85, "name": "EfficientNetV2-S ", "nameShort": "EfficientNetV2-S ", "nameDetails": "21k", "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-01", "y": 0.8390000000000001, "name": "EfficientNetV2-S", "nameShort": "EfficientNetV2-S", "nameDetails": null, "paperSlug": "efficientnetv2-smaller-models-and-faster", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.825, "name": "LeViT-384", "nameShort": "LeViT-384", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.816, "name": "LeViT-256", "nameShort": "LeViT-256", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.8, "name": "LeViT-192", "nameShort": "LeViT-192", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.7959999999999999, "name": "LeViT-128", "nameShort": "LeViT-128", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-02", "y": 0.757, "name": "LeViT-128S", "nameShort": "LeViT-128S", "nameDetails": null, "paperSlug": "levit-a-vision-transformer-in-convnet-s", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8271, "name": "CCT-14/7x2 | 384", "nameShort": "CCT-14/7x2 | 384", "nameDetails": null, "paperSlug": "escaping-the-big-data-paradigm-with-compact", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8134, "name": "CCT-14/7x2", "nameShort": "CCT-14/7x2", "nameDetails": null, "paperSlug": "escaping-the-big-data-paradigm-with-compact", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8079999999999999, "name": "LocalViT-S", "nameShort": "LocalViT-S", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.8028, "name": "CCT-16/7x2", "nameShort": "CCT-16/7x2", "nameDetails": null, "paperSlug": "escaping-the-big-data-paradigm-with-compact", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.782, "name": "LocalViT-PVT", "nameShort": "LocalViT-PVT", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.759, "name": "LocalViT-TNT", "nameShort": "LocalViT-TNT", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.748, "name": "LocalViT-T", "nameShort": "LocalViT-T", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-12", "y": 0.725, "name": "LocalViT-T2T", "nameShort": "LocalViT-T2T", "nameDetails": null, "paperSlug": "localvit-bringing-locality-to-vision", "usesAdditionalData": false}, {"x": "2021-04-15", "y": 0.754, "name": "AsymmNet-Large \u00d71.0", "nameShort": "AsymmNet-Large \u00d71.0", "nameDetails": null, "paperSlug": "asymmnet-towards-ultralight-convolution", "usesAdditionalData": false}, {"x": "2021-04-15", "y": 0.6920000000000001, "name": "AsymmNet-Large \u00d70.5", "nameShort": "AsymmNet-Large \u00d70.5", "nameDetails": null, "paperSlug": "asymmnet-towards-ultralight-convolution", "usesAdditionalData": false}, {"x": "2021-04-15", "y": 0.684, "name": "AsymmNet-Small \u00d71.0", "nameShort": "AsymmNet-Small \u00d71.0", "nameDetails": null, "paperSlug": "asymmnet-towards-ultralight-convolution", "usesAdditionalData": false}, {"x": "2021-04-16", "y": 0.716, "name": "PDC", "nameShort": "PDC", "nameDetails": null, "paperSlug": "polynomial-networks-in-deep-classifiers", "usesAdditionalData": false}, {"x": "2021-04-16", "y": 0.68, "name": "ReActNet-A ", "nameShort": "ReActNet-A ", "nameDetails": "BN-Free", "paperSlug": "bnn-bn-training-binary-neural-networks", "usesAdditionalData": false}, {"x": "2021-04-20", "y": 0.82, "name": "DIFFQ ", "nameShort": "DIFFQ ", "nameDetails": "\u03bb=1e\u22122", "paperSlug": "differentiable-model-compression-via-pseudo", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.8640000000000001, "name": "LV-ViT-L", "nameShort": "LV-ViT-L", "nameDetails": null, "paperSlug": "token-labeling-training-a-85-5-top-1-accuracy", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.848, "name": "MViT-B-24", "nameShort": "MViT-B-24", "nameDetails": null, "paperSlug": "multiscale-vision-transformers", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.841, "name": "LV-ViT-M", "nameShort": "LV-ViT-M", "nameDetails": null, "paperSlug": "token-labeling-training-a-85-5-top-1-accuracy", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.833, "name": "LV-ViT-S", "nameShort": "LV-ViT-S", "nameDetails": null, "paperSlug": "token-labeling-training-a-85-5-top-1-accuracy", "usesAdditionalData": false}, {"x": "2021-04-22", "y": 0.83, "name": "MViT-B-16", "nameShort": "MViT-B-16", "nameDetails": null, "paperSlug": "multiscale-vision-transformers", "usesAdditionalData": false}, {"x": "2021-04-26", "y": 0.8220000000000001, "name": "Visformer-S", "nameShort": "Visformer-S", "nameDetails": null, "paperSlug": "visformer-the-vision-friendly-transformer", "usesAdditionalData": false}, {"x": "2021-04-26", "y": 0.7859999999999999, "name": "Visformer-Ti", "nameShort": "Visformer-Ti", "nameDetails": null, "paperSlug": "visformer-the-vision-friendly-transformer", "usesAdditionalData": false}, {"x": "2021-04-28", "y": 0.8370000000000001, "name": "Twins-SVT-L", "nameShort": "Twins-SVT-L", "nameDetails": null, "paperSlug": "twins-revisiting-spatial-attention-design-in", "usesAdditionalData": false}, {"x": "2021-04-28", "y": 0.755, "name": "PAWS ", "nameShort": "PAWS ", "nameDetails": "ResNet-50, 10% labels", "paperSlug": "semi-supervised-learning-of-visual-features", "usesAdditionalData": false}, {"x": "2021-04-28", "y": 0.665, "name": "PAWS ", "nameShort": "PAWS ", "nameDetails": "ResNet-50, 1% labels", "paperSlug": "semi-supervised-learning-of-visual-features", "usesAdditionalData": false}, {"x": "2021-05-04", "y": 0.8794, "name": "Mixer-H/14 ", "nameShort": "Mixer-H/14 ", "nameDetails": "JFT-300M pre-train", "paperSlug": "mlp-mixer-an-all-mlp-architecture-for-vision", "usesAdditionalData": false}, {"x": "2021-05-04", "y": 0.853, "name": "ViT-L/16 Dosovitskiy et al. ", "nameShort": "ViT-L/16 Dosovitskiy et al. ", "nameDetails": "2021", "paperSlug": "mlp-mixer-an-all-mlp-architecture-for-vision", "usesAdditionalData": false}, {"x": "2021-05-04", "y": 0.7644, "name": "Mixer-B/16", "nameShort": "Mixer-B/16", "nameDetails": null, "paperSlug": "mlp-mixer-an-all-mlp-architecture-for-vision", "usesAdditionalData": false}, {"x": "2021-05-05", "y": 0.8170000000000001, "name": "T2T-ViT-14", "nameShort": "T2T-ViT-14", "nameDetails": null, "paperSlug": "beyond-self-attention-external-attention", "usesAdditionalData": false}, {"x": "2021-05-05", "y": 0.7859999999999999, "name": "RepMLP-Res50", "nameShort": "RepMLP-Res50", "nameDetails": null, "paperSlug": "repmlp-re-parameterizing-convolutions-into", "usesAdditionalData": false}, {"x": "2021-05-06", "y": 0.7490000000000001, "name": "FF", "nameShort": "FF", "nameDetails": null, "paperSlug": "do-you-even-need-attention-a-stack-of-feed", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.836, "name": "ResMLP-B24/8", "nameShort": "ResMLP-B24/8", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.8079999999999999, "name": "ResMLP-S24", "nameShort": "ResMLP-S24", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.8, "name": "BasisNet-MV3", "nameShort": "BasisNet-MV3", "nameDetails": null, "paperSlug": "basisnet-two-stage-model-synthesis-for-1", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.797, "name": "ResMLP-36", "nameShort": "ResMLP-36", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.794, "name": "ResMLP-24", "nameShort": "ResMLP-24", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.7859999999999999, "name": "ResMLP-12 ", "nameShort": "ResMLP-12 ", "nameDetails": "distilled, class-MLP", "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-07", "y": 0.778, "name": "ResMLP-S12", "nameShort": "ResMLP-S12", "nameDetails": null, "paperSlug": "resmlp-feedforward-networks-for-image", "usesAdditionalData": false}, {"x": "2021-05-09", "y": 0.841, "name": "Conformer-B", "nameShort": "Conformer-B", "nameDetails": null, "paperSlug": "conformer-local-features-coupling-global", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.8270000000000001, "name": "RVT-B*", "nameShort": "RVT-B*", "nameDetails": null, "paperSlug": "rethinking-the-design-principles-of-robust", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.8190000000000001, "name": "RVT-S*", "nameShort": "RVT-S*", "nameDetails": null, "paperSlug": "rethinking-the-design-principles-of-robust", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.816, "name": "gMLP-B", "nameShort": "gMLP-B", "nameDetails": null, "paperSlug": "pay-attention-to-mlps", "usesAdditionalData": false}, {"x": "2021-05-17", "y": 0.792, "name": "RVT-Ti*", "nameShort": "RVT-Ti*", "nameDetails": null, "paperSlug": "rethinking-the-design-principles-of-robust", "usesAdditionalData": false}, {"x": "2021-05-19", "y": 0.6859999999999999, "name": "Heteroscedastic ", "nameShort": "Heteroscedastic ", "nameDetails": "InceptionResNet-v2", "paperSlug": "correlated-input-dependent-label-noise-in", "usesAdditionalData": false}, {"x": "2021-05-26", "y": 0.838, "name": "Transformer local-attention ", "nameShort": "Transformer local-attention ", "nameDetails": "NesT-B", "paperSlug": "aggregating-nested-transformers", "usesAdditionalData": false}, {"x": "2021-05-26", "y": 0.833, "name": "Transformer local-attention ", "nameShort": "Transformer local-attention ", "nameDetails": "NesT-S", "paperSlug": "aggregating-nested-transformers", "usesAdditionalData": false}, {"x": "2021-05-26", "y": 0.815, "name": "Transformer local-attention ", "nameShort": "Transformer local-attention ", "nameDetails": "NesT-T", "paperSlug": "aggregating-nested-transformers", "usesAdditionalData": false}, {"x": "2021-05-27", "y": 0.8678, "name": "NFNet-F5 w/ SAM w/ augmult=16", "nameShort": "NFNet-F5 w/ SAM w/ augmult=16", "nameDetails": null, "paperSlug": "drawing-multiple-augmentation-samples-per", "usesAdditionalData": false}, {"x": "2021-05-28", "y": 0.836, "name": "ResT-Large", "nameShort": "ResT-Large", "nameDetails": null, "paperSlug": "rest-an-efficient-transformer-for-visual", "usesAdditionalData": false}, {"x": "2021-05-28", "y": 0.7959999999999999, "name": "ResT-Small", "nameShort": "ResT-Small", "nameDetails": null, "paperSlug": "rest-an-efficient-transformer-for-visual", "usesAdditionalData": false}, {"x": "2021-05-31", "y": 0.8043, "name": "DVT ", "nameShort": "DVT ", "nameDetails": "T2T-ViT-12", "paperSlug": "not-all-images-are-worth-16x16-words-dynamic", "usesAdditionalData": false}, {"x": "2021-05-31", "y": 0.7974, "name": "DVT ", "nameShort": "DVT ", "nameDetails": "T2T-ViT-10", "paperSlug": "not-all-images-are-worth-16x16-words-dynamic", "usesAdditionalData": false}, {"x": "2021-05-31", "y": 0.7848, "name": "DVT ", "nameShort": "DVT ", "nameDetails": "T2T-ViT-7", "paperSlug": "not-all-images-are-worth-16x16-words-dynamic", "usesAdditionalData": false}, {"x": "2021-06-02", "y": 0.8270000000000001, "name": "Container Container", "nameShort": "Container Container", "nameDetails": null, "paperSlug": "container-context-aggregation-network", "usesAdditionalData": false}, {"x": "2021-06-02", "y": 0.82, "name": "Container-Light", "nameShort": "Container-Light", "nameDetails": null, "paperSlug": "container-context-aggregation-network", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.8390000000000001, "name": "DynamicViT-LV-M/0.8", "nameShort": "DynamicViT-LV-M/0.8", "nameDetails": null, "paperSlug": "dynamicvit-efficient-vision-transformers-with", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.8109999999999999, "name": "ResNet-152x2-SAM", "nameShort": "ResNet-152x2-SAM", "nameDetails": null, "paperSlug": "when-vision-transformers-outperform-resnets", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.799, "name": "ViT-B/16-SAM", "nameShort": "ViT-B/16-SAM", "nameDetails": null, "paperSlug": "when-vision-transformers-outperform-resnets", "usesAdditionalData": false}, {"x": "2021-06-03", "y": 0.79, "name": "Mixer-B/8-SAM", "nameShort": "Mixer-B/8-SAM", "nameDetails": null, "paperSlug": "when-vision-transformers-outperform-resnets", "usesAdditionalData": false}, {"x": "2021-06-04", "y": 0.7659999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "X-volution, stage3", "paperSlug": "x-volution-on-the-unification-of-convolution", "usesAdditionalData": false}, {"x": "2021-06-04", "y": 0.75, "name": "ResNet-34 ", "nameShort": "ResNet-34 ", "nameDetails": "X-volution, stage3", "paperSlug": "x-volution-on-the-unification-of-convolution", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.8603000000000001, "name": "Refiner-ViT-L", "nameShort": "Refiner-ViT-L", "nameDetails": null, "paperSlug": "refiner-refining-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.836, "name": "ViTAE-B-Stage", "nameShort": "ViTAE-B-Stage", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.8220000000000001, "name": "ViTAE-S-Stage", "nameShort": "ViTAE-S-Stage", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.81, "name": "ViTAE-13M", "nameShort": "ViTAE-13M", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.779, "name": "ViTAE-6M", "nameShort": "ViTAE-6M", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.768, "name": "ViTAE-T-Stage", "nameShort": "ViTAE-T-Stage", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-07", "y": 0.753, "name": "ViTAE-T", "nameShort": "ViTAE-T", "nameDetails": null, "paperSlug": "vitae-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.9045000000000001, "name": "CoAtNet-6", "nameShort": "CoAtNet-6", "nameDetails": null, "paperSlug": "coatnet-marrying-convolution-and-attention", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.8852, "name": "CoAtNet-3 @384", "nameShort": "CoAtNet-3 @384", "nameDetails": null, "paperSlug": "coatnet-marrying-convolution-and-attention", "usesAdditionalData": false}, {"x": "2021-06-09", "y": 0.828, "name": "FunMatch - T384+224 ", "nameShort": "FunMatch - T384+224 ", "nameDetails": "ResNet-50", "paperSlug": "knowledge-distillation-a-good-teacher-is", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8835999999999999, "name": "V-MoE-H/14 ", "nameShort": "V-MoE-H/14 ", "nameDetails": "Every-2", "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8823000000000001, "name": "V-MoE-H/14 ", "nameShort": "V-MoE-H/14 ", "nameDetails": "Last-5", "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8808, "name": "VIT-H/14", "nameShort": "VIT-H/14", "nameDetails": null, "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-10", "y": 0.8741, "name": "V-MoE-L/16 ", "nameShort": "V-MoE-L/16 ", "nameDetails": "Every-2", "paperSlug": "scaling-vision-with-sparse-mixture-of-experts", "usesAdditionalData": false}, {"x": "2021-06-15", "y": 0.8859999999999999, "name": "BEiT-L ", "nameShort": "BEiT-L ", "nameDetails": "ViT; ImageNet-22K pretrain", "paperSlug": "beit-bert-pre-training-of-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-15", "y": 0.863, "name": "BEiT-L ", "nameShort": "BEiT-L ", "nameDetails": "ViT; ImageNet 1k pretrain", "paperSlug": "beit-bert-pre-training-of-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.86, "name": "XCiT-L24", "nameShort": "XCiT-L24", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.858, "name": "XCiT-M24", "nameShort": "XCiT-M24", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.856, "name": "XCiT-S24", "nameShort": "XCiT-S24", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-17", "y": 0.851, "name": "XCiT-S12", "nameShort": "XCiT-S12", "nameDetails": null, "paperSlug": "xcit-cross-covariance-image-transformers", "usesAdditionalData": false}, {"x": "2021-06-21", "y": 0.8887, "name": "TokenLearner L/8 ", "nameShort": "TokenLearner L/8 ", "nameDetails": "24+11", "paperSlug": "tokenlearner-what-can-8-learned-tokens-do-for", "usesAdditionalData": false}, {"x": "2021-06-21", "y": 0.8706999999999999, "name": "16-TokenLearner B/16 ", "nameShort": "16-TokenLearner B/16 ", "nameDetails": "21", "paperSlug": "tokenlearner-what-can-8-learned-tokens-do-for", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.871, "name": "VOLO-D5", "nameShort": "VOLO-D5", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.868, "name": "VOLO-D4", "nameShort": "VOLO-D4", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.863, "name": "VOLO-D3", "nameShort": "VOLO-D3", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.86, "name": "VOLO-D2", "nameShort": "VOLO-D2", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-24", "y": 0.852, "name": "VOLO-D1", "nameShort": "VOLO-D1", "nameDetails": null, "paperSlug": "volo-vision-outlooker-for-visual-recognition", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.838, "name": "PVTv2-B4", "nameShort": "PVTv2-B4", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.8320000000000001, "name": "PVTv2-B3", "nameShort": "PVTv2-B3", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.82, "name": "PVTv2-B2", "nameShort": "PVTv2-B2", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.787, "name": "PVTv2-B1", "nameShort": "PVTv2-B1", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-06-25", "y": 0.705, "name": "PVTv2-B0", "nameShort": "PVTv2-B0", "nameDetails": null, "paperSlug": "pvtv2-improved-baselines-with-pyramid-vision", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.875, "name": "CSWin-L ", "nameShort": "CSWin-L ", "nameDetails": "384 res,ImageNet-22k pretrain", "paperSlug": "cswin-transformer-a-general-vision", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.8290000000000001, "name": "GFNet-H-B", "nameShort": "GFNet-H-B", "nameDetails": null, "paperSlug": "global-filter-networks-for-image", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.8240000000000001, "name": "AutoFormer-base", "nameShort": "AutoFormer-base", "nameDetails": null, "paperSlug": "autoformer-searching-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.8170000000000001, "name": "AutoFormer-small", "nameShort": "AutoFormer-small", "nameDetails": null, "paperSlug": "autoformer-searching-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-07-01", "y": 0.747, "name": "AutoFormer-tiny", "nameShort": "AutoFormer-tiny", "nameDetails": null, "paperSlug": "autoformer-searching-transformers-for-visual", "usesAdditionalData": false}, {"x": "2021-07-07", "y": 0.823, "name": "GLiT-Bases", "nameShort": "GLiT-Bases", "nameDetails": null, "paperSlug": "glit-neural-architecture-search-for-global", "usesAdditionalData": false}, {"x": "2021-07-07", "y": 0.805, "name": "GLiT-Smalls", "nameShort": "GLiT-Smalls", "nameDetails": null, "paperSlug": "glit-neural-architecture-search-for-global", "usesAdditionalData": false}, {"x": "2021-07-07", "y": 0.763, "name": "GLiT-Tinys", "nameShort": "GLiT-Tinys", "nameDetails": null, "paperSlug": "glit-neural-architecture-search-for-global", "usesAdditionalData": false}, {"x": "2021-07-08", "y": 0.815, "name": "CoE-Large + CondConv", "nameShort": "CoE-Large + CondConv", "nameDetails": null, "paperSlug": "collaboration-of-experts-achieving-80-top-1", "usesAdditionalData": false}, {"x": "2021-07-08", "y": 0.807, "name": "CoE-Large", "nameShort": "CoE-Large", "nameDetails": null, "paperSlug": "collaboration-of-experts-achieving-80-top-1", "usesAdditionalData": false}, {"x": "2021-07-08", "y": 0.8, "name": "CoE-Small + CondConv + PWLU", "nameShort": "CoE-Small + CondConv + PWLU", "nameDetails": null, "paperSlug": "collaboration-of-experts-achieving-80-top-1", "usesAdditionalData": false}, {"x": "2021-07-13", "y": 0.8420000000000001, "name": "ViP-B|384", "nameShort": "ViP-B|384", "nameDetails": null, "paperSlug": "visual-parser-representing-part-whole", "usesAdditionalData": false}, {"x": "2021-07-21", "y": 0.8320000000000001, "name": "CycleMLP-B5", "nameShort": "CycleMLP-B5", "nameDetails": null, "paperSlug": "cyclemlp-a-mlp-like-architecture-for-dense", "usesAdditionalData": false}, {"x": "2021-07-23", "y": 0.7709999999999999, "name": "SkipblockNet-L", "nameShort": "SkipblockNet-L", "nameDetails": null, "paperSlug": "bias-loss-for-mobile-neural-networks", "usesAdditionalData": false}, {"x": "2021-07-23", "y": 0.762, "name": "SkipblockNet-M", "nameShort": "SkipblockNet-M", "nameDetails": null, "paperSlug": "bias-loss-for-mobile-neural-networks", "usesAdditionalData": false}, {"x": "2021-07-25", "y": 0.8009000000000001, "name": "WideNet-H", "nameShort": "WideNet-H", "nameDetails": null, "paperSlug": "go-wider-instead-of-deeper", "usesAdditionalData": false}, {"x": "2021-07-25", "y": 0.7948999999999999, "name": "WideNet-L", "nameShort": "WideNet-L", "nameDetails": null, "paperSlug": "go-wider-instead-of-deeper", "usesAdditionalData": false}, {"x": "2021-07-25", "y": 0.7754000000000001, "name": "WideNet-B", "nameShort": "WideNet-B", "nameDetails": null, "paperSlug": "go-wider-instead-of-deeper", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.846, "name": "SE-CoTNetD-152", "nameShort": "SE-CoTNetD-152", "nameDetails": null, "paperSlug": "contextual-transformer-networks-for-visual", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.8320000000000001, "name": "SE-CoTNetD-101", "nameShort": "SE-CoTNetD-101", "nameDetails": null, "paperSlug": "contextual-transformer-networks-for-visual", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.818, "name": "ResNet-200", "nameShort": "ResNet-200", "nameDetails": null, "paperSlug": "parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.816, "name": "SE-CoTNetD-50", "nameShort": "SE-CoTNetD-50", "nameDetails": null, "paperSlug": "contextual-transformer-networks-for-visual", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.813, "name": "ResNet-152", "nameShort": "ResNet-152", "nameDetails": null, "paperSlug": "parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2021-07-26", "y": 0.809, "name": "ResNet-101", "nameShort": "ResNet-101", "nameDetails": null, "paperSlug": "parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.8240000000000001, "name": "DeiT-B with iRPE-K", "nameShort": "DeiT-B with iRPE-K", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.8140000000000001, "name": "DeiT-S with iRPE-QKV", "nameShort": "DeiT-S with iRPE-QKV", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.8109999999999999, "name": "DeiT-S with iRPE-QK", "nameShort": "DeiT-S with iRPE-QK", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.809, "name": "DeiT-S with iRPE-K", "nameShort": "DeiT-S with iRPE-K", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-07-29", "y": 0.737, "name": "DeiT-Ti with iRPE-K", "nameShort": "DeiT-Ti with iRPE-K", "nameDetails": null, "paperSlug": "rethinking-and-improving-relative-position", "usesAdditionalData": false}, {"x": "2021-08-03", "y": 0.8220000000000001, "name": "Evo-LeViT-384*", "nameShort": "Evo-LeViT-384*", "nameDetails": null, "paperSlug": "evo-vit-slow-fast-token-evolution-for-dynamic", "usesAdditionalData": false}, {"x": "2021-08-17", "y": 0.7903, "name": "Co-ResNet-152", "nameShort": "Co-ResNet-152", "nameDetails": null, "paperSlug": "contextual-convolutional-neural-networks", "usesAdditionalData": false}, {"x": "2021-09-09", "y": 0.802, "name": "ConvMLP-L", "nameShort": "ConvMLP-L", "nameDetails": null, "paperSlug": "convmlp-hierarchical-convolutional-mlps-for", "usesAdditionalData": false}, {"x": "2021-09-09", "y": 0.79, "name": "ConvMLP-M", "nameShort": "ConvMLP-M", "nameDetails": null, "paperSlug": "convmlp-hierarchical-convolutional-mlps-for", "usesAdditionalData": false}, {"x": "2021-09-09", "y": 0.768, "name": "ConvMLP-S", "nameShort": "ConvMLP-S", "nameDetails": null, "paperSlug": "convmlp-hierarchical-convolutional-mlps-for", "usesAdditionalData": false}, {"x": "2021-09-12", "y": 0.8340000000000001, "name": "sMLPNet-B ", "nameShort": "sMLPNet-B ", "nameDetails": "ImageNet-1k", "paperSlug": "sparse-mlp-for-image-recognition-is-self", "usesAdditionalData": false}, {"x": "2021-09-12", "y": 0.831, "name": "sMLPNet-S ", "nameShort": "sMLPNet-S ", "nameDetails": "ImageNet-1k", "paperSlug": "sparse-mlp-for-image-recognition-is-self", "usesAdditionalData": false}, {"x": "2021-09-12", "y": 0.8190000000000001, "name": "sMLPNet-T ", "nameShort": "sMLPNet-T ", "nameDetails": "ImageNet-1k", "paperSlug": "sparse-mlp-for-image-recognition-is-self", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.8290000000000001, "name": "NASViT ", "nameShort": "NASViT ", "nameDetails": "supernet", "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.8220000000000001, "name": "ConvMixer-1536/20", "nameShort": "ConvMixer-1536/20", "nameDetails": null, "paperSlug": "patches-are-all-you-need-1", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.818, "name": "NASViT-A5", "nameShort": "NASViT-A5", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.8140000000000001, "name": "NASViT-A4", "nameShort": "NASViT-A4", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.81, "name": "NASViT-A3", "nameShort": "NASViT-A3", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.805, "name": "NASViT-A2", "nameShort": "NASViT-A2", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.797, "name": "NASViT-A1", "nameShort": "NASViT-A1", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-09-29", "y": 0.782, "name": "NASViT-A0", "nameShort": "NASViT-A0", "nameDetails": null, "paperSlug": "nasvit-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.8240000000000001, "name": "ResNet-152 ", "nameShort": "ResNet-152 ", "nameDetails": "A2 + reg", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.818, "name": "ResNet-152 ", "nameShort": "ResNet-152 ", "nameDetails": "A2", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.804, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "T2", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.804, "name": "ResNet50 ", "nameShort": "ResNet50 ", "nameDetails": "A1", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-01", "y": 0.7809999999999999, "name": "ResNet50 ", "nameShort": "ResNet50 ", "nameDetails": "A3", "paperSlug": "resnet-strikes-back-an-improved-training", "usesAdditionalData": false}, {"x": "2021-10-05", "y": 0.784, "name": "MobileViT-S", "nameShort": "MobileViT-S", "nameDetails": null, "paperSlug": "mobilevit-light-weight-general-purpose-and", "usesAdditionalData": false}, {"x": "2021-10-05", "y": 0.748, "name": "MobileViT-XS", "nameShort": "MobileViT-XS", "nameDetails": null, "paperSlug": "mobilevit-light-weight-general-purpose-and", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.852, "name": "UniNet-B5", "nameShort": "UniNet-B5", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.8420000000000001, "name": "UniNet-B4", "nameShort": "UniNet-B4", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.8270000000000001, "name": "UniNet-B2", "nameShort": "UniNet-B2", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.804, "name": "UniNet-B1", "nameShort": "UniNet-B1", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-08", "y": 0.7909999999999999, "name": "UniNet-B0", "nameShort": "UniNet-B0", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with", "usesAdditionalData": false}, {"x": "2021-10-18", "y": 0.828, "name": "HRFormer-B", "nameShort": "HRFormer-B", "nameDetails": null, "paperSlug": "hrformer-high-resolution-transformer-for", "usesAdditionalData": false}, {"x": "2021-10-18", "y": 0.785, "name": "HRFormer-T", "nameShort": "HRFormer-T", "nameDetails": null, "paperSlug": "hrformer-high-resolution-transformer-for", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.848, "name": "SReT-B ", "nameShort": "SReT-B ", "nameDetails": "384 res, ImageNet-1K only", "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.843, "name": "SReT-S ", "nameShort": "SReT-S ", "nameDetails": "512 res, ImageNet-1K only", "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.838, "name": "SReT-S ", "nameShort": "SReT-S ", "nameDetails": "384 res, ImageNet-1K only", "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.7759999999999999, "name": "SReT-T", "nameShort": "SReT-T", "nameDetails": null, "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-09", "y": 0.74, "name": "SReT-ExT", "nameShort": "SReT-ExT", "nameDetails": null, "paperSlug": "sliced-recursive-transformer-1", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.878, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-H, 448", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.8690000000000001, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-H", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.8590000000000001, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-L", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-11", "y": 0.836, "name": "MAE ", "nameShort": "MAE ", "nameDetails": "ViT-L", "paperSlug": "masked-autoencoders-are-scalable-vision", "usesAdditionalData": false}, {"x": "2021-11-18", "y": 0.9017000000000001, "name": "SwinV2-G", "nameShort": "SwinV2-G", "nameDetails": null, "paperSlug": "swin-transformer-v2-scaling-up-capacity-and", "usesAdditionalData": false}, {"x": "2021-11-18", "y": 0.871, "name": "SwinV2-B", "nameShort": "SwinV2-B", "nameDetails": null, "paperSlug": "swin-transformer-v2-scaling-up-capacity-and", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.841, "name": "FBNetV5-F-CLS", "nameShort": "FBNetV5-F-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.826, "name": "FBNetV5-C-CLS", "nameShort": "FBNetV5-C-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.818, "name": "FBNetV5", "nameShort": "FBNetV5", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.8170000000000001, "name": "FBNetV5-A-CLS", "nameShort": "FBNetV5-A-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.784, "name": "FBNetV5-AC-CLS", "nameShort": "FBNetV5-AC-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-19", "y": 0.772, "name": "FBNetV5-AR-CLS", "nameShort": "FBNetV5-AR-CLS", "nameDetails": null, "paperSlug": "fbnetv5-neural-architecture-search-for", "usesAdditionalData": false}, {"x": "2021-11-20", "y": 0.8506999999999999, "name": "DiscreteViT", "nameShort": "DiscreteViT", "nameDetails": null, "paperSlug": "discrete-representations-strengthen-vision-1", "usesAdditionalData": false}, {"x": "2021-11-22", "y": 0.9005, "name": "Florence-CoSwin-H", "nameShort": "Florence-CoSwin-H", "nameDetails": null, "paperSlug": "florence-a-new-foundation-model-for-computer", "usesAdditionalData": false}, {"x": "2021-11-22", "y": 0.825, "name": "MetaFormer PoolFormer-M48", "nameShort": "MetaFormer PoolFormer-M48", "nameDetails": null, "paperSlug": "metaformer-is-actually-what-you-need-for", "usesAdditionalData": false}, {"x": "2021-11-24", "y": 0.883, "name": "PeCo ", "nameShort": "PeCo ", "nameDetails": "ViT-H, 448", "paperSlug": "peco-perceptual-codebook-for-bert-pre", "usesAdditionalData": false}, {"x": "2021-11-24", "y": 0.875, "name": "PeCo ", "nameShort": "PeCo ", "nameDetails": "ViT-H, 224", "paperSlug": "peco-perceptual-codebook-for-bert-pre", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.8108, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.7940999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.7635, "name": "ResNet-34 ", "nameShort": "ResNet-34 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-11-30", "y": 0.7232999999999999, "name": "ResNet-18 ", "nameShort": "ResNet-18 ", "nameDetails": "SAMix", "paperSlug": "boosting-discriminative-visual-representation", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.888, "name": "MViTv2-H ", "nameShort": "MViTv2-H ", "nameDetails": "512 res, ImageNet-21k pretrain", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.884, "name": "MViTv2-L ", "nameShort": "MViTv2-L ", "nameDetails": "384 res, ImageNet-21k pretrain", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.88, "name": "MViTv2-H ", "nameShort": "MViTv2-H ", "nameDetails": "mageNet-21k pretrain", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.863, "name": "MViTv2-L ", "nameShort": "MViTv2-L ", "nameDetails": "384 res", "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.823, "name": "MViTv2-T", "nameShort": "MViTv2-T", "nameDetails": null, "paperSlug": "improved-multiscale-vision-transformers-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.8190000000000001, "name": "ResNet-101 ", "nameShort": "ResNet-101 ", "nameDetails": "224 res, Fast Knowledge Distillation", "paperSlug": "a-fast-knowledge-distillation-framework-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.8009999999999999, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "224 res, Fast Knowledge Distillation", "paperSlug": "a-fast-knowledge-distillation-framework-for", "usesAdditionalData": false}, {"x": "2021-12-02", "y": 0.787, "name": "SReT-LT ", "nameShort": "SReT-LT ", "nameDetails": "Fast Knowledge Distillation", "paperSlug": "a-fast-knowledge-distillation-framework-for", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.8370000000000001, "name": "QnA-ViT-Base", "nameShort": "QnA-ViT-Base", "nameDetails": null, "paperSlug": "learned-queries-for-efficient-local-attention", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.8320000000000001, "name": "QnA-ViT-Small", "nameShort": "QnA-ViT-Small", "nameDetails": null, "paperSlug": "learned-queries-for-efficient-local-attention", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.818, "name": "RepMLPNet-L256", "nameShort": "RepMLPNet-L256", "nameDetails": null, "paperSlug": "repmlpnet-hierarchical-vision-mlp-with-re", "usesAdditionalData": false}, {"x": "2021-12-21", "y": 0.8170000000000001, "name": "QnA-ViT-Tiny", "nameShort": "QnA-ViT-Tiny", "nameDetails": null, "paperSlug": "learned-queries-for-efficient-local-attention", "usesAdditionalData": false}, {"x": "2021-12-23", "y": 0.872, "name": "ELSA-VOLO-D5 ", "nameShort": "ELSA-VOLO-D5 ", "nameDetails": "512*512", "paperSlug": "elsa-enhanced-local-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-12-23", "y": 0.847, "name": "ELSA-VOLO-D1", "nameShort": "ELSA-VOLO-D1", "nameDetails": null, "paperSlug": "elsa-enhanced-local-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-12-23", "y": 0.8270000000000001, "name": "ELSA-Swin-T", "nameShort": "ELSA-Swin-T", "nameDetails": null, "paperSlug": "elsa-enhanced-local-self-attention-for-vision", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.871, "name": "PatchConvNet-L120-21k-384", "nameShort": "PatchConvNet-L120-21k-384", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.865, "name": "PatchConvNet-B60-21k-384", "nameShort": "PatchConvNet-B60-21k-384", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.8540000000000001, "name": "PatchConvNet-S60-21k-512", "nameShort": "PatchConvNet-S60-21k-512", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.841, "name": "PatchConvNet-B120", "nameShort": "PatchConvNet-B120", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.835, "name": "PatchConvNet-B60", "nameShort": "PatchConvNet-B60", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.8320000000000001, "name": "PatchConvNet-S120", "nameShort": "PatchConvNet-S120", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2021-12-27", "y": 0.821, "name": "PatchConvNet-S60", "nameShort": "PatchConvNet-S60", "nameDetails": null, "paperSlug": "augmenting-convolutional-networks-with", "usesAdditionalData": false}, {"x": "2022-01-03", "y": 0.848, "name": "DAT-B ", "nameShort": "DAT-B ", "nameDetails": "384 res, IN-1K only", "paperSlug": "vision-transformer-with-deformable-attention", "usesAdditionalData": false}, {"x": "2022-01-03", "y": 0.8370000000000001, "name": "DAT-S", "nameShort": "DAT-S", "nameDetails": null, "paperSlug": "vision-transformer-with-deformable-attention", "usesAdditionalData": false}, {"x": "2022-01-03", "y": 0.82, "name": "DAT-T", "nameShort": "DAT-T", "nameDetails": null, "paperSlug": "vision-transformer-with-deformable-attention", "usesAdditionalData": false}, {"x": "2022-01-10", "y": 0.878, "name": "ConvNeXt-XL ", "nameShort": "ConvNeXt-XL ", "nameDetails": "ImageNet-22k", "paperSlug": "a-convnet-for-the-2020s", "usesAdditionalData": false}, {"x": "2022-01-10", "y": 0.855, "name": "ConvNeXt-L ", "nameShort": "ConvNeXt-L ", "nameDetails": "384 res", "paperSlug": "a-convnet-for-the-2020s", "usesAdditionalData": false}, {"x": "2022-01-10", "y": 0.821, "name": "ConvNeXt-T", "nameShort": "ConvNeXt-T", "nameDetails": null, "paperSlug": "a-convnet-for-the-2020s", "usesAdditionalData": false}, {"x": "2022-01-20", "y": 0.8859999999999999, "name": "SWAG ", "nameShort": "SWAG ", "nameDetails": "ViT H/14", "paperSlug": "revisiting-weakly-supervised-pre-training-of", "usesAdditionalData": false}, {"x": "2022-01-20", "y": 0.86, "name": "Omnivore ", "nameShort": "Omnivore ", "nameDetails": "Swin-L", "paperSlug": "omnivore-a-single-model-for-many-visual", "usesAdditionalData": false}, {"x": "2022-01-20", "y": 0.853, "name": "Omnivore ", "nameShort": "Omnivore ", "nameDetails": "Swin-B", "paperSlug": "omnivore-a-single-model-for-many-visual", "usesAdditionalData": false}, {"x": "2022-01-24", "y": 0.863, "name": "UniFormer-L ", "nameShort": "UniFormer-L ", "nameDetails": "384 res", "paperSlug": "uniformer-unifying-convolution-and-self", "usesAdditionalData": false}, {"x": "2022-01-24", "y": 0.856, "name": "UniFormer-L", "nameShort": "UniFormer-L", "nameDetails": null, "paperSlug": "uniformer-unifying-convolution-and-self", "usesAdditionalData": false}, {"x": "2022-01-24", "y": 0.8340000000000001, "name": "UniFormer-S", "nameShort": "UniFormer-S", "nameDetails": null, "paperSlug": "uniformer-unifying-convolution-and-self", "usesAdditionalData": false}, {"x": "2022-01-26", "y": 0.833, "name": "Shift-B", "nameShort": "Shift-B", "nameDetails": null, "paperSlug": "when-shift-operation-meets-vision-transformer", "usesAdditionalData": false}, {"x": "2022-01-26", "y": 0.828, "name": "Shift-S", "nameShort": "Shift-S", "nameDetails": null, "paperSlug": "when-shift-operation-meets-vision-transformer", "usesAdditionalData": false}, {"x": "2022-01-26", "y": 0.8170000000000001, "name": "Shift-T", "nameShort": "Shift-T", "nameDetails": null, "paperSlug": "when-shift-operation-meets-vision-transformer", "usesAdditionalData": false}, {"x": "2022-02-07", "y": 0.866, "name": "data2vec", "nameShort": "data2vec", "nameDetails": null, "paperSlug": "data2vec-a-general-framework-for-self-1", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.865, "name": "MKD ViT-L", "nameShort": "MKD ViT-L", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.858, "name": "SEER ", "nameShort": "SEER ", "nameDetails": "RG-10B", "paperSlug": "vision-models-are-more-robust-and-fair-when", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.851, "name": "MKD ViT-B", "nameShort": "MKD ViT-B", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.831, "name": "MKD ViT-S", "nameShort": "MKD ViT-S", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-16", "y": 0.7709999999999999, "name": "MKD ViT-T", "nameShort": "MKD ViT-T", "nameDetails": null, "paperSlug": "meta-knowledge-distillation", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.878, "name": "VAN-B6 ", "nameShort": "VAN-B6 ", "nameDetails": "22K, 384res", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.87, "name": "VAN-B5 ", "nameShort": "VAN-B5 ", "nameDetails": "22K, 384res", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.866, "name": "VAN-B4 ", "nameShort": "VAN-B4 ", "nameDetails": "22K, 384res", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.863, "name": "VAN-B5 ", "nameShort": "VAN-B5 ", "nameDetails": "22K", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.857, "name": "VAN-B4 ", "nameShort": "VAN-B4 ", "nameDetails": "22K", "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.828, "name": "VAN-B2", "nameShort": "VAN-B2", "nameDetails": null, "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.8109999999999999, "name": "VAN-B1", "nameShort": "VAN-B1", "nameDetails": null, "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-20", "y": 0.754, "name": "VAN-B0", "nameShort": "VAN-B0", "nameDetails": null, "paperSlug": "visual-attention-network", "usesAdditionalData": false}, {"x": "2022-02-21", "y": 0.885, "name": "ViTAE-H + MAE ", "nameShort": "ViTAE-H + MAE ", "nameDetails": "448", "paperSlug": "vitaev2-vision-transformer-advanced-by", "usesAdditionalData": false}, {"x": "2022-03-08", "y": 0.7863, "name": "EdgeFormer-S", "nameShort": "EdgeFormer-S", "nameDetails": null, "paperSlug": "edgeformer-improving-light-weight-convnets-by", "usesAdditionalData": false}, {"x": "2022-03-10", "y": 0.9094, "name": "Model soups ", "nameShort": "Model soups ", "nameDetails": "ViT-G/14", "paperSlug": "model-soups-averaging-weights-of-multiple", "usesAdditionalData": false}, {"x": "2022-03-11", "y": 0.836, "name": "ActiveMLP-L", "nameShort": "ActiveMLP-L", "nameDetails": null, "paperSlug": "activemlp-an-mlp-like-architecture-with", "usesAdditionalData": false}, {"x": "2022-03-11", "y": 0.82, "name": "ActiveMLP-T", "nameShort": "ActiveMLP-T", "nameDetails": null, "paperSlug": "activemlp-an-mlp-like-architecture-with", "usesAdditionalData": false}, {"x": "2022-03-13", "y": 0.878, "name": "RepLKNet-XL", "nameShort": "RepLKNet-XL", "nameDetails": null, "paperSlug": "scaling-up-your-kernels-to-31x31-revisiting", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.855, "name": "ViT-L@384 ", "nameShort": "ViT-L@384 ", "nameDetails": "attn finetune", "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.843, "name": "ViT-B@384 ", "nameShort": "ViT-B@384 ", "nameDetails": "attn finetune", "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.841, "name": "ViT-B-36x1", "nameShort": "ViT-B-36x1", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.841, "name": "ViT-B-18x2", "nameShort": "ViT-B-18x2", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.8340000000000001, "name": "ViT-B ", "nameShort": "ViT-B ", "nameDetails": "hMLP + BeiT", "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.826, "name": "ViT-S-24x2", "nameShort": "ViT-S-24x2", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-18", "y": 0.823, "name": "ViT-S-48x1", "nameShort": "ViT-S-48x1", "nameDetails": null, "paperSlug": "three-things-everyone-should-know-about", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8197, "name": "Swin-T ", "nameShort": "Swin-T ", "nameDetails": "SAMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8192, "name": "Swin-T ", "nameShort": "Swin-T ", "nameDetails": "AutoMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8116, "name": "Swin-T ", "nameShort": "Swin-T ", "nameDetails": "PuzzleMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8112, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "SAMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8090999999999999, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "AutoMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.8025, "name": "DeiT-S ", "nameShort": "DeiT-S ", "nameDetails": "PuzzleMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.7875, "name": "SAMix+DM ", "nameShort": "SAMix+DM ", "nameDetails": "ResNet-50 RSB A3", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.7862, "name": "AutoMix+DM ", "nameShort": "AutoMix+DM ", "nameDetails": "ResNet-50 RSB A3", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.7815000000000001, "name": "PuzzleMix+DM ", "nameShort": "PuzzleMix+DM ", "nameDetails": "ResNet-50 RSB A3", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.2229, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "PuzzleMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.21850000000000003, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "AutoMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-21", "y": 0.2164, "name": "ResNet-50 ", "nameShort": "ResNet-50 ", "nameDetails": "SAMix+DM", "paperSlug": "decoupled-mixup-for-data-efficient-learning", "usesAdditionalData": false}, {"x": "2022-03-29", "y": 0.7613, "name": "ELP ", "nameShort": "ELP ", "nameDetails": "naive ResNet50", "paperSlug": "a-simple-episodic-linear-probe-improves", "usesAdditionalData": false}, {"x": "2022-04-03", "y": 0.873, "name": "VOLO-D5+HAT", "nameShort": "VOLO-D5+HAT", "nameDetails": null, "paperSlug": "improving-vision-transformers-by-revisiting", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8953, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "512res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8941, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "384res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8941, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "512res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8912, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "384res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8881999999999999, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "512res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.887, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "512res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8869, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "384res, JFT", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8851, "name": "MaxViT-XL ", "nameShort": "MaxViT-XL ", "nameDetails": "384res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8845999999999999, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "512res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8837999999999999, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "512res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8832, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "384res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8824, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "384res, 21K", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.867, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "512res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8640000000000001, "name": "MaxViT-L ", "nameShort": "MaxViT-L ", "nameDetails": "384res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8634000000000001, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "384res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8619, "name": "MaxViT-S ", "nameShort": "MaxViT-S ", "nameDetails": "512res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8572, "name": "MaxViT-T", "nameShort": "MaxViT-T", "nameDetails": "512res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8523999999999999, "name": "MaxViT-T ", "nameShort": "MaxViT-T ", "nameDetails": "384res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8495, "name": "MaxViT-B ", "nameShort": "MaxViT-B ", "nameDetails": "224res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8445, "name": "MaxViT-S ", "nameShort": "MaxViT-S ", "nameDetails": "224res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-04", "y": 0.8362, "name": "MaxViT-T ", "nameShort": "MaxViT-T ", "nameDetails": "224res", "paperSlug": "maxvit-multi-axis-vision-transformer", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.904, "name": "DaViT-G", "nameShort": "DaViT-G", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.902, "name": "DaViT-H", "nameShort": "DaViT-H", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.875, "name": "DaViT-L ", "nameShort": "DaViT-L ", "nameDetails": "ImageNet-22k", "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.8690000000000001, "name": "DaViT-B ", "nameShort": "DaViT-B ", "nameDetails": "ImageNet-22k", "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.846, "name": "DaViT-B", "nameShort": "DaViT-B", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.8420000000000001, "name": "DaViT-S", "nameShort": "DaViT-S", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-07", "y": 0.828, "name": "DaViT-T", "nameShort": "DaViT-T", "nameDetails": null, "paperSlug": "davit-dual-attention-vision-transformers", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.867, "name": "ViT-B @384 ", "nameShort": "ViT-B @384 ", "nameDetails": "DeiT III, 21k", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.858, "name": "ViT-L @384 ", "nameShort": "ViT-L @384 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.857, "name": "ViT-B @224 ", "nameShort": "ViT-B @224 ", "nameDetails": "DeiT III, 21k", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.855, "name": "Mini-Swin-B@384", "nameShort": "Mini-Swin-B@384", "nameDetails": null, "paperSlug": "minivit-compressing-vision-transformers-with", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.852, "name": "ViT-H @224 ", "nameShort": "ViT-H @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.85, "name": "ViT-B @384 ", "nameShort": "ViT-B @384 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8490000000000001, "name": "ViT-L @224 ", "nameShort": "ViT-L @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.843, "name": "NAT-Base", "nameShort": "NAT-Base", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.838, "name": "ViT-B @224 ", "nameShort": "ViT-B @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8370000000000001, "name": "NAT-Small", "nameShort": "NAT-Small", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8340000000000001, "name": "ViT-S @384 ", "nameShort": "ViT-S @384 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8320000000000001, "name": "NAT-Tiny", "nameShort": "NAT-Tiny", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.831, "name": "ViT-S @224 ", "nameShort": "ViT-S @224 ", "nameDetails": "DeiT III, 21k", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.818, "name": "NAT-Mini", "nameShort": "NAT-Mini", "nameDetails": null, "paperSlug": "neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-04-14", "y": 0.8140000000000001, "name": "ViT-S @224 ", "nameShort": "ViT-S @224 ", "nameDetails": "DeiT III", "paperSlug": "deit-iii-revenge-of-the-vit", "usesAdditionalData": false}, {"x": "2022-04-26", "y": 0.872, "name": "EfficientNetV2 ", "nameShort": "EfficientNetV2 ", "nameDetails": "PolyLoss", "paperSlug": "polyloss-a-polynomial-expansion-perspective-1", "usesAdditionalData": false}, {"x": "2022-04-26", "y": 0.871, "name": "FAN-L-Hybrid++", "nameShort": "FAN-L-Hybrid++", "nameDetails": null, "paperSlug": "understanding-the-robustness-in-vision", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.9059999999999999, "name": "CoCa ", "nameShort": "CoCa ", "nameDetails": "frozen", "paperSlug": "coca-contrastive-captioners-are-image-text", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.846, "name": "Sequencer2D-L\u2191392", "nameShort": "Sequencer2D-L\u2191392", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.8340000000000001, "name": "Sequencer2D-L", "nameShort": "Sequencer2D-L", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.828, "name": "Sequencer2D-M", "nameShort": "Sequencer2D-M", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-04", "y": 0.823, "name": "Sequencer2D-S", "nameShort": "Sequencer2D-S", "nameDetails": null, "paperSlug": "sequencer-deep-lstm-for-image-classification", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8661, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:ViT+D:EffNet-B7", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8645999999999999, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:ViT+D:VOLO-D3", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8642, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:ConvNeXt-L+D:EffNet-B7", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8528, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:D1+D:D5", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-19", "y": 0.8388, "name": "CLCNet ", "nameShort": "CLCNet ", "nameDetails": "S:B4+D:B7", "paperSlug": "clcnet-rethinking-of-ensemble-modeling-with", "usesAdditionalData": false}, {"x": "2022-05-21", "y": 0.871, "name": "Bamboo ", "nameShort": "Bamboo ", "nameDetails": "Bamboo-H", "paperSlug": "deeper-vs-wider-a-revisit-of-transformer", "usesAdditionalData": false}, {"x": "2022-05-21", "y": 0.863, "name": "Bamboo ", "nameShort": "Bamboo ", "nameDetails": "Bamboo-L", "paperSlug": "deeper-vs-wider-a-revisit-of-transformer", "usesAdditionalData": false}, {"x": "2022-05-21", "y": 0.8420000000000001, "name": "Bamboo ", "nameShort": "Bamboo ", "nameDetails": "Bamboo-B", "paperSlug": "deeper-vs-wider-a-revisit-of-transformer", "usesAdditionalData": false}, {"x": "2022-05-25", "y": 0.8674, "name": "\u00b52Net ", "nameShort": "\u00b52Net ", "nameDetails": "ViT-L/16", "paperSlug": "an-evolutionary-approach-to-dynamic", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.851, "name": "MixMIM-B", "nameShort": "MixMIM-B", "nameDetails": null, "paperSlug": "mixmim-mixed-and-masked-image-modeling-for", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.847, "name": "LITv2-B|384", "nameShort": "LITv2-B|384", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8367, "name": "TransBoost-ViT-S", "nameShort": "TransBoost-ViT-S", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.836, "name": "LITv2-B", "nameShort": "LITv2-B", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.833, "name": "LITv2-M", "nameShort": "LITv2-M", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8245999999999999, "name": "TransBoost-ConvNext-T", "nameShort": "TransBoost-ConvNext-T", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8216, "name": "TransBoost-Swin-T", "nameShort": "TransBoost-Swin-T", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.82, "name": "LITv2-S", "nameShort": "LITv2-S", "nameDetails": null, "paperSlug": "fast-vision-transformers-with-hilo-attention", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8115000000000001, "name": "TransBoost-ResNet50-StrikesBack", "nameShort": "TransBoost-ResNet50-StrikesBack", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.8064, "name": "TransBoost-ResNet152", "nameShort": "TransBoost-ResNet152", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7986, "name": "TransBoost-ResNet101", "nameShort": "TransBoost-ResNet101", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7903, "name": "TransBoost-ResNet50", "nameShort": "TransBoost-ResNet50", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7859999999999999, "name": "TransBoost-EfficientNetB0", "nameShort": "TransBoost-EfficientNetB0", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7681, "name": "TransBoost-MobileNetV3-L", "nameShort": "TransBoost-MobileNetV3-L", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.767, "name": "TransBoost-ResNet34", "nameShort": "TransBoost-ResNet34", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-26", "y": 0.7336, "name": "TransBoost-ResNet18", "nameShort": "TransBoost-ResNet18", "nameDetails": null, "paperSlug": "transboost-improving-the-best-imagenet", "usesAdditionalData": false}, {"x": "2022-05-27", "y": 0.89, "name": "FD ", "nameShort": "FD ", "nameDetails": "CLIP ViT-L-336", "paperSlug": "contrastive-learning-rivals-masked-image", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.8370000000000001, "name": "Pyramid ViG-B", "nameShort": "Pyramid ViG-B", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.831, "name": "Pyramid ViG-M", "nameShort": "Pyramid ViG-M", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.821, "name": "Pyramid ViG-S", "nameShort": "Pyramid ViG-S", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-01", "y": 0.782, "name": "Pyramid ViG-Ti", "nameShort": "Pyramid ViG-Ti", "nameDetails": null, "paperSlug": "vision-gnn-an-image-is-worth-graph-of-nodes", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.845, "name": "GC ViT-B", "nameShort": "GC ViT-B", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.84, "name": "GC ViT-S", "nameShort": "GC ViT-S", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.8340000000000001, "name": "GC ViT-T", "nameShort": "GC ViT-T", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.82, "name": "GC ViT-XT", "nameShort": "GC ViT-XT", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-20", "y": 0.7979999999999999, "name": "GC ViT-XXT", "nameShort": "GC ViT-XXT", "nameDetails": null, "paperSlug": "global-context-vision-transformers", "usesAdditionalData": false}, {"x": "2022-06-21", "y": 0.794, "name": "EdgeNeXt-S", "nameShort": "EdgeNeXt-S", "nameDetails": null, "paperSlug": "edgenext-efficiently-amalgamated-cnn", "usesAdditionalData": false}, {"x": "2022-06-21", "y": 0.7120000000000001, "name": "EdgeNeXt-XXS", "nameShort": "EdgeNeXt-XXS", "nameDetails": null, "paperSlug": "edgenext-efficiently-amalgamated-cnn", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.8420000000000001, "name": "RevBiFPN-S6", "nameShort": "RevBiFPN-S6", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.8370000000000001, "name": "RevBiFPN-S5", "nameShort": "RevBiFPN-S5", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.83, "name": "RevBiFPN-S4", "nameShort": "RevBiFPN-S4", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.8109999999999999, "name": "RevBiFPN-S3", "nameShort": "RevBiFPN-S3", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.79, "name": "RevBiFPN-S2", "nameShort": "RevBiFPN-S2", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.759, "name": "RevBiFPN-S1", "nameShort": "RevBiFPN-S1", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-06-28", "y": 0.728, "name": "RevBiFPN-S0", "nameShort": "RevBiFPN-S0", "nameDetails": null, "paperSlug": "revbifpn-the-fully-reversible-bidirectional", "usesAdditionalData": false}, {"x": "2022-07-11", "y": 0.855, "name": "Wave-ViT-L", "nameShort": "Wave-ViT-L", "nameDetails": null, "paperSlug": "wave-vit-unifying-wavelet-and-transformers", "usesAdditionalData": false}, {"x": "2022-07-11", "y": 0.848, "name": "Wave-ViT-B", "nameShort": "Wave-ViT-B", "nameDetails": null, "paperSlug": "wave-vit-unifying-wavelet-and-transformers", "usesAdditionalData": false}, {"x": "2022-07-11", "y": 0.8390000000000001, "name": "Wave-ViT-S", "nameShort": "Wave-ViT-S", "nameDetails": null, "paperSlug": "wave-vit-unifying-wavelet-and-transformers", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.8740000000000001, "name": "UniNet-B6", "nameShort": "UniNet-B6", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with-1", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.87, "name": "UniNet-B5", "nameShort": "UniNet-B5", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with-1", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.847, "name": "Next-ViT-L @384", "nameShort": "Next-ViT-L @384", "nameDetails": null, "paperSlug": "next-vit-next-generation-vision-transformer", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.8320000000000001, "name": "Next-ViT-B", "nameShort": "Next-ViT-B", "nameDetails": null, "paperSlug": "next-vit-next-generation-vision-transformer", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.825, "name": "Next-ViT-S", "nameShort": "Next-ViT-S", "nameDetails": null, "paperSlug": "next-vit-next-generation-vision-transformer", "usesAdditionalData": false}, {"x": "2022-07-12", "y": 0.8079999999999999, "name": "UniNet-B0", "nameShort": "UniNet-B0", "nameDetails": null, "paperSlug": "uninet-unified-architecture-search-with-1", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.865, "name": "TinyViT-21M-512-distill ", "nameShort": "TinyViT-21M-512-distill ", "nameDetails": "512 res, 21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.862, "name": "TinyViT-21M-384-distill ", "nameShort": "TinyViT-21M-384-distill ", "nameDetails": "384 res, 21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.848, "name": "TinyViT-21M-distill ", "nameShort": "TinyViT-21M-distill ", "nameDetails": "21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.8320000000000001, "name": "TinyViT-11M-distill ", "nameShort": "TinyViT-11M-distill ", "nameDetails": "21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.831, "name": "TinyViT-21M", "nameShort": "TinyViT-21M", "nameDetails": null, "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.815, "name": "TinyViT-11M", "nameShort": "TinyViT-11M", "nameDetails": null, "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.807, "name": "TinyViT-5M-distill ", "nameShort": "TinyViT-5M-distill ", "nameDetails": "21k", "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-21", "y": 0.7909999999999999, "name": "TinyViT-5M", "nameShort": "TinyViT-5M", "nameDetails": null, "paperSlug": "tinyvit-fast-pretraining-distillation-for", "usesAdditionalData": false}, {"x": "2022-07-28", "y": 0.877, "name": "HorNet-L ", "nameShort": "HorNet-L ", "nameDetails": "GF", "paperSlug": "hornet-efficient-high-order-spatial", "usesAdditionalData": false}, {"x": "2022-08-24", "y": 0.8301000000000001, "name": "gSwin-S", "nameShort": "gSwin-S", "nameDetails": null, "paperSlug": "gswin-gated-mlp-vision-model-with", "usesAdditionalData": false}, {"x": "2022-08-24", "y": 0.8170999999999999, "name": "gSwin-T", "nameShort": "gSwin-T", "nameDetails": null, "paperSlug": "gswin-gated-mlp-vision-model-with", "usesAdditionalData": false}, {"x": "2022-08-24", "y": 0.8031999999999999, "name": "gSwin-VT", "nameShort": "gSwin-VT", "nameDetails": null, "paperSlug": "gswin-gated-mlp-vision-model-with", "usesAdditionalData": false}, {"x": "2022-09-08", "y": 0.882, "name": "dBOT ViT-H ", "nameShort": "dBOT ViT-H ", "nameDetails": "CLIP as Teacher", "paperSlug": "exploring-target-representations-for-masked", "usesAdditionalData": false}, {"x": "2022-09-08", "y": 0.878, "name": "dBOT ViT-L ", "nameShort": "dBOT ViT-L ", "nameDetails": "CLIP as Teacher", "paperSlug": "exploring-target-representations-for-masked", "usesAdditionalData": false}, {"x": "2022-09-08", "y": 0.857, "name": "dBOT ViT-B ", "nameShort": "dBOT ViT-B ", "nameDetails": "CLIP as Teacher", "paperSlug": "exploring-target-representations-for-masked", "usesAdditionalData": false}, {"x": "2022-09-14", "y": 0.909, "name": "ViT-e", "nameShort": "ViT-e", "nameDetails": null, "paperSlug": "pali-a-jointly-scaled-multilingual-language", "usesAdditionalData": false}, {"x": "2022-09-16", "y": 0.8702, "name": "MAE+DAT ", "nameShort": "MAE+DAT ", "nameDetails": "ViT-H", "paperSlug": "enhance-the-visual-representation-via", "usesAdditionalData": false}, {"x": "2022-09-21", "y": 0.8240000000000001, "name": "Mega", "nameShort": "Mega", "nameDetails": null, "paperSlug": "mega-moving-average-equipped-gated-attention", "usesAdditionalData": false}, {"x": "2022-09-26", "y": 0.8601000000000001, "name": "GPaCo ", "nameShort": "GPaCo ", "nameDetails": "ViT-L", "paperSlug": "generalized-parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2022-09-26", "y": 0.84, "name": "GPaCo ", "nameShort": "GPaCo ", "nameDetails": "Vit-B", "paperSlug": "generalized-parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2022-09-26", "y": 0.797, "name": "GPaCo ", "nameShort": "GPaCo ", "nameDetails": "ResNet-50", "paperSlug": "generalized-parametric-contrastive-learning", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8740000000000001, "name": "DiNAT_s-Large ", "nameShort": "DiNAT_s-Large ", "nameDetails": "384res; Pretrained on IN22K@224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8731, "name": "DiNAT-Large ", "nameShort": "DiNAT-Large ", "nameDetails": "11x11ks; 384res; Pretrained on IN22K@224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8718, "name": "DiNAT-Large ", "nameShort": "DiNAT-Large ", "nameDetails": "384x384; Pretrained on ImageNet-22K @ 224x224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.865, "name": "DiNAT_s-Large ", "nameShort": "DiNAT_s-Large ", "nameDetails": "224x224; Pretrained on ImageNet-22K @ 224x224", "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8440000000000001, "name": "DiNAT-Base", "nameShort": "DiNAT-Base", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.838, "name": "DiNAT-Small", "nameShort": "DiNAT-Small", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.8270000000000001, "name": "DiNAT-Tiny", "nameShort": "DiNAT-Tiny", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-29", "y": 0.818, "name": "DiNAT-Mini", "nameShort": "DiNAT-Mini", "nameDetails": null, "paperSlug": "dilated-neighborhood-attention-transformer", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7929999999999999, "name": "MobileViTv3-S", "nameShort": "MobileViTv3-S", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7864, "name": "MobileViTv3-1.0", "nameShort": "MobileViTv3-1.0", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.767, "name": "MobileViTv3-XS", "nameShort": "MobileViTv3-XS", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7655, "name": "MobileViTv3-0.75", "nameShort": "MobileViTv3-0.75", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7232999999999999, "name": "MobileViTv3-0.5", "nameShort": "MobileViTv3-0.5", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-09-30", "y": 0.7098, "name": "MobileViTv3-XXS", "nameShort": "MobileViTv3-XXS", "nameDetails": null, "paperSlug": "mobilevitv3-mobile-friendly-vision", "usesAdditionalData": false}, {"x": "2022-10-04", "y": 0.8909999999999999, "name": "MOAT-4 22K+1K", "nameShort": "MOAT-4 22K+1K", "nameDetails": null, "paperSlug": "moat-alternating-mobile-convolution-and", "usesAdditionalData": false}, {"x": "2022-10-04", "y": 0.867, "name": "MOAT-3 1K only", "nameShort": "MOAT-3 1K only", "nameDetails": null, "paperSlug": "moat-alternating-mobile-convolution-and", "usesAdditionalData": false}, {"x": "2022-10-04", "y": 0.833, "name": "MOAT-0 1K only", "nameShort": "MOAT-0 1K only", "nameDetails": null, "paperSlug": "moat-alternating-mobile-convolution-and", "usesAdditionalData": false}, {"x": "2022-10-13", "y": 0.677, "name": "WaveMixLite-256/24", "nameShort": "WaveMixLite-256/24", "nameDetails": null, "paperSlug": "wavemix-lite-a-resource-efficient-neural-1", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.846, "name": "MogaNet-L", "nameShort": "MogaNet-L", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.8420000000000001, "name": "MogaNet-B", "nameShort": "MogaNet-B", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.8340000000000001, "name": "MogaNet-S", "nameShort": "MogaNet-S", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.8, "name": "MogaNet-T ", "nameShort": "MogaNet-T ", "nameDetails": "256res", "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.79, "name": "MogaNet-T", "nameShort": "MogaNet-T", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-07", "y": 0.765, "name": "MogaNet-XT", "nameShort": "MogaNet-XT", "nameDetails": null, "paperSlug": "efficient-multi-order-gated-aggregation", "usesAdditionalData": false}, {"x": "2022-11-10", "y": 0.892, "name": "InternImage-DCNv3-H", "nameShort": "InternImage-DCNv3-H", "nameDetails": null, "paperSlug": "internimage-exploring-large-scale-vision", "usesAdditionalData": false}], "yValueFormatString": "#.# %"}}}}</script>
    <script id="evaluation-table-metrics" type="application/json">[{"id": 271, "name": "Top 1 Accuracy", "is_loss": false, "is_fixed": false}, {"id": 272, "name": "Top 5 Accuracy", "is_loss": false, "is_fixed": false}, {"id": 1341, "name": "Number of params", "is_loss": true, "is_fixed": false}, {"id": 44852, "name": "GFLOPs", "is_loss": true, "is_fixed": false}, {"id": 33929, "name": "Hardware Burden", "is_loss": false, "is_fixed": true}, {"id": 33930, "name": "Operations per network pass", "is_loss": false, "is_fixed": true}, {"id": 54446, "name": "Number of parameters (M)", "is_loss": true, "is_fixed": false}]</script>
    <script id="evaluation-table-data" type="application/json">[{"table_id": 116, "row_id": 53996, "rank": 1, "method": "CoCa (finetuned)", "mlmodel": {}, "method_short": "CoCa ", "method_details": "finetuned", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-04", "metrics": {"Top 1 Accuracy": "91.0%", "Top 5 Accuracy": null, "Number of params": "2100M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 91.0, "Top 5 Accuracy": null, "Number of params": 2100000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1004211, "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "url": "/paper/coca-contrastive-captioners-are-image-text", "published": "2022-05-04T00:00:00.000000", "code": true, "review_url": "/paper/coca-contrastive-captioners-are-image-text/review/?hl=53996"}, "external_source_url": null, "tags": [{"id": 247, "name": "ALIGN", "color": "#2771D3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 103, "name": "JFT-3B", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 57543, "rank": 2, "method": "Model soups (BASIC-L)", "mlmodel": {}, "method_short": "Model soups ", "method_details": "BASIC-L", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-10", "metrics": {"Top 1 Accuracy": "90.98%", "Top 5 Accuracy": null, "Number of params": "2440M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.98, "Top 5 Accuracy": null, "Number of params": 2440000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 974897, "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "url": "/paper/model-soups-averaging-weights-of-multiple", "published": "2022-03-10T00:00:00.000000", "code": true, "review_url": "/paper/model-soups-averaging-weights-of-multiple/review/?hl=57543"}, "external_source_url": null, "tags": [{"id": 98, "name": "Conv+Transformer", "color": "#ff2600"}, {"id": 103, "name": "JFT-3B", "color": "#2771D3"}, {"id": 247, "name": "ALIGN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 48974, "rank": 3, "method": "Model soups (ViT-G/14)", "mlmodel": {}, "method_short": "Model soups ", "method_details": "ViT-G/14", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-10", "metrics": {"Top 1 Accuracy": "90.94%", "Top 5 Accuracy": null, "Number of params": "1843M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.94, "Top 5 Accuracy": null, "Number of params": 1843000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 974897, "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "url": "/paper/model-soups-averaging-weights-of-multiple", "published": "2022-03-10T00:00:00.000000", "code": true, "review_url": "/paper/model-soups-averaging-weights-of-multiple/review/?hl=48974"}, "external_source_url": null, "tags": [{"id": 103, "name": "JFT-3B", "color": "#2771D3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 70405, "rank": 4, "method": "ViT-e", "mlmodel": {}, "method_short": "ViT-e", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-14", "metrics": {"Top 1 Accuracy": "90.9%", "Top 5 Accuracy": null, "Number of params": "3900M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.9, "Top 5 Accuracy": null, "Number of params": 3900000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1074922, "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "url": "/paper/pali-a-jointly-scaled-multilingual-language", "published": "2022-09-14T00:00:00.000000", "code": false, "review_url": "/paper/pali-a-jointly-scaled-multilingual-language/review/?hl=70405"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 103, "name": "JFT-3B", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 34580, "rank": 5, "method": "CoAtNet-7", "mlmodel": {}, "method_short": "CoAtNet-7", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-09", "metrics": {"Top 1 Accuracy": "90.88%", "Top 5 Accuracy": null, "Number of params": "2440M", "GFLOPs": "2586", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.88, "Top 5 Accuracy": null, "Number of params": 2440000000.0, "GFLOPs": 2586.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 814553, "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes", "url": "/paper/coatnet-marrying-convolution-and-attention", "published": "2021-06-09T00:00:00.000000", "code": true, "review_url": "/paper/coatnet-marrying-convolution-and-attention/review/?hl=34580"}, "external_source_url": null, "tags": [{"id": 98, "name": "Conv+Transformer", "color": "#ff2600"}, {"id": 103, "name": "JFT-3B", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 53995, "rank": 6, "method": "CoCa (frozen)", "mlmodel": {}, "method_short": "CoCa ", "method_details": "frozen", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-04", "metrics": {"Top 1 Accuracy": "90.60%", "Top 5 Accuracy": null, "Number of params": "2100M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.6, "Top 5 Accuracy": null, "Number of params": 2100000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1004211, "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "url": "/paper/coca-contrastive-captioners-are-image-text", "published": "2022-05-04T00:00:00.000000", "code": true, "review_url": "/paper/coca-contrastive-captioners-are-image-text/review/?hl=53995"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 247, "name": "ALIGN", "color": "#2771D3"}, {"id": 103, "name": "JFT-3B", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 40143, "rank": 7, "method": "CoAtNet-6", "mlmodel": {}, "method_short": "CoAtNet-6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-09", "metrics": {"Top 1 Accuracy": "90.45%", "Top 5 Accuracy": null, "Number of params": "1470M", "GFLOPs": "1521", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.45, "Top 5 Accuracy": null, "Number of params": 1470000000.0, "GFLOPs": 1521.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 814553, "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes", "url": "/paper/coatnet-marrying-convolution-and-attention", "published": "2021-06-09T00:00:00.000000", "code": true, "review_url": "/paper/coatnet-marrying-convolution-and-attention/review/?hl=40143"}, "external_source_url": null, "tags": [{"id": 98, "name": "Conv+Transformer", "color": "#ff2600"}, {"id": 103, "name": "JFT-3B", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 34458, "rank": 8, "method": "ViT-G/14", "mlmodel": {}, "method_short": "ViT-G/14", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-08", "metrics": {"Top 1 Accuracy": "90.45%", "Top 5 Accuracy": null, "Number of params": "1843M", "GFLOPs": "2859.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.45, "Top 5 Accuracy": null, "Number of params": 1843000000.0, "GFLOPs": 2859.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 813674, "title": "Scaling Vision Transformers", "url": "/paper/scaling-vision-transformers", "published": "2021-06-08T00:00:00.000000", "code": true, "review_url": "/paper/scaling-vision-transformers/review/?hl=34458"}, "external_source_url": null, "tags": [{"id": 103, "name": "JFT-3B", "color": "#2771D3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 51577, "rank": 9, "method": "DaViT-G", "mlmodel": {}, "method_short": "DaViT-G", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "90.4%", "Top 5 Accuracy": null, "Number of params": "1437M", "GFLOPs": "1038", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.4, "Top 5 Accuracy": null, "Number of params": 1437000000.0, "GFLOPs": 1038.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=51577"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 24286, "rank": 10, "method": "Meta Pseudo Labels (EfficientNet-L2)", "mlmodel": {}, "method_short": "Meta Pseudo Labels ", "method_details": "EfficientNet-L2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-23", "metrics": {"Top 1 Accuracy": "90.2%", "Top 5 Accuracy": "98.8%", "Number of params": "480M", "GFLOPs": null, "Hardware Burden": "95040G", "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.2, "Top 5 Accuracy": 98.8, "Number of params": 480000000.0, "GFLOPs": null, "Hardware Burden": 95040.0, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188189, "title": "Meta Pseudo Labels", "url": "/paper/meta-pseudo-labels", "published": "2020-03-23T00:00:00.000000", "code": true, "review_url": "/paper/meta-pseudo-labels/review/?hl=24286"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 51583, "rank": 11, "method": "DaViT-H", "mlmodel": {}, "method_short": "DaViT-H", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "90.2%", "Top 5 Accuracy": null, "Number of params": "362M", "GFLOPs": "334", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.2, "Top 5 Accuracy": null, "Number of params": 362000000.0, "GFLOPs": 334.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=51583"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 42925, "rank": 12, "method": "SwinV2-G", "mlmodel": {}, "method_short": "SwinV2-G", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-18", "metrics": {"Top 1 Accuracy": "90.17%", "Top 5 Accuracy": null, "Number of params": "3000M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.17, "Top 5 Accuracy": null, "Number of params": 3000000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912369, "title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "url": "/paper/swin-transformer-v2-scaling-up-capacity-and", "published": "2021-11-18T00:00:00.000000", "code": true, "review_url": "/paper/swin-transformer-v2-scaling-up-capacity-and/review/?hl=42925"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 43156, "rank": 13, "method": "Florence-CoSwin-H", "mlmodel": {}, "method_short": "Florence-CoSwin-H", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-22", "metrics": {"Top 1 Accuracy": "90.05%", "Top 5 Accuracy": "99.02%", "Number of params": "893M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.05, "Top 5 Accuracy": 99.02, "Number of params": 893000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 914419, "title": "Florence: A New Foundation Model for Computer Vision", "url": "/paper/florence-a-new-foundation-model-for-computer", "published": "2021-11-22T00:00:00.000000", "code": true, "review_url": "/paper/florence-a-new-foundation-model-for-computer/review/?hl=43156"}, "external_source_url": null, "tags": [{"id": 248, "name": "FLD-900M", "color": "#2771D3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 24284, "rank": 14, "method": "Meta Pseudo Labels (EfficientNet-B6-Wide)", "mlmodel": {}, "method_short": "Meta Pseudo Labels ", "method_details": "EfficientNet-B6-Wide", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-23", "metrics": {"Top 1 Accuracy": "90%", "Top 5 Accuracy": "98.7%", "Number of params": "390M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 90.0, "Top 5 Accuracy": 98.7, "Number of params": 390000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188189, "title": "Meta Pseudo Labels", "url": "/paper/meta-pseudo-labels", "published": "2020-03-23T00:00:00.000000", "code": true, "review_url": "/paper/meta-pseudo-labels/review/?hl=24284"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 61437, "rank": 15, "method": "MaxViT-XL (512res, JFT)", "mlmodel": {}, "method_short": "MaxViT-XL ", "method_details": "512res, JFT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "89.53%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.53, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=61437"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}], "reports": []}, {"table_id": 116, "row_id": 61433, "rank": 16, "method": "MaxViT-XL (384res, JFT)", "mlmodel": {}, "method_short": "MaxViT-XL ", "method_details": "384res, JFT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "89.41%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.41, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=61433"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}], "reports": []}, {"table_id": 116, "row_id": 71568, "rank": 17, "method": "MaxViT-L (512res, JFT)", "mlmodel": {}, "method_short": "MaxViT-L ", "method_details": "512res, JFT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "89.41%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.41, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71568"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 77882, "rank": 18, "method": "InternImage-DCNv3-H", "mlmodel": {}, "method_short": "InternImage-DCNv3-H", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-10", "metrics": {"Top 1 Accuracy": "89.2", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1109218, "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions", "url": "/paper/internimage-exploring-large-scale-vision", "published": "2022-11-10T00:00:00.000000", "code": true, "review_url": "/paper/internimage-exploring-large-scale-vision/review/?hl=77882"}, "external_source_url": null, "tags": [{"id": 14, "name": "DCN", "color": "#2771D3"}, {"id": 337, "name": "Deformable Convolution", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25384, "rank": 19, "method": "NFNet-F4+", "mlmodel": {}, "method_short": "NFNet-F4+", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "89.2%", "Top 5 Accuracy": null, "Number of params": "527M", "GFLOPs": "367", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.2, "Top 5 Accuracy": null, "Number of params": 527000000.0, "GFLOPs": 367.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25384"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 104, "name": "JFT-300M", "color": "#c4bc00"}], "reports": []}, {"table_id": 116, "row_id": 71564, "rank": 20, "method": "MaxViT-L (384res, JFT)", "mlmodel": {}, "method_short": "MaxViT-L ", "method_details": "384res, JFT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "89.12%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.12, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71564"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71508, "rank": 21, "method": "MOAT-4 22K+1K", "mlmodel": {}, "method_short": "MOAT-4 22K+1K", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-10-04", "metrics": {"Top 1 Accuracy": "89.1%", "Top 5 Accuracy": null, "Number of params": "483.2M", "GFLOPs": "648.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.1, "Top 5 Accuracy": null, "Number of params": 483200000.0, "GFLOPs": 648.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1087041, "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models", "url": "/paper/moat-alternating-mobile-convolution-and", "published": "2022-10-04T00:00:00.000000", "code": true, "review_url": "/paper/moat-alternating-mobile-convolution-and/review/?hl=71508"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 55918, "rank": 22, "method": "FD (CLIP ViT-L-336)", "mlmodel": {}, "method_short": "FD ", "method_details": "CLIP ViT-L-336", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-27", "metrics": {"Top 1 Accuracy": "89.0%", "Top 5 Accuracy": null, "Number of params": "307M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 89.0, "Top 5 Accuracy": null, "Number of params": 307000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1017290, "title": "Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation", "url": "/paper/contrastive-learning-rivals-masked-image", "published": "2022-05-27T00:00:00.000000", "code": false, "review_url": "/paper/contrastive-learning-rivals-masked-image/review/?hl=55918"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 300, "name": "CLIP data ", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 40306, "rank": 23, "method": "TokenLearner L/8 (24+11)", "mlmodel": {}, "method_short": "TokenLearner L/8 ", "method_details": "24+11", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-21", "metrics": {"Top 1 Accuracy": "88.87%", "Top 5 Accuracy": null, "Number of params": "460M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.87, "Top 5 Accuracy": null, "Number of params": 460000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 821783, "title": "TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?", "url": "/paper/tokenlearner-what-can-8-learned-tokens-do-for", "published": "2021-06-21T00:00:00.000000", "code": true, "review_url": "/paper/tokenlearner-what-can-8-learned-tokens-do-for/review/?hl=40306"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 104, "name": "JFT-300M", "color": "#c4bc00"}], "reports": []}, {"table_id": 116, "row_id": 61436, "rank": 24, "method": "MaxViT-B (512res, JFT)", "mlmodel": {}, "method_short": "MaxViT-B ", "method_details": "512res, JFT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.82%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.82, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=61436"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}], "reports": []}, {"table_id": 116, "row_id": 53598, "rank": 25, "method": "MViTv2-H (512 res, ImageNet-21k pretrain)", "mlmodel": {}, "method_short": "MViTv2-H ", "method_details": "512 res, ImageNet-21k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "88.8%", "Top 5 Accuracy": null, "Number of params": "667M", "GFLOPs": "763.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.8, "Top 5 Accuracy": null, "Number of params": 667000000.0, "GFLOPs": 763.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924692, "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection", "url": "/paper/improved-multiscale-vision-transformers-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/improved-multiscale-vision-transformers-for/review/?hl=53598"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 71569, "rank": 26, "method": "MaxViT-XL (512res, 21K)", "mlmodel": {}, "method_short": "MaxViT-XL ", "method_details": "512res, 21K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71569"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71562, "rank": 27, "method": "MaxViT-B (384res, JFT)", "mlmodel": {}, "method_short": "MaxViT-B ", "method_details": "384res, JFT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.69%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.69, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71562"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25440, "rank": 28, "method": "ALIGN (EfficientNet-L2)", "mlmodel": {}, "method_short": "ALIGN ", "method_details": "EfficientNet-L2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "88.64%", "Top 5 Accuracy": "98.67%", "Number of params": "480M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.64, "Top 5 Accuracy": 98.67, "Number of params": 480000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744362, "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "url": "/paper/scaling-up-visual-and-vision-language", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/scaling-up-visual-and-vision-language/review/?hl=25440"}, "external_source_url": null, "tags": [{"id": 247, "name": "ALIGN", "color": "#2771D3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 23974, "rank": 29, "method": "EfficientNet-L2-475 (SAM)", "mlmodel": {}, "method_short": "EfficientNet-L2-475 ", "method_details": "SAM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-03", "metrics": {"Top 1 Accuracy": "88.61%", "Top 5 Accuracy": null, "Number of params": "480M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.61, "Top 5 Accuracy": null, "Number of params": 480000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 225198, "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "url": "/paper/sharpness-aware-minimization-for-efficiently-1", "published": "2020-10-03T00:00:00.000000", "code": true, "review_url": "/paper/sharpness-aware-minimization-for-efficiently-1/review/?hl=23974"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 36894, "rank": 30, "method": "BEiT-L (ViT; ImageNet-22K pretrain)", "mlmodel": {}, "method_short": "BEiT-L ", "method_details": "ViT; ImageNet-22K pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-15", "metrics": {"Top 1 Accuracy": "88.60%", "Top 5 Accuracy": "98.66%", "Number of params": "331M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.6, "Top 5 Accuracy": 98.66, "Number of params": 331000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 818838, "title": "BEiT: BERT Pre-Training of Image Transformers", "url": "/paper/beit-bert-pre-training-of-image-transformers", "published": "2021-06-15T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 45645, "rank": 31, "method": "SWAG (ViT H/14)", "mlmodel": {}, "method_short": "SWAG ", "method_details": "ViT H/14", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-20", "metrics": {"Top 1 Accuracy": "88.6%", "Top 5 Accuracy": null, "Number of params": "633.5M", "GFLOPs": "1018.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.6, "Top 5 Accuracy": null, "Number of params": 633500000.0, "GFLOPs": 1018.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 948291, "title": "Revisiting Weakly Supervised Pre-Training of Visual Perception Models", "url": "/paper/revisiting-weakly-supervised-pre-training-of", "published": "2022-01-20T00:00:00.000000", "code": true, "review_url": "/paper/revisiting-weakly-supervised-pre-training-of/review/?hl=45645"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 20349, "rank": 32, "method": "ViT-H/14", "mlmodel": {}, "method_short": "ViT-H/14", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-22", "metrics": {"Top 1 Accuracy": "88.55%", "Top 5 Accuracy": null, "Number of params": "632M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.55, "Top 5 Accuracy": null, "Number of params": 632000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 229828, "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "url": "/paper/an-image-is-worth-16x16-words-transformers-1", "published": "2020-10-22T00:00:00.000000", "code": true, "review_url": "/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=20349"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 63447, "rank": 33, "method": "CoAtNet-3 @384", "mlmodel": {}, "method_short": "CoAtNet-3 @384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-09", "metrics": {"Top 1 Accuracy": "88.52%", "Top 5 Accuracy": null, "Number of params": "168M", "GFLOPs": "114", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.52, "Top 5 Accuracy": null, "Number of params": 168000000.0, "GFLOPs": 114.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 814553, "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes", "url": "/paper/coatnet-marrying-convolution-and-attention", "published": "2021-06-09T00:00:00.000000", "code": true, "review_url": "/paper/coatnet-marrying-convolution-and-attention/review/?hl=63447"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71565, "rank": 34, "method": "MaxViT-XL (384res, 21K)", "mlmodel": {}, "method_short": "MaxViT-XL ", "method_details": "384res, 21K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.51%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.51, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71565"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 10363, "rank": 35, "method": "FixEfficientNet-L2", "mlmodel": {}, "method_short": "FixEfficientNet-L2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "88.5%", "Top 5 Accuracy": "98.7%", "Number of params": "480M", "GFLOPs": "585", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.5, "Top 5 Accuracy": 98.7, "Number of params": 480000000.0, "GFLOPs": 585.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10363"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 48208, "rank": 36, "method": "ViTAE-H + MAE (448)", "mlmodel": {}, "method_short": "ViTAE-H + MAE ", "method_details": "448", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-21", "metrics": {"Top 1 Accuracy": "88.5%", "Top 5 Accuracy": null, "Number of params": "644M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.5, "Top 5 Accuracy": null, "Number of params": 644000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964748, "title": "ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond", "url": "/paper/vitaev2-vision-transformer-advanced-by", "published": "2022-02-21T00:00:00.000000", "code": true, "review_url": "/paper/vitaev2-vision-transformer-advanced-by/review/?hl=48208"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71567, "rank": 37, "method": "MaxViT-L (512res, 21K)", "mlmodel": {}, "method_short": "MaxViT-L ", "method_details": "512res, 21K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.46%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.46, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71567"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 9589, "rank": 38, "method": "NoisyStudent (EfficientNet-L2)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-L2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-01-07", "metrics": {"Top 1 Accuracy": "88.4%", "Top 5 Accuracy": "98.7%", "Number of params": "480M", "GFLOPs": null, "Hardware Burden": "51800G", "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.4, "Top 5 Accuracy": 98.7, "Number of params": 480000000.0, "GFLOPs": null, "Hardware Burden": 51800.0, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=9589"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_efficientnet_l2.ipynb"}]}, {"table_id": 116, "row_id": 53596, "rank": 39, "method": "MViTv2-L (384 res, ImageNet-21k pretrain)", "mlmodel": {}, "method_short": "MViTv2-L ", "method_details": "384 res, ImageNet-21k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "88.4%", "Top 5 Accuracy": null, "Number of params": "218M", "GFLOPs": "140.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.4, "Top 5 Accuracy": null, "Number of params": 218000000.0, "GFLOPs": 140.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924692, "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection", "url": "/paper/improved-multiscale-vision-transformers-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/improved-multiscale-vision-transformers-for/review/?hl=53596"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 71566, "rank": 40, "method": "MaxViT-B (512res, 21K)", "mlmodel": {}, "method_short": "MaxViT-B ", "method_details": "512res, 21K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.38%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.38, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71566"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 34957, "rank": 41, "method": "V-MoE-H/14 (Every-2)", "mlmodel": {}, "method_short": "V-MoE-H/14 ", "method_details": "Every-2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-10", "metrics": {"Top 1 Accuracy": "88.36%", "Top 5 Accuracy": null, "Number of params": "7200M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.36, "Top 5 Accuracy": null, "Number of params": 7200000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 816109, "title": "Scaling Vision with Sparse Mixture of Experts", "url": "/paper/scaling-vision-with-sparse-mixture-of-experts", "published": "2021-06-10T00:00:00.000000", "code": true, "review_url": "/paper/scaling-vision-with-sparse-mixture-of-experts/review/?hl=34957"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71563, "rank": 42, "method": "MaxViT-L (384res, 21K)", "mlmodel": {}, "method_short": "MaxViT-L ", "method_details": "384res, 21K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.32%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.32, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71563"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 45189, "rank": 43, "method": "PeCo (ViT-H, 448)", "mlmodel": {}, "method_short": "PeCo ", "method_details": "ViT-H, 448", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-24", "metrics": {"Top 1 Accuracy": "88.3%", "Top 5 Accuracy": null, "Number of params": "656M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.3, "Top 5 Accuracy": null, "Number of params": 656000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 916192, "title": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers", "url": "/paper/peco-perceptual-codebook-for-bert-pre", "published": "2021-11-24T00:00:00.000000", "code": true, "review_url": "/paper/peco-perceptual-codebook-for-bert-pre/review/?hl=45189"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 71561, "rank": 44, "method": "MaxViT-B (384res, 21K)", "mlmodel": {}, "method_short": "MaxViT-B ", "method_details": "384res, 21K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "88.24%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.24, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71561"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 34952, "rank": 45, "method": "V-MoE-H/14 (Last-5)", "mlmodel": {}, "method_short": "V-MoE-H/14 ", "method_details": "Last-5", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-10", "metrics": {"Top 1 Accuracy": "88.23%", "Top 5 Accuracy": null, "Number of params": "2700M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.23, "Top 5 Accuracy": null, "Number of params": 2700000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 816109, "title": "Scaling Vision with Sparse Mixture of Experts", "url": "/paper/scaling-vision-with-sparse-mixture-of-experts", "published": "2021-06-10T00:00:00.000000", "code": true, "review_url": "/paper/scaling-vision-with-sparse-mixture-of-experts/review/?hl=34952"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 69824, "rank": 46, "method": "dBOT ViT-H (CLIP as Teacher)", "mlmodel": {}, "method_short": "dBOT ViT-H ", "method_details": "CLIP as Teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-08", "metrics": {"Top 1 Accuracy": "88.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1071401, "title": "Exploring Target Representations for Masked Autoencoders", "url": "/paper/exploring-target-representations-for-masked", "published": "2022-09-08T00:00:00.000000", "code": true, "review_url": "/paper/exploring-target-representations-for-masked/review/?hl=69824"}, "external_source_url": null, "tags": [{"id": 300, "name": "CLIP data ", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 34942, "rank": 47, "method": "VIT-H/14", "mlmodel": {}, "method_short": "VIT-H/14", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-10", "metrics": {"Top 1 Accuracy": "88.08%", "Top 5 Accuracy": null, "Number of params": "656M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.08, "Top 5 Accuracy": null, "Number of params": 656000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 816109, "title": "Scaling Vision with Sparse Mixture of Experts", "url": "/paper/scaling-vision-with-sparse-mixture-of-experts", "published": "2021-06-10T00:00:00.000000", "code": true, "review_url": "/paper/scaling-vision-with-sparse-mixture-of-experts/review/?hl=34942"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 53595, "rank": 48, "method": "MViTv2-H (mageNet-21k pretrain)", "mlmodel": {}, "method_short": "MViTv2-H ", "method_details": "mageNet-21k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "88%", "Top 5 Accuracy": null, "Number of params": "667M", "GFLOPs": "120.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 88.0, "Top 5 Accuracy": null, "Number of params": 667000000.0, "GFLOPs": 120.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924692, "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection", "url": "/paper/improved-multiscale-vision-transformers-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/improved-multiscale-vision-transformers-for/review/?hl=53595"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 31606, "rank": 49, "method": "Mixer-H/14 (JFT-300M pre-train)", "mlmodel": {}, "method_short": "Mixer-H/14 ", "method_details": "JFT-300M pre-train", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-04", "metrics": {"Top 1 Accuracy": "87.94%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.94, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 793349, "title": "MLP-Mixer: An all-MLP Architecture for Vision", "url": "/paper/mlp-mixer-an-all-mlp-architecture-for-vision", "published": "2021-05-04T00:00:00.000000", "code": true, "review_url": "/paper/mlp-mixer-an-all-mlp-architecture-for-vision/review/?hl=31606"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 69823, "rank": 50, "method": "dBOT ViT-L (CLIP as Teacher)", "mlmodel": {}, "method_short": "dBOT ViT-L ", "method_details": "CLIP as Teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-08", "metrics": {"Top 1 Accuracy": "87.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1071401, "title": "Exploring Target Representations for Masked Autoencoders", "url": "/paper/exploring-target-representations-for-masked", "published": "2022-09-08T00:00:00.000000", "code": true, "review_url": "/paper/exploring-target-representations-for-masked/review/?hl=69823"}, "external_source_url": null, "tags": [{"id": 300, "name": "CLIP data ", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 75979, "rank": 51, "method": "VAN-B6 (22K, 384res)", "mlmodel": {}, "method_short": "VAN-B6 ", "method_details": "22K, 384res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "87.8%", "Top 5 Accuracy": null, "Number of params": "200M", "GFLOPs": "114.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.8, "Top 5 Accuracy": null, "Number of params": 200000000.0, "GFLOPs": 114.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=75979"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 63755, "rank": 52, "method": "RepLKNet-XL", "mlmodel": {}, "method_short": "RepLKNet-XL", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-13", "metrics": {"Top 1 Accuracy": "87.8%", "Top 5 Accuracy": null, "Number of params": "335M", "GFLOPs": "128.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.8, "Top 5 Accuracy": null, "Number of params": 335000000.0, "GFLOPs": 128.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 976188, "title": "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs", "url": "/paper/scaling-up-your-kernels-to-31x31-revisiting", "published": "2022-03-13T00:00:00.000000", "code": true, "review_url": "/paper/scaling-up-your-kernels-to-31x31-revisiting/review/?hl=63755"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 45304, "rank": 53, "method": "ConvNeXt-XL (ImageNet-22k)", "mlmodel": {}, "method_short": "ConvNeXt-XL ", "method_details": "ImageNet-22k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-10", "metrics": {"Top 1 Accuracy": "87.8%", "Top 5 Accuracy": null, "Number of params": "350M", "GFLOPs": "179", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.8, "Top 5 Accuracy": null, "Number of params": 350000000.0, "GFLOPs": 179.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 942596, "title": "A ConvNet for the 2020s", "url": "/paper/a-convnet-for-the-2020s", "published": "2022-01-10T00:00:00.000000", "code": true, "review_url": "/paper/a-convnet-for-the-2020s/review/?hl=45304"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 44087, "rank": 54, "method": "MAE (ViT-H, 448)", "mlmodel": {}, "method_short": "MAE ", "method_details": "ViT-H, 448", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-11", "metrics": {"Top 1 Accuracy": "87.8%", "Top 5 Accuracy": null, "Number of params": "656M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.8, "Top 5 Accuracy": null, "Number of params": 656000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 908690, "title": "Masked Autoencoders Are Scalable Vision Learners", "url": "/paper/masked-autoencoders-are-scalable-vision", "published": "2021-11-11T00:00:00.000000", "code": true, "review_url": "/paper/masked-autoencoders-are-scalable-vision/review/?hl=44087"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 20350, "rank": 55, "method": "ViT-L/16", "mlmodel": {}, "method_short": "ViT-L/16", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-22", "metrics": {"Top 1 Accuracy": "87.76%", "Top 5 Accuracy": null, "Number of params": "307M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.76, "Top 5 Accuracy": null, "Number of params": 307000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 229828, "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "url": "/paper/an-image-is-worth-16x16-words-transformers-1", "published": "2020-10-22T00:00:00.000000", "code": true, "review_url": "/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=20350"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60810, "rank": 56, "method": "HorNet-L (GF)", "mlmodel": {}, "method_short": "HorNet-L ", "method_details": "GF", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-28", "metrics": {"Top 1 Accuracy": "87.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "101.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 101.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1051716, "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions", "url": "/paper/hornet-efficient-high-order-spatial", "published": "2022-07-28T00:00:00.000000", "code": true, "review_url": "/paper/hornet-efficient-high-order-spatial/review/?hl=60810"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 9478, "rank": 57, "method": "BiT-L (ResNet)", "mlmodel": {}, "method_short": "BiT-L ", "method_details": "ResNet", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-24", "metrics": {"Top 1 Accuracy": "87.54%", "Top 5 Accuracy": "98.46%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.54, "Top 5 Accuracy": 98.46, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 178162, "title": "Big Transfer (BiT): General Visual Representation Learning", "url": "/paper/large-scale-learning-of-general-visual", "published": "2019-12-24T00:00:00.000000", "code": true, "review_url": "/paper/large-scale-learning-of-general-visual/review/?hl=9478"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 45188, "rank": 58, "method": "PeCo (ViT-H, 224)", "mlmodel": {}, "method_short": "PeCo ", "method_details": "ViT-H, 224", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-24", "metrics": {"Top 1 Accuracy": "87.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 916192, "title": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers", "url": "/paper/peco-perceptual-codebook-for-bert-pre", "published": "2021-11-24T00:00:00.000000", "code": true, "review_url": "/paper/peco-perceptual-codebook-for-bert-pre/review/?hl=45188"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 36295, "rank": 59, "method": "CSWin-L (384 res,ImageNet-22k pretrain)", "mlmodel": {}, "method_short": "CSWin-L ", "method_details": "384 res,ImageNet-22k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-01", "metrics": {"Top 1 Accuracy": "87.5%", "Top 5 Accuracy": null, "Number of params": "173M", "GFLOPs": "96.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.5, "Top 5 Accuracy": null, "Number of params": 173000000.0, "GFLOPs": 96.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 828741, "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows", "url": "/paper/cswin-transformer-a-general-vision", "published": "2021-07-01T00:00:00.000000", "code": true, "review_url": "/paper/cswin-transformer-a-general-vision/review/?hl=36295"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60763, "rank": 60, "method": "DaViT-L (ImageNet-22k)", "mlmodel": {}, "method_short": "DaViT-L ", "method_details": "ImageNet-22k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "87.5%", "Top 5 Accuracy": null, "Number of params": "196.8M", "GFLOPs": "103", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.5, "Top 5 Accuracy": null, "Number of params": 196800000.0, "GFLOPs": 103.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=60763"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 34947, "rank": 61, "method": "V-MoE-L/16 (Every-2)", "mlmodel": {}, "method_short": "V-MoE-L/16 ", "method_details": "Every-2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-10", "metrics": {"Top 1 Accuracy": "87.41%", "Top 5 Accuracy": null, "Number of params": "3400M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.41, "Top 5 Accuracy": null, "Number of params": 3400000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 816109, "title": "Scaling Vision with Sparse Mixture of Experts", "url": "/paper/scaling-vision-with-sparse-mixture-of-experts", "published": "2021-06-10T00:00:00.000000", "code": true, "review_url": "/paper/scaling-vision-with-sparse-mixture-of-experts/review/?hl=34947"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60742, "rank": 62, "method": "UniNet-B6", "mlmodel": {}, "method_short": "UniNet-B6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-12", "metrics": {"Top 1 Accuracy": "87.4%", "Top 5 Accuracy": null, "Number of params": "117M", "GFLOPs": "51", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.4, "Top 5 Accuracy": null, "Number of params": 117000000.0, "GFLOPs": 51.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042502, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with-1", "published": "2022-07-12T00:00:00.000000", "code": true, "review_url": "/paper/uninet-unified-architecture-search-with-1/review/?hl=60742"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 73674, "rank": 63, "method": "DiNAT_s-Large (384res; Pretrained on IN22K@224)", "mlmodel": {}, "method_short": "DiNAT_s-Large ", "method_details": "384res; Pretrained on IN22K@224", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "87.4%", "Top 5 Accuracy": null, "Number of params": "197M", "GFLOPs": "101.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.4, "Top 5 Accuracy": null, "Number of params": 197000000.0, "GFLOPs": 101.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=73674"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 73672, "rank": 64, "method": "DiNAT-Large (11x11ks; 384res; Pretrained on IN22K@224)", "mlmodel": {}, "method_short": "DiNAT-Large ", "method_details": "11x11ks; 384res; Pretrained on IN22K@224", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "87.31%", "Top 5 Accuracy": null, "Number of params": "200M", "GFLOPs": "92.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.31, "Top 5 Accuracy": null, "Number of params": 200000000.0, "GFLOPs": 92.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=73672"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 28884, "rank": 65, "method": "Swin-L (384 res, ImageNet-22k pretrain)", "mlmodel": {}, "method_short": "Swin-L ", "method_details": "384 res, ImageNet-22k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-25", "metrics": {"Top 1 Accuracy": "87.3%", "Top 5 Accuracy": null, "Number of params": "197M", "GFLOPs": "103.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.3, "Top 5 Accuracy": null, "Number of params": 197000000.0, "GFLOPs": 103.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 757245, "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "url": "/paper/swin-transformer-hierarchical-vision", "published": "2021-03-25T00:00:00.000000", "code": true, "review_url": "/paper/swin-transformer-hierarchical-vision/review/?hl=28884"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 55835, "rank": 66, "method": "VOLO-D5+HAT", "mlmodel": {}, "method_short": "VOLO-D5+HAT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-03", "metrics": {"Top 1 Accuracy": "87.3%", "Top 5 Accuracy": null, "Number of params": "295.5M", "GFLOPs": "412", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.3, "Top 5 Accuracy": null, "Number of params": 295500000.0, "GFLOPs": 412.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988272, "title": "Improving Vision Transformers by Revisiting High-frequency Components", "url": "/paper/improving-vision-transformers-by-revisiting", "published": "2022-04-03T00:00:00.000000", "code": true, "review_url": "/paper/improving-vision-transformers-by-revisiting/review/?hl=55835"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 47814, "rank": 67, "method": "EfficientNetV2 (PolyLoss)", "mlmodel": {}, "method_short": "EfficientNetV2 ", "method_details": "PolyLoss", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-26", "metrics": {"Top 1 Accuracy": "87.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1000670, "title": "PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions", "url": "/paper/polyloss-a-polynomial-expansion-perspective-1", "published": "2022-04-26T00:00:00.000000", "code": true, "review_url": "/paper/polyloss-a-polynomial-expansion-perspective-1/review/?hl=47814"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 44846, "rank": 68, "method": "ELSA-VOLO-D5 (512*512)", "mlmodel": {}, "method_short": "ELSA-VOLO-D5 ", "method_details": "512*512", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-23", "metrics": {"Top 1 Accuracy": "87.2%", "Top 5 Accuracy": null, "Number of params": "298M", "GFLOPs": "437", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.2, "Top 5 Accuracy": null, "Number of params": 298000000.0, "GFLOPs": 437.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 932838, "title": "ELSA: Enhanced Local Self-Attention for Vision Transformer", "url": "/paper/elsa-enhanced-local-self-attention-for-vision", "published": "2021-12-23T00:00:00.000000", "code": true, "review_url": "/paper/elsa-enhanced-local-self-attention-for-vision/review/?hl=44846"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 73671, "rank": 69, "method": "DiNAT-Large (384x384; Pretrained on ImageNet-22K @ 224x224)", "mlmodel": {}, "method_short": "DiNAT-Large ", "method_details": "384x384; Pretrained on ImageNet-22K @ 224x224", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "87.18%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "89.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.18, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 89.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=73671"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 10364, "rank": 70, "method": "FixEfficientNet-B7", "mlmodel": {}, "method_short": "FixEfficientNet-B7", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "87.1%", "Top 5 Accuracy": "98.2%", "Number of params": "66M", "GFLOPs": " 82", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.1, "Top 5 Accuracy": 98.2, "Number of params": 66000000.0, "GFLOPs": 82.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10364"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 55712, "rank": 71, "method": "Bamboo (Bamboo-H)", "mlmodel": {}, "method_short": "Bamboo ", "method_details": "Bamboo-H", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-21", "metrics": {"Top 1 Accuracy": "87.1", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1014071, "title": "Deeper vs Wider: A Revisit of Transformer Configuration", "url": "/paper/deeper-vs-wider-a-revisit-of-transformer", "published": "2022-05-21T00:00:00.000000", "code": false, "review_url": "/paper/deeper-vs-wider-a-revisit-of-transformer/review/?hl=55712"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 53383, "rank": 72, "method": "FAN-L-Hybrid++", "mlmodel": {}, "method_short": "FAN-L-Hybrid++", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-26", "metrics": {"Top 1 Accuracy": "87.1%", "Top 5 Accuracy": null, "Number of params": "76.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.1, "Top 5 Accuracy": null, "Number of params": 76800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 999980, "title": "Understanding The Robustness in Vision Transformers", "url": "/paper/understanding-the-robustness-in-vision", "published": "2022-04-26T00:00:00.000000", "code": true, "review_url": "/paper/understanding-the-robustness-in-vision/review/?hl=53383"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 61470, "rank": 73, "method": "SwinV2-B", "mlmodel": {}, "method_short": "SwinV2-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-18", "metrics": {"Top 1 Accuracy": "87.1%", "Top 5 Accuracy": null, "Number of params": "88M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.1, "Top 5 Accuracy": null, "Number of params": 88000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912369, "title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "url": "/paper/swin-transformer-v2-scaling-up-capacity-and", "published": "2021-11-18T00:00:00.000000", "code": true, "review_url": "/paper/swin-transformer-v2-scaling-up-capacity-and/review/?hl=61470"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 35495, "rank": 74, "method": "VOLO-D5", "mlmodel": {}, "method_short": "VOLO-D5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-24", "metrics": {"Top 1 Accuracy": "87.1%", "Top 5 Accuracy": null, "Number of params": "296M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.1, "Top 5 Accuracy": null, "Number of params": 296000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 824294, "title": "VOLO: Vision Outlooker for Visual Recognition", "url": "/paper/volo-vision-outlooker-for-visual-recognition", "published": "2021-06-24T00:00:00.000000", "code": true, "review_url": "/paper/volo-vision-outlooker-for-visual-recognition/review/?hl=35495"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 44966, "rank": 75, "method": "PatchConvNet-L120-21k-384", "mlmodel": {}, "method_short": "PatchConvNet-L120-21k-384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "87.1%", "Top 5 Accuracy": null, "Number of params": "334.3M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.1, "Top 5 Accuracy": null, "Number of params": 334300000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}, {"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 35379, "rank": 76, "method": "16-TokenLearner B/16 (21)", "mlmodel": {}, "method_short": "16-TokenLearner B/16 ", "method_details": "21", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-21", "metrics": {"Top 1 Accuracy": "87.07%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.07, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 821783, "title": "TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?", "url": "/paper/tokenlearner-what-can-8-learned-tokens-do-for", "published": "2021-06-21T00:00:00.000000", "code": true, "review_url": "/paper/tokenlearner-what-can-8-learned-tokens-do-for/review/?hl=35379"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 104, "name": "JFT-300M", "color": "#c4bc00"}], "reports": []}, {"table_id": 116, "row_id": 68557, "rank": 77, "method": "MAE+DAT (ViT-H)", "mlmodel": {}, "method_short": "MAE+DAT ", "method_details": "ViT-H", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-16", "metrics": {"Top 1 Accuracy": "87.02%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.02, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1075790, "title": "Enhance the Visual Representation via Discrete Adversarial Training", "url": "/paper/enhance-the-visual-representation-via", "published": "2022-09-16T00:00:00.000000", "code": true, "review_url": "/paper/enhance-the-visual-representation-via/review/?hl=68557"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60741, "rank": 78, "method": "UniNet-B5", "mlmodel": {}, "method_short": "UniNet-B5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-12", "metrics": {"Top 1 Accuracy": "87%", "Top 5 Accuracy": null, "Number of params": "72.9M", "GFLOPs": "20.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.0, "Top 5 Accuracy": null, "Number of params": 72900000.0, "GFLOPs": 20.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042502, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with-1", "published": "2022-07-12T00:00:00.000000", "code": true, "review_url": "/paper/uninet-unified-architecture-search-with-1/review/?hl=60741"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 75978, "rank": 79, "method": "VAN-B5 (22K, 384res)", "mlmodel": {}, "method_short": "VAN-B5 ", "method_details": "22K, 384res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "87%", "Top 5 Accuracy": null, "Number of params": "90M", "GFLOPs": "50.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 87.0, "Top 5 Accuracy": null, "Number of params": 90000000.0, "GFLOPs": 50.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=75978"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11196, "rank": 80, "method": "NoisyStudent (EfficientNet-B7)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "86.9%", "Top 5 Accuracy": "98.1%", "Number of params": "66M", "GFLOPs": "37", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.9, "Top 5 Accuracy": 98.1, "Number of params": 66000000.0, "GFLOPs": 37.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11196"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 44086, "rank": 81, "method": "MAE (ViT-H)", "mlmodel": {}, "method_short": "MAE ", "method_details": "ViT-H", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-11", "metrics": {"Top 1 Accuracy": "86.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 908690, "title": "Masked Autoencoders Are Scalable Vision Learners", "url": "/paper/masked-autoencoders-are-scalable-vision", "published": "2021-11-11T00:00:00.000000", "code": true, "review_url": "/paper/masked-autoencoders-are-scalable-vision/review/?hl=44086"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60762, "rank": 82, "method": "DaViT-B (ImageNet-22k)", "mlmodel": {}, "method_short": "DaViT-B ", "method_details": "ImageNet-22k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "86.9%", "Top 5 Accuracy": null, "Number of params": "87.9M", "GFLOPs": "46.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.9, "Top 5 Accuracy": null, "Number of params": 87900000.0, "GFLOPs": 46.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=60762"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29230, "rank": 83, "method": "EfficientNetV2-L (21k)", "mlmodel": {}, "method_short": "EfficientNetV2-L ", "method_details": "21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-01", "metrics": {"Top 1 Accuracy": "86.8%", "Top 5 Accuracy": null, "Number of params": "121M", "GFLOPs": "53", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.8, "Top 5 Accuracy": null, "Number of params": 121000000.0, "GFLOPs": 53.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 773495, "title": "EfficientNetV2: Smaller Models and Faster Training", "url": "/paper/efficientnetv2-smaller-models-and-faster", "published": "2021-04-01T00:00:00.000000", "code": true, "review_url": "/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29230"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 35496, "rank": 84, "method": "VOLO-D4", "mlmodel": {}, "method_short": "VOLO-D4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-24", "metrics": {"Top 1 Accuracy": "86.8%", "Top 5 Accuracy": null, "Number of params": "193M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.8, "Top 5 Accuracy": null, "Number of params": 193000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 824294, "title": "VOLO: Vision Outlooker for Visual Recognition", "url": "/paper/volo-vision-outlooker-for-visual-recognition", "published": "2021-06-24T00:00:00.000000", "code": true, "review_url": "/paper/volo-vision-outlooker-for-visual-recognition/review/?hl=35496"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 33897, "rank": 85, "method": "NFNet-F5 w/ SAM w/ augmult=16", "mlmodel": {}, "method_short": "NFNet-F5 w/ SAM w/ augmult=16", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-27", "metrics": {"Top 1 Accuracy": "86.78%", "Top 5 Accuracy": null, "Number of params": "377.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.78, "Top 5 Accuracy": null, "Number of params": 377200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 806611, "title": "Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error", "url": "/paper/drawing-multiple-augmentation-samples-per", "published": "2021-05-27T00:00:00.000000", "code": false, "review_url": "/paper/drawing-multiple-augmentation-samples-per/review/?hl=33897"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 70807, "rank": 86, "method": "\u00b52Net (ViT-L/16)", "mlmodel": {}, "method_short": "\u00b52Net ", "method_details": "ViT-L/16", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-25", "metrics": {"Top 1 Accuracy": "86.74%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.74, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1015923, "title": "An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems", "url": "/paper/an-evolutionary-approach-to-dynamic", "published": "2022-05-25T00:00:00.000000", "code": true, "review_url": "/paper/an-evolutionary-approach-to-dynamic/review/?hl=70807"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 18577, "rank": 87, "method": "FixEfficientNet-B6", "mlmodel": {}, "method_short": "FixEfficientNet-B6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "86.7%", "Top 5 Accuracy": "98.0%", "Number of params": "43M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.7, "Top 5 Accuracy": 98.0, "Number of params": 43000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=18577"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 52545, "rank": 88, "method": "ViT-B @384 (DeiT III, 21k)", "mlmodel": {}, "method_short": "ViT-B @384 ", "method_details": "DeiT III, 21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "86.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52545"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 60633, "rank": 89, "method": "MaxViT-L (512res)", "mlmodel": {}, "method_short": "MaxViT-L ", "method_details": "512res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "86.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=60633"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71507, "rank": 90, "method": "MOAT-3 1K only", "mlmodel": {}, "method_short": "MOAT-3 1K only", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-10-04", "metrics": {"Top 1 Accuracy": "86.7%", "Top 5 Accuracy": null, "Number of params": "190M", "GFLOPs": "271", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.7, "Top 5 Accuracy": null, "Number of params": 190000000.0, "GFLOPs": 271.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1087041, "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models", "url": "/paper/moat-alternating-mobile-convolution-and", "published": "2022-10-04T00:00:00.000000", "code": true, "review_url": "/paper/moat-alternating-mobile-convolution-and/review/?hl=71507"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 64411, "rank": 91, "method": "CLCNet (S:ViT+D:EffNet-B7) (retrain)", "mlmodel": {}, "method_short": "CLCNet ", "method_details": "S:ViT+D:EffNet-B7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-19", "metrics": {"Top 1 Accuracy": "86.61%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "51.93", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.61, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 51.93, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1012740, "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network", "url": "/paper/clcnet-rethinking-of-ensemble-modeling-with", "published": "2022-05-19T00:00:00.000000", "code": true, "review_url": "/paper/clcnet-rethinking-of-ensemble-modeling-with/review/?hl=64411"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 52585, "rank": 92, "method": "data2vec", "mlmodel": {}, "method_short": "data2vec", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-07", "metrics": {"Top 1 Accuracy": "86.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 957898, "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language", "url": "/paper/data2vec-a-general-framework-for-self-1", "published": "2022-02-07T00:00:00.000000", "code": true, "review_url": "/paper/data2vec-a-general-framework-for-self-1/review/?hl=52585"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 75976, "rank": 93, "method": "VAN-B4 (22K, 384res)", "mlmodel": {}, "method_short": "VAN-B4 ", "method_details": "22K, 384res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "86.6%", "Top 5 Accuracy": null, "Number of params": "60M", "GFLOPs": "35.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.6, "Top 5 Accuracy": null, "Number of params": 60000000.0, "GFLOPs": 35.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=75976"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60264, "rank": 94, "method": "TinyViT-21M-512-distill (512 res, 21k)", "mlmodel": {}, "method_short": "TinyViT-21M-512-distill ", "method_details": "512 res, 21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "86.5%", "Top 5 Accuracy": "97.9%", "Number of params": "21M", "GFLOPs": "27.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.5, "Top 5 Accuracy": 97.9, "Number of params": 21000000.0, "GFLOPs": 27.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60264"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 25383, "rank": 95, "method": "NFNet-F6 w/ SAM", "mlmodel": {}, "method_short": "NFNet-F6 w/ SAM", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "86.5%", "Top 5 Accuracy": "97.9%", "Number of params": "438.4M", "GFLOPs": "377.28", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.5, "Top 5 Accuracy": 97.9, "Number of params": 438400000.0, "GFLOPs": 377.28, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25383"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 73673, "rank": 96, "method": "DiNAT_s-Large (224x224; Pretrained on ImageNet-22K @ 224x224)", "mlmodel": {}, "method_short": "DiNAT_s-Large ", "method_details": "224x224; Pretrained on ImageNet-22K @ 224x224", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "86.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "34.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 34.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=73673"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 76000, "rank": 97, "method": "MKD ViT-L", "mlmodel": {}, "method_short": "MKD ViT-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-16", "metrics": {"Top 1 Accuracy": "86.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 962993, "title": "Meta Knowledge Distillation", "url": "/paper/meta-knowledge-distillation", "published": "2022-02-16T00:00:00.000000", "code": false, "review_url": "/paper/meta-knowledge-distillation/review/?hl=76000"}, "external_source_url": null, "tags": [{"id": 326, "name": "Teacher-22k", "color": "#a96800"}, {"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 44965, "rank": 98, "method": "PatchConvNet-B60-21k-384", "mlmodel": {}, "method_short": "PatchConvNet-B60-21k-384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "86.5%", "Top 5 Accuracy": null, "Number of params": "99.4M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.5, "Top 5 Accuracy": null, "Number of params": 99400000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}, {"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 29676, "rank": 99, "method": "CaiT-M-48-448", "mlmodel": {}, "method_short": "CaiT-M-48-448", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "86.5%", "Top 5 Accuracy": null, "Number of params": "438M", "GFLOPs": "377.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.5, "Top 5 Accuracy": null, "Number of params": 438000000.0, "GFLOPs": 377.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29676"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 64413, "rank": 100, "method": "CLCNet (S:ViT+D:VOLO-D3) (retrain)", "mlmodel": {}, "method_short": "CLCNet ", "method_details": "S:ViT+D:VOLO-D3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-19", "metrics": {"Top 1 Accuracy": "86.46%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "57.46", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.46, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 57.46, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1012740, "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network", "url": "/paper/clcnet-rethinking-of-ensemble-modeling-with", "published": "2022-05-19T00:00:00.000000", "code": true, "review_url": "/paper/clcnet-rethinking-of-ensemble-modeling-with/review/?hl=64413"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 64412, "rank": 101, "method": "CLCNet (S:ConvNeXt-L+D:EffNet-B7) (retrain)", "mlmodel": {}, "method_short": "CLCNet ", "method_details": "S:ConvNeXt-L+D:EffNet-B7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-19", "metrics": {"Top 1 Accuracy": "86.42%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "45.43", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.42, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 45.43, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1012740, "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network", "url": "/paper/clcnet-rethinking-of-ensemble-modeling-with", "published": "2022-05-19T00:00:00.000000", "code": true, "review_url": "/paper/clcnet-rethinking-of-ensemble-modeling-with/review/?hl=64412"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5974, "rank": 102, "method": "FixResNeXt-101 32x48d", "mlmodel": {}, "method_short": "FixResNeXt-101 32x48d", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-14", "metrics": {"Top 1 Accuracy": "86.4%", "Top 5 Accuracy": "98.0", "Number of params": "829M", "GFLOPs": null, "Hardware Burden": "62G", "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.4, "Top 5 Accuracy": 98.0, "Number of params": 829000000.0, "GFLOPs": null, "Hardware Burden": 62.0, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 142997, "title": "Fixing the train-test resolution discrepancy", "url": "/paper/fixing-the-train-test-resolution-discrepancy", "published": "2019-06-14T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy/review/?hl=5974"}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 10366, "rank": 103, "method": "FixEfficientNet-B5", "mlmodel": {}, "method_short": "FixEfficientNet-B5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "86.4%", "Top 5 Accuracy": "97.9%", "Number of params": "30M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.4, "Top 5 Accuracy": 97.9, "Number of params": 30000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10366"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 11197, "rank": 104, "method": "NoisyStudent (EfficientNet-B6)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B6", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "86.4%", "Top 5 Accuracy": "97.9%", "Number of params": "43M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.4, "Top 5 Accuracy": 97.9, "Number of params": 43000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11197"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 71544, "rank": 105, "method": "MaxViT-L (384res)", "mlmodel": {}, "method_short": "MaxViT-L ", "method_details": "384res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "86.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71544"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 28808, "rank": 106, "method": "Swin-B (384 res, ImageNet-22k pretrain)", "mlmodel": {}, "method_short": "Swin-B ", "method_details": "384 res, ImageNet-22k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-25", "metrics": {"Top 1 Accuracy": "86.4%", "Top 5 Accuracy": null, "Number of params": "88M", "GFLOPs": "47", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.4, "Top 5 Accuracy": null, "Number of params": 88000000.0, "GFLOPs": 47.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 757245, "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "url": "/paper/swin-transformer-hierarchical-vision", "published": "2021-03-25T00:00:00.000000", "code": true, "review_url": "/paper/swin-transformer-hierarchical-vision/review/?hl=28808"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_swin_base_patch4_window12_384.ipynb"}]}, {"table_id": 116, "row_id": 30486, "rank": 107, "method": "LV-ViT-L", "mlmodel": {}, "method_short": "LV-ViT-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-22", "metrics": {"Top 1 Accuracy": "86.4%", "Top 5 Accuracy": null, "Number of params": "151M", "GFLOPs": "214.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.4, "Top 5 Accuracy": null, "Number of params": 151000000.0, "GFLOPs": 214.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 787097, "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers", "url": "/paper/token-labeling-training-a-85-5-top-1-accuracy", "published": "2021-04-22T00:00:00.000000", "code": true, "review_url": "/paper/token-labeling-training-a-85-5-top-1-accuracy/review/?hl=30486"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60630, "rank": 108, "method": "MaxViT-B (384res)", "mlmodel": {}, "method_short": "MaxViT-B ", "method_details": "384res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "86.34%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.34, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=60630"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 55711, "rank": 109, "method": "Bamboo (Bamboo-L)", "mlmodel": {}, "method_short": "Bamboo ", "method_details": "Bamboo-L", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-21", "metrics": {"Top 1 Accuracy": "86.3", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1014071, "title": "Deeper vs Wider: A Revisit of Transformer Configuration", "url": "/paper/deeper-vs-wider-a-revisit-of-transformer", "published": "2022-05-21T00:00:00.000000", "code": false, "review_url": "/paper/deeper-vs-wider-a-revisit-of-transformer/review/?hl=55711"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35025, "rank": 110, "method": "BEiT-L (ViT; ImageNet 1k pretrain)", "mlmodel": {}, "method_short": "BEiT-L ", "method_details": "ViT; ImageNet 1k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-15", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "86M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 86000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 818838, "title": "BEiT: BERT Pre-Training of Image Transformers", "url": "/paper/beit-bert-pre-training-of-image-transformers", "published": "2021-06-15T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35497, "rank": 111, "method": "VOLO-D3", "mlmodel": {}, "method_short": "VOLO-D3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-24", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "86M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 86000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 824294, "title": "VOLO: Vision Outlooker for Visual Recognition", "url": "/paper/volo-vision-outlooker-for-visual-recognition", "published": "2021-06-24T00:00:00.000000", "code": true, "review_url": "/paper/volo-vision-outlooker-for-visual-recognition/review/?hl=35497"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 75977, "rank": 112, "method": "VAN-B5 (22K)", "mlmodel": {}, "method_short": "VAN-B5 ", "method_details": "22K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "90M", "GFLOPs": "17.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 90000000.0, "GFLOPs": 17.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=75977"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60755, "rank": 113, "method": "UniFormer-L (384 res)", "mlmodel": {}, "method_short": "UniFormer-L ", "method_details": "384 res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-24", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "100M", "GFLOPs": "39.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 100000000.0, "GFLOPs": 39.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 949483, "title": "UniFormer: Unifying Convolution and Self-attention for Visual Recognition", "url": "/paper/uniformer-unifying-convolution-and-self", "published": "2022-01-24T00:00:00.000000", "code": true, "review_url": "/paper/uniformer-unifying-convolution-and-self/review/?hl=60755"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 53593, "rank": 114, "method": "MViTv2-L (384 res)", "mlmodel": {}, "method_short": "MViTv2-L ", "method_details": "384 res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "218M", "GFLOPs": "140.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 218000000.0, "GFLOPs": 140.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924692, "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection", "url": "/paper/improved-multiscale-vision-transformers-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/improved-multiscale-vision-transformers-for/review/?hl=53593"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29200, "rank": 115, "method": "CAIT-M36-448", "mlmodel": {}, "method_short": "CAIT-M36-448", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "271M", "GFLOPs": "247.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 271000000.0, "GFLOPs": 247.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29200"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 25385, "rank": 116, "method": "NFNet-F5 w/ SAM", "mlmodel": {}, "method_short": "NFNet-F5 w/ SAM", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "86.3%", "Top 5 Accuracy": null, "Number of params": "377.2M", "GFLOPs": "289.76", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.3, "Top 5 Accuracy": null, "Number of params": 377200000.0, "GFLOPs": 289.76, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25385"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60265, "rank": 117, "method": "TinyViT-21M-384-distill (384 res, 21k)", "mlmodel": {}, "method_short": "TinyViT-21M-384-distill ", "method_details": "384 res, 21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "86.2%", "Top 5 Accuracy": "97.8%", "Number of params": "21M", "GFLOPs": "13.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.2, "Top 5 Accuracy": 97.8, "Number of params": 21000000.0, "GFLOPs": 13.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60265"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60631, "rank": 118, "method": "MaxViT-S (512res)", "mlmodel": {}, "method_short": "MaxViT-S ", "method_details": "512res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "86.19%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.19, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=60631"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11198, "rank": 119, "method": "NoisyStudent (EfficientNet-B5)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B5", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "86.1%", "Top 5 Accuracy": "97.8%", "Number of params": "30M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.1, "Top 5 Accuracy": 97.8, "Number of params": 30000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11198"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 29229, "rank": 120, "method": "EfficientNetV2-M (21k)", "mlmodel": {}, "method_short": "EfficientNetV2-M ", "method_details": "21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-01", "metrics": {"Top 1 Accuracy": "86.1%", "Top 5 Accuracy": null, "Number of params": "55M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.1, "Top 5 Accuracy": null, "Number of params": 55000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 773495, "title": "EfficientNetV2: Smaller Models and Faster Training", "url": "/paper/efficientnetv2-smaller-models-and-faster", "published": "2021-04-01T00:00:00.000000", "code": true, "review_url": "/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29229"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 29201, "rank": 121, "method": "CAIT-M-36", "mlmodel": {}, "method_short": "CAIT-M-36", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "86.1%", "Top 5 Accuracy": null, "Number of params": "270.9M", "GFLOPs": "173.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.1, "Top 5 Accuracy": null, "Number of params": 270900000.0, "GFLOPs": 173.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29201"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 35035, "rank": 122, "method": "Refiner-ViT-L", "mlmodel": {}, "method_short": "Refiner-ViT-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "86.03", "Top 5 Accuracy": null, "Number of params": "81M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.03, "Top 5 Accuracy": null, "Number of params": 81000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812454, "title": "Refiner: Refining Self-attention for Vision Transformers", "url": "/paper/refiner-refining-self-attention-for-vision", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/refiner-refining-self-attention-for-vision/review/?hl=35035"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 70344, "rank": 123, "method": "GPaCo (ViT-L)", "mlmodel": {}, "method_short": "GPaCo ", "method_details": "ViT-L", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-26", "metrics": {"Top 1 Accuracy": "86.01%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.01, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1080045, "title": "Generalized Parametric Contrastive Learning", "url": "/paper/generalized-parametric-contrastive-learning", "published": "2022-09-26T00:00:00.000000", "code": true, "review_url": "/paper/generalized-parametric-contrastive-learning/review/?hl=70344"}, "external_source_url": null, "tags": [{"id": 25, "name": "Contrastive", "color": "#995d0b"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 45688, "rank": 124, "method": "Omnivore (Swin-L)", "mlmodel": {}, "method_short": "Omnivore ", "method_details": "Swin-L", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-20", "metrics": {"Top 1 Accuracy": "86.0%", "Top 5 Accuracy": "97.7%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.0, "Top 5 Accuracy": 97.7, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 948292, "title": "Omnivore: A Single Model for Many Visual Modalities", "url": "/paper/omnivore-a-single-model-for-many-visual", "published": "2022-01-20T00:00:00.000000", "code": true, "review_url": "/paper/omnivore-a-single-model-for-many-visual/review/?hl=45688"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25386, "rank": 125, "method": "NFNet-F5", "mlmodel": {}, "method_short": "NFNet-F5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "86.0%", "Top 5 Accuracy": "97.6%", "Number of params": "377.2M", "GFLOPs": "289.76", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.0, "Top 5 Accuracy": 97.6, "Number of params": 377200000.0, "GFLOPs": 289.76, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25386"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35498, "rank": 126, "method": "VOLO-D2", "mlmodel": {}, "method_short": "VOLO-D2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-24", "metrics": {"Top 1 Accuracy": "86%", "Top 5 Accuracy": null, "Number of params": "59M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.0, "Top 5 Accuracy": null, "Number of params": 59000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 824294, "title": "VOLO: Vision Outlooker for Visual Recognition", "url": "/paper/volo-vision-outlooker-for-visual-recognition", "published": "2021-06-24T00:00:00.000000", "code": true, "review_url": "/paper/volo-vision-outlooker-for-visual-recognition/review/?hl=35498"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35219, "rank": 127, "method": "XCiT-L24", "mlmodel": {}, "method_short": "XCiT-L24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-17", "metrics": {"Top 1 Accuracy": "86%", "Top 5 Accuracy": null, "Number of params": "189M", "GFLOPs": "417.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 86.0, "Top 5 Accuracy": null, "Number of params": 189000000.0, "GFLOPs": 417.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 820317, "title": "XCiT: Cross-Covariance Image Transformers", "url": "/paper/xcit-cross-covariance-image-transformers", "published": "2021-06-17T00:00:00.000000", "code": true, "review_url": "/paper/xcit-cross-covariance-image-transformers/review/?hl=35219"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10367, "rank": 128, "method": "FixEfficientNet-B4", "mlmodel": {}, "method_short": "FixEfficientNet-B4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "85.9%", "Top 5 Accuracy": "97.7%", "Number of params": "19M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.9, "Top 5 Accuracy": 97.7, "Number of params": 19000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10367"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 44085, "rank": 129, "method": "MAE (ViT-L)", "mlmodel": {}, "method_short": "MAE ", "method_details": "ViT-L", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-11", "metrics": {"Top 1 Accuracy": "85.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 908690, "title": "Masked Autoencoders Are Scalable Vision Learners", "url": "/paper/masked-autoencoders-are-scalable-vision", "published": "2021-11-11T00:00:00.000000", "code": true, "review_url": "/paper/masked-autoencoders-are-scalable-vision/review/?hl=44085"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 25397, "rank": 130, "method": "NFNet-F4", "mlmodel": {}, "method_short": "NFNet-F4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "85.9%", "Top 5 Accuracy": null, "Number of params": "316.1M", "GFLOPs": "215.24", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.9, "Top 5 Accuracy": null, "Number of params": 316100000.0, "GFLOPs": 215.24, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25397"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35218, "rank": 131, "method": "XCiT-M24", "mlmodel": {}, "method_short": "XCiT-M24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-17", "metrics": {"Top 1 Accuracy": "85.8%", "Top 5 Accuracy": null, "Number of params": "84M", "GFLOPs": "188", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.8, "Top 5 Accuracy": null, "Number of params": 84000000.0, "GFLOPs": 188.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 820317, "title": "XCiT: Cross-Covariance Image Transformers", "url": "/paper/xcit-cross-covariance-image-transformers", "published": "2021-06-17T00:00:00.000000", "code": true, "review_url": "/paper/xcit-cross-covariance-image-transformers/review/?hl=35218"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 15817, "rank": 132, "method": "Fix-EfficientNet-B8 (MaxUp + CutMix)", "mlmodel": {}, "method_short": "Fix-EfficientNet-B8 ", "method_details": "MaxUp + CutMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-02-20", "metrics": {"Top 1 Accuracy": "85.8%", "Top 5 Accuracy": null, "Number of params": "87.42M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.8, "Top 5 Accuracy": null, "Number of params": 87420000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 184301, "title": "MaxUp: A Simple Way to Improve Generalization of Neural Network Training", "url": "/paper/maxup-a-simple-way-to-improve-generalization", "published": "2020-02-20T00:00:00.000000", "code": true, "review_url": "/paper/maxup-a-simple-way-to-improve-generalization/review/?hl=15817"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 10515, "rank": 133, "method": "KDforAA (EfficientNet-B8)", "mlmodel": {}, "method_short": "KDforAA ", "method_details": "EfficientNet-B8", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-25", "metrics": {"Top 1 Accuracy": "85.8%", "Top 5 Accuracy": null, "Number of params": "88M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.8, "Top 5 Accuracy": null, "Number of params": 88000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188259, "title": "Circumventing Outliers of AutoAugment with Knowledge Distillation", "url": "/paper/circumventing-outliers-of-autoaugment-with", "published": "2020-03-25T00:00:00.000000", "code": false, "review_url": "/paper/circumventing-outliers-of-autoaugment-with/review/?hl=10515"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 29202, "rank": 134, "method": "CAIT-M-24", "mlmodel": {}, "method_short": "CAIT-M-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "85.8%", "Top 5 Accuracy": null, "Number of params": "185.9M", "GFLOPs": "116.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.8, "Top 5 Accuracy": null, "Number of params": 185900000.0, "GFLOPs": 116.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29202"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 52541, "rank": 135, "method": "ViT-L @384 (DeiT III)", "mlmodel": {}, "method_short": "ViT-L @384 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "85.8%", "Top 5 Accuracy": null, "Number of params": "304.8M", "GFLOPs": "191.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.8, "Top 5 Accuracy": null, "Number of params": 304800000.0, "GFLOPs": 191.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52541"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 47951, "rank": 136, "method": "SEER (RG-10B)", "mlmodel": {}, "method_short": "SEER ", "method_details": "RG-10B", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-16", "metrics": {"Top 1 Accuracy": "85.8%", "Top 5 Accuracy": null, "Number of params": "10000M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.8, "Top 5 Accuracy": null, "Number of params": 10000000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 963673, "title": "Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision", "url": "/paper/vision-models-are-more-robust-and-fair-when", "published": "2022-02-16T00:00:00.000000", "code": true, "review_url": "/paper/vision-models-are-more-robust-and-fair-when/review/?hl=47951"}, "external_source_url": null, "tags": [{"id": 67, "name": "Self-Supervised Learning", "color": "#d72727"}, {"id": 108, "name": "RegNet", "color": "#2771D3"}, {"id": 107, "name": "IG-1B", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 71545, "rank": 137, "method": "MaxViT-T(512res)", "mlmodel": {}, "method_short": "MaxViT-T", "method_details": "512res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "85.72%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.72, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71545"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10368, "rank": 138, "method": "FixEfficientNet-B8", "mlmodel": {}, "method_short": "FixEfficientNet-B8", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "85.7%", "Top 5 Accuracy": "97.6%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.7, "Top 5 Accuracy": 97.6, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10368"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25396, "rank": 139, "method": "NFNet-F3", "mlmodel": {}, "method_short": "NFNet-F3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "85.7%", "Top 5 Accuracy": "97.5%", "Number of params": "254.9M", "GFLOPs": "114.76", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.7, "Top 5 Accuracy": 97.5, "Number of params": 254900000.0, "GFLOPs": 114.76, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25396"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29227, "rank": 140, "method": "EfficientNetV2-L", "mlmodel": {}, "method_short": "EfficientNetV2-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-01", "metrics": {"Top 1 Accuracy": "85.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 773495, "title": "EfficientNetV2: Smaller Models and Faster Training", "url": "/paper/efficientnetv2-smaller-models-and-faster", "published": "2021-04-01T00:00:00.000000", "code": true, "review_url": "/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29227"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 52544, "rank": 141, "method": "ViT-B @224 (DeiT III, 21k)", "mlmodel": {}, "method_short": "ViT-B @224 ", "method_details": "DeiT III, 21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "85.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52544"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 69822, "rank": 142, "method": "dBOT ViT-B (CLIP as Teacher)", "mlmodel": {}, "method_short": "dBOT ViT-B ", "method_details": "CLIP as Teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-08", "metrics": {"Top 1 Accuracy": "85.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1071401, "title": "Exploring Target Representations for Masked Autoencoders", "url": "/paper/exploring-target-representations-for-masked", "published": "2022-09-08T00:00:00.000000", "code": true, "review_url": "/paper/exploring-target-representations-for-masked/review/?hl=69822"}, "external_source_url": null, "tags": [{"id": 300, "name": "CLIP data ", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 76001, "rank": 143, "method": "VAN-B4 (22K)", "mlmodel": {}, "method_short": "VAN-B4 ", "method_details": "22K", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "85.7%", "Top 5 Accuracy": null, "Number of params": "60M", "GFLOPs": "12.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.7, "Top 5 Accuracy": null, "Number of params": 60000000.0, "GFLOPs": 12.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=76001"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 35217, "rank": 144, "method": "XCiT-S24", "mlmodel": {}, "method_short": "XCiT-S24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-17", "metrics": {"Top 1 Accuracy": "85.6%", "Top 5 Accuracy": null, "Number of params": "48M", "GFLOPs": "106", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.6, "Top 5 Accuracy": null, "Number of params": 48000000.0, "GFLOPs": 106.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 820317, "title": "XCiT: Cross-Covariance Image Transformers", "url": "/paper/xcit-cross-covariance-image-transformers", "published": "2021-06-17T00:00:00.000000", "code": true, "review_url": "/paper/xcit-cross-covariance-image-transformers/review/?hl=35217"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 46097, "rank": 145, "method": "UniFormer-L", "mlmodel": {}, "method_short": "UniFormer-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-24", "metrics": {"Top 1 Accuracy": "85.6%", "Top 5 Accuracy": null, "Number of params": "100M", "GFLOPs": "12.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.6, "Top 5 Accuracy": null, "Number of params": 100000000.0, "GFLOPs": 12.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 949483, "title": "UniFormer: Unifying Convolution and Self-attention for Visual Recognition", "url": "/paper/uniformer-unifying-convolution-and-self", "published": "2022-01-24T00:00:00.000000", "code": true, "review_url": "/paper/uniformer-unifying-convolution-and-self/review/?hl=46097"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 53640, "rank": 146, "method": "Mini-Swin-B@384", "mlmodel": {}, "method_short": "Mini-Swin-B@384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": "97.5", "Number of params": "47M", "GFLOPs": "98.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": 97.5, "Number of params": 47000000.0, "GFLOPs": 98.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994462, "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing", "url": "/paper/minivit-compressing-vision-transformers-with", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/minivit-compressing-vision-transformers-with/review/?hl=53640"}, "external_source_url": null, "tags": [{"id": 98, "name": "Conv+Transformer", "color": "#ff2600"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 63779, "rank": 147, "method": "Wave-ViT-L", "mlmodel": {}, "method_short": "Wave-ViT-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-11", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": "97.3%", "Number of params": "57.5M", "GFLOPs": "14.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": 97.3, "Number of params": 57500000.0, "GFLOPs": 14.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042136, "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning", "url": "/paper/wave-vit-unifying-wavelet-and-transformers", "published": "2022-07-11T00:00:00.000000", "code": true, "review_url": "/paper/wave-vit-unifying-wavelet-and-transformers/review/?hl=63779"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 8898, "rank": 148, "method": "AdvProp (EfficientNet-B8)", "mlmodel": {}, "method_short": "AdvProp ", "method_details": "EfficientNet-B8", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-21", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": "97.3", "Number of params": "88M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": 97.3, "Number of params": 88000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 173727, "title": "Adversarial Examples Improve Image Recognition", "url": "/paper/adversarial-examples-improve-image", "published": "2019-11-21T00:00:00.000000", "code": true, "review_url": "/paper/adversarial-examples-improve-image/review/?hl=8898"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 49540, "rank": 149, "method": "ViT-L@384 (attn finetune)", "mlmodel": {}, "method_short": "ViT-L@384 ", "method_details": "attn finetune", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49540"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10516, "rank": 150, "method": "KDforAA (EfficientNet-B7)", "mlmodel": {}, "method_short": "KDforAA ", "method_details": "EfficientNet-B7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-25", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": null, "Number of params": "66M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": null, "Number of params": 66000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188259, "title": "Circumventing Outliers of AutoAugment with Knowledge Distillation", "url": "/paper/circumventing-outliers-of-autoaugment-with", "published": "2020-03-25T00:00:00.000000", "code": false, "review_url": "/paper/circumventing-outliers-of-autoaugment-with/review/?hl=10516"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 28724, "rank": 151, "method": "HaloNet4 (base 128, Conv-12)", "mlmodel": {}, "method_short": "HaloNet4 ", "method_details": "base 128, Conv-12", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-23", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": null, "Number of params": "87M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": null, "Number of params": 87000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 756297, "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones", "url": "/paper/scaling-local-self-attention-for-parameter", "published": "2021-03-23T00:00:00.000000", "code": true, "review_url": "/paper/scaling-local-self-attention-for-parameter/review/?hl=28724"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 61425, "rank": 152, "method": "ConvNeXt-L (384 res)", "mlmodel": {}, "method_short": "ConvNeXt-L ", "method_details": "384 res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-10", "metrics": {"Top 1 Accuracy": "85.5%", "Top 5 Accuracy": null, "Number of params": "198M", "GFLOPs": "101", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.5, "Top 5 Accuracy": null, "Number of params": 198000000.0, "GFLOPs": 101.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 942596, "title": "A ConvNet for the 2020s", "url": "/paper/a-convnet-for-the-2020s", "published": "2022-01-10T00:00:00.000000", "code": true, "review_url": "/paper/a-convnet-for-the-2020s/review/?hl=61425"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 5615, "rank": 153, "method": "ResNeXt-101 32x48d", "mlmodel": {}, "method_short": "ResNeXt-101 32x48d", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-05-02", "metrics": {"Top 1 Accuracy": "85.4%", "Top 5 Accuracy": "97.6%", "Number of params": "829M", "GFLOPs": "306", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.4, "Top 5 Accuracy": 97.6, "Number of params": 829000000.0, "GFLOPs": 306.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 4635, "title": "Exploring the Limits of Weakly Supervised Pretraining", "url": "/paper/exploring-the-limits-of-weakly-supervised", "published": "2018-05-02T00:00:00.000000", "code": true, "review_url": "/paper/exploring-the-limits-of-weakly-supervised/review/?hl=5615"}, "external_source_url": null, "tags": [{"id": 107, "name": "IG-1B", "color": "#2771D3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 16783, "rank": 154, "method": "EfficientNet-B8 (RandAugment)", "mlmodel": {}, "method_short": "EfficientNet-B8 ", "method_details": "RandAugment", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-09-30", "metrics": {"Top 1 Accuracy": "85.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 156406, "title": "RandAugment: Practical automated data augmentation with a reduced search space", "url": "/paper/randaugment-practical-data-augmentation-with", "published": "2019-09-30T00:00:00.000000", "code": true, "review_url": "/paper/randaugment-practical-data-augmentation-with/review/?hl=16783"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 44964, "rank": 155, "method": "PatchConvNet-S60-21k-512", "mlmodel": {}, "method_short": "PatchConvNet-S60-21k-512", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "85.4%", "Top 5 Accuracy": null, "Number of params": "25.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.4, "Top 5 Accuracy": null, "Number of params": 25200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}], "reports": []}, {"table_id": 116, "row_id": 29204, "rank": 156, "method": "CAIT-S-36", "mlmodel": {}, "method_short": "CAIT-S-36", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "85.4%", "Top 5 Accuracy": null, "Number of params": "68.2M", "GFLOPs": "48", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.4, "Top 5 Accuracy": null, "Number of params": 68200000.0, "GFLOPs": 48.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29204"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 9482, "rank": 157, "method": "BiT-M (ResNet)", "mlmodel": {}, "method_short": "BiT-M ", "method_details": "ResNet", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-24", "metrics": {"Top 1 Accuracy": "85.39%", "Top 5 Accuracy": "97.69%", "Number of params": "928M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.39, "Top 5 Accuracy": 97.69, "Number of params": 928000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 178162, "title": "Big Transfer (BiT): General Visual Representation Learning", "url": "/paper/large-scale-learning-of-general-visual", "published": "2019-12-24T00:00:00.000000", "code": true, "review_url": "/paper/large-scale-learning-of-general-visual/review/?hl=9482"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 45687, "rank": 158, "method": "Omnivore (Swin-B)", "mlmodel": {}, "method_short": "Omnivore ", "method_details": "Swin-B", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-20", "metrics": {"Top 1 Accuracy": "85.3%", "Top 5 Accuracy": "97.5%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.3, "Top 5 Accuracy": 97.5, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 948292, "title": "Omnivore: A Single Model for Many Visual Modalities", "url": "/paper/omnivore-a-single-model-for-many-visual", "published": "2022-01-20T00:00:00.000000", "code": true, "review_url": "/paper/omnivore-a-single-model-for-many-visual/review/?hl=45687"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11199, "rank": 159, "method": "NoisyStudent (EfficientNet-B4)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B4", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "85.3%", "Top 5 Accuracy": "97.5%", "Number of params": "19M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.3, "Top 5 Accuracy": 97.5, "Number of params": 19000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11199"}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 32048, "rank": 160, "method": "ViT-L/16 Dosovitskiy et al. (2021)", "mlmodel": {}, "method_short": "ViT-L/16 Dosovitskiy et al. ", "method_details": "2021", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-04", "metrics": {"Top 1 Accuracy": "85.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 793349, "title": "MLP-Mixer: An all-MLP Architecture for Vision", "url": "/paper/mlp-mixer-an-all-mlp-architecture-for-vision", "published": "2021-05-04T00:00:00.000000", "code": true, "review_url": "/paper/mlp-mixer-an-all-mlp-architecture-for-vision/review/?hl=32048"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29203, "rank": 161, "method": "CAIT-S-48", "mlmodel": {}, "method_short": "CAIT-S-48", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "85.3%", "Top 5 Accuracy": null, "Number of params": "89.5M", "GFLOPs": "63.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.3, "Top 5 Accuracy": null, "Number of params": 89500000.0, "GFLOPs": 63.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29203"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 55045, "rank": 162, "method": "CLCNet (S:D1+D:D5)", "mlmodel": {}, "method_short": "CLCNet ", "method_details": "S:D1+D:D5", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-19", "metrics": {"Top 1 Accuracy": "85.28%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "47.43", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.28, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 47.43, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1012740, "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network", "url": "/paper/clcnet-rethinking-of-ensemble-modeling-with", "published": "2022-05-19T00:00:00.000000", "code": true, "review_url": "/paper/clcnet-rethinking-of-ensemble-modeling-with/review/?hl=55045"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 71542, "rank": 163, "method": "MaxViT-T (384res)", "mlmodel": {}, "method_short": "MaxViT-T ", "method_details": "384res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "85.24%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.24, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71542"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 75970, "rank": 164, "method": "ViT-B/16", "mlmodel": {}, "method_short": "ViT-B/16", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-22", "metrics": {"Top 1 Accuracy": "85.22", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "33.03", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.22, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 33.03, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 229828, "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "url": "/paper/an-image-is-worth-16x16-words-transformers-1", "published": "2020-10-22T00:00:00.000000", "code": true, "review_url": "/paper/an-image-is-worth-16x16-words-transformers-1/review/?hl=75970"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 8899, "rank": 165, "method": "AdvProp (EfficientNet-B7)", "mlmodel": {}, "method_short": "AdvProp ", "method_details": "EfficientNet-B7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-21", "metrics": {"Top 1 Accuracy": "85.2%", "Top 5 Accuracy": "97.2", "Number of params": "66M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.2, "Top 5 Accuracy": 97.2, "Number of params": 66000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 173727, "title": "Adversarial Examples Improve Image Recognition", "url": "/paper/adversarial-examples-improve-image", "published": "2019-11-21T00:00:00.000000", "code": true, "review_url": "/paper/adversarial-examples-improve-image/review/?hl=8899"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 52542, "rank": 166, "method": "ViT-H @224 (DeiT III)", "mlmodel": {}, "method_short": "ViT-H @224 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "85.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52542"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35499, "rank": 167, "method": "VOLO-D1", "mlmodel": {}, "method_short": "VOLO-D1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-24", "metrics": {"Top 1 Accuracy": "85.2%", "Top 5 Accuracy": null, "Number of params": "27M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.2, "Top 5 Accuracy": null, "Number of params": 27000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 824294, "title": "VOLO: Vision Outlooker for Visual Recognition", "url": "/paper/volo-vision-outlooker-for-visual-recognition", "published": "2021-06-24T00:00:00.000000", "code": true, "review_url": "/paper/volo-vision-outlooker-for-visual-recognition/review/?hl=35499"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 48309, "rank": 168, "method": "UniNet-B5", "mlmodel": {}, "method_short": "UniNet-B5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-08", "metrics": {"Top 1 Accuracy": "85.2%", "Top 5 Accuracy": null, "Number of params": "73.5M", "GFLOPs": "23.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.2, "Top 5 Accuracy": null, "Number of params": 73500000.0, "GFLOPs": 23.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 881795, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with", "published": "2021-10-08T00:00:00.000000", "code": false, "review_url": "/paper/uninet-unified-architecture-search-with/review/?hl=48309"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 23829, "rank": 169, "method": "DeiT-B 384", "mlmodel": {}, "method_short": "DeiT-B 384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-12-23", "metrics": {"Top 1 Accuracy": "85.2%", "Top 5 Accuracy": null, "Number of params": "87M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.2, "Top 5 Accuracy": null, "Number of params": 87000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 731001, "title": "Training data-efficient image transformers \u0026 distillation through attention", "url": "/paper/training-data-efficient-image-transformers", "published": "2020-12-23T00:00:00.000000", "code": true, "review_url": "/paper/training-data-efficient-image-transformers/review/?hl=23829"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 6127, "rank": 170, "method": "ResNeXt-101 32x32d", "mlmodel": {}, "method_short": "ResNeXt-101 32x32d", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-05-02", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": "97.5%", "Number of params": "466M", "GFLOPs": "174", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": 97.5, "Number of params": 466000000.0, "GFLOPs": 174.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 4635, "title": "Exploring the Limits of Weakly Supervised Pretraining", "url": "/paper/exploring-the-limits-of-weakly-supervised", "published": "2018-05-02T00:00:00.000000", "code": true, "review_url": "/paper/exploring-the-limits-of-weakly-supervised/review/?hl=6127"}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 18191, "rank": 171, "method": "ResNet200_vd_26w_4s_ssld", "mlmodel": {}, "method_short": "ResNet200_vd_26w_4s_ssld", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-18", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": "97.4%", "Number of params": "76M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": 97.4, "Number of params": 76000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 203194, "title": "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset", "url": "/paper/semi-supervised-recognition-under-a-noisy-and", "published": "2020-06-18T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25395, "rank": 172, "method": "NFNet-F2", "mlmodel": {}, "method_short": "NFNet-F2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": "97.3%", "Number of params": "193.8M", "GFLOPs": "62.59", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": 97.3, "Number of params": 193800000.0, "GFLOPs": 62.59, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25395"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29226, "rank": 173, "method": "EfficientNetV2-M", "mlmodel": {}, "method_short": "EfficientNetV2-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-01", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 773495, "title": "EfficientNetV2: Smaller Models and Faster Training", "url": "/paper/efficientnetv2-smaller-models-and-faster", "published": "2021-04-01T00:00:00.000000", "code": true, "review_url": "/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29226"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 75999, "rank": 174, "method": "MKD ViT-B", "mlmodel": {}, "method_short": "MKD ViT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-16", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 962993, "title": "Meta Knowledge Distillation", "url": "/paper/meta-knowledge-distillation", "published": "2022-02-16T00:00:00.000000", "code": false, "review_url": "/paper/meta-knowledge-distillation/review/?hl=75999"}, "external_source_url": null, "tags": [{"id": 326, "name": "Teacher-22k", "color": "#a96800"}, {"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}], "reports": []}, {"table_id": 116, "row_id": 35216, "rank": 175, "method": "XCiT-S12", "mlmodel": {}, "method_short": "XCiT-S12", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-17", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": null, "Number of params": "26M", "GFLOPs": "55.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": null, "Number of params": 26000000.0, "GFLOPs": 55.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 820317, "title": "XCiT: Cross-Covariance Image Transformers", "url": "/paper/xcit-cross-covariance-image-transformers", "published": "2021-06-17T00:00:00.000000", "code": true, "review_url": "/paper/xcit-cross-covariance-image-transformers/review/?hl=35216"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29205, "rank": 176, "method": "CAIT-S-24", "mlmodel": {}, "method_short": "CAIT-S-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": null, "Number of params": "46.9M", "GFLOPs": "32.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": null, "Number of params": 46900000.0, "GFLOPs": 32.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29205"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 57828, "rank": 177, "method": "MixMIM-B", "mlmodel": {}, "method_short": "MixMIM-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "85.1%", "Top 5 Accuracy": null, "Number of params": "88M", "GFLOPs": "16.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.1, "Top 5 Accuracy": null, "Number of params": 88000000.0, "GFLOPs": 16.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016710, "title": "MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning", "url": "/paper/mixmim-mixed-and-masked-image-modeling-for", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/mixmim-mixed-and-masked-image-modeling-for/review/?hl=57828"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 44733, "rank": 178, "method": "DiscreteViT", "mlmodel": {}, "method_short": "DiscreteViT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-20", "metrics": {"Top 1 Accuracy": "85.07%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.07, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 914582, "title": "Discrete Representations Strengthen Vision Transformer Robustness", "url": "/paper/discrete-representations-strengthen-vision-1", "published": "2021-11-20T00:00:00.000000", "code": true, "review_url": "/paper/discrete-representations-strengthen-vision-1/review/?hl=44733"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10369, "rank": 179, "method": "FixEfficientNet-B3", "mlmodel": {}, "method_short": "FixEfficientNet-B3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "85%", "Top 5 Accuracy": "97.4%", "Number of params": "12M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.0, "Top 5 Accuracy": 97.4, "Number of params": 12000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10369"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 29228, "rank": 180, "method": "EfficientNetV2-S (21k)", "mlmodel": {}, "method_short": "EfficientNetV2-S ", "method_details": "21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-01", "metrics": {"Top 1 Accuracy": "85.0%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 773495, "title": "EfficientNetV2: Smaller Models and Faster Training", "url": "/paper/efficientnetv2-smaller-models-and-faster", "published": "2021-04-01T00:00:00.000000", "code": true, "review_url": "/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29228"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 16786, "rank": 181, "method": "EfficientNet-B7 (RandAugment)", "mlmodel": {}, "method_short": "EfficientNet-B7 ", "method_details": "RandAugment", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-09-30", "metrics": {"Top 1 Accuracy": "85%", "Top 5 Accuracy": null, "Number of params": "66M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.0, "Top 5 Accuracy": null, "Number of params": 66000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 156406, "title": "RandAugment: Practical automated data augmentation with a reduced search space", "url": "/paper/randaugment-practical-data-augmentation-with", "published": "2019-09-30T00:00:00.000000", "code": true, "review_url": "/paper/randaugment-practical-data-augmentation-with/review/?hl=16786"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 52539, "rank": 182, "method": "ViT-B @384 (DeiT III)", "mlmodel": {}, "method_short": "ViT-B @384 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "85.0%", "Top 5 Accuracy": null, "Number of params": "87M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 85.0, "Top 5 Accuracy": null, "Number of params": 87000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52539"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 71540, "rank": 183, "method": "MaxViT-B (224res)", "mlmodel": {}, "method_short": "MaxViT-B ", "method_details": "224res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "84.95%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.95, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71540"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 52540, "rank": 184, "method": "ViT-L @224 (DeiT III)", "mlmodel": {}, "method_short": "ViT-L @224 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "84.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52540"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29253, "rank": 185, "method": "CvT-21 (384 res, ImageNet-22k pretrain)", "mlmodel": {}, "method_short": "CvT-21 ", "method_details": "384 res, ImageNet-22k pretrain", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "84.9%", "Top 5 Accuracy": null, "Number of params": "32M", "GFLOPs": "25", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.9, "Top 5 Accuracy": null, "Number of params": 32000000.0, "GFLOPs": 25.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758429, "title": "CvT: Introducing Convolutions to Vision Transformers", "url": "/paper/cvt-introducing-convolutions-to-vision", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/cvt-introducing-convolutions-to-vision/review/?hl=29253"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 8298, "rank": 186, "method": "ResNeXt-101 32x16d (semi-weakly sup.)", "mlmodel": {}, "method_short": "ResNeXt-101 32x16d ", "method_details": "semi-weakly sup.", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-02", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": "97.4", "Number of params": "193M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": 97.4, "Number of params": 193000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 113449, "title": "Billion-scale semi-supervised learning for image classification", "url": "/paper/billion-scale-semi-supervised-learning-for", "published": "2019-05-02T00:00:00.000000", "code": true, "review_url": "/paper/billion-scale-semi-supervised-learning-for/review/?hl=8298"}, "external_source_url": null, "tags": [{"id": 107, "name": "IG-1B", "color": "#2771D3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 60266, "rank": 187, "method": "TinyViT-21M-distill (21k)", "mlmodel": {}, "method_short": "TinyViT-21M-distill ", "method_details": "21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": "97.3%", "Number of params": "21M", "GFLOPs": "4.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": 97.3, "Number of params": 21000000.0, "GFLOPs": 4.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60266"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 63781, "rank": 188, "method": "Wave-ViT-B", "mlmodel": {}, "method_short": "Wave-ViT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-11", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": "97.1%", "Number of params": "33.5M", "GFLOPs": "7.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": 97.1, "Number of params": 33500000.0, "GFLOPs": 7.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042136, "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning", "url": "/paper/wave-vit-unifying-wavelet-and-transformers", "published": "2022-07-11T00:00:00.000000", "code": true, "review_url": "/paper/wave-vit-unifying-wavelet-and-transformers/review/?hl=63781"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29206, "rank": 189, "method": "CAIT-XS-36", "mlmodel": {}, "method_short": "CAIT-XS-36", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": null, "Number of params": "38.6M", "GFLOPs": "28.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": null, "Number of params": 38600000.0, "GFLOPs": 28.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29206"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 44122, "rank": 190, "method": "SReT-B (384 res, ImageNet-1K only)", "mlmodel": {}, "method_short": "SReT-B ", "method_details": "384 res, ImageNet-1K only", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-09", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": null, "Number of params": "71.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": null, "Number of params": 71200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 907412, "title": "Sliced Recursive Transformer", "url": "/paper/sliced-recursive-transformer-1", "published": "2021-11-09T00:00:00.000000", "code": true, "review_url": "/paper/sliced-recursive-transformer-1/review/?hl=44122"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 38423, "rank": 191, "method": "MViT-B-24", "mlmodel": {}, "method_short": "MViT-B-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-22", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": null, "Number of params": "72.9M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": null, "Number of params": 72900000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 787025, "title": "Multiscale Vision Transformers", "url": "/paper/multiscale-vision-transformers", "published": "2021-04-22T00:00:00.000000", "code": true, "review_url": "/paper/multiscale-vision-transformers/review/?hl=38423"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 45406, "rank": 192, "method": "DAT-B (384 res, IN-1K only)", "mlmodel": {}, "method_short": "DAT-B ", "method_details": "384 res, IN-1K only", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-03", "metrics": {"Top 1 Accuracy": "84.8%", "Top 5 Accuracy": null, "Number of params": "88M", "GFLOPs": "49.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.8, "Top 5 Accuracy": null, "Number of params": 88000000.0, "GFLOPs": 49.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 934161, "title": "Vision Transformer with Deformable Attention", "url": "/paper/vision-transformer-with-deformable-attention", "published": "2022-01-03T00:00:00.000000", "code": true, "review_url": "/paper/vision-transformer-with-deformable-attention/review/?hl=45406"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 25394, "rank": 193, "method": "NFNet-F1", "mlmodel": {}, "method_short": "NFNet-F1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "84.7%", "Top 5 Accuracy": "97.1%", "Number of params": "132.6M", "GFLOPs": "35.54", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.7, "Top 5 Accuracy": 97.1, "Number of params": 132600000.0, "GFLOPs": 35.54, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25394"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24943, "rank": 194, "method": "BoTNet T7", "mlmodel": {}, "method_short": "BoTNet T7", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "84.7%", "Top 5 Accuracy": "97%", "Number of params": "75.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.7, "Top 5 Accuracy": 97.0, "Number of params": 75100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24943"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 44845, "rank": 195, "method": "ELSA-VOLO-D1", "mlmodel": {}, "method_short": "ELSA-VOLO-D1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-23", "metrics": {"Top 1 Accuracy": "84.7%", "Top 5 Accuracy": null, "Number of params": "27M", "GFLOPs": "8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.7, "Top 5 Accuracy": null, "Number of params": 27000000.0, "GFLOPs": 8.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 932838, "title": "ELSA: Enhanced Local Self-Attention for Vision Transformer", "url": "/paper/elsa-enhanced-local-self-attention-for-vision", "published": "2021-12-23T00:00:00.000000", "code": true, "review_url": "/paper/elsa-enhanced-local-self-attention-for-vision/review/?hl=44845"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 63703, "rank": 196, "method": "Next-ViT-L @384", "mlmodel": {}, "method_short": "Next-ViT-L @384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-12", "metrics": {"Top 1 Accuracy": "84.7%", "Top 5 Accuracy": null, "Number of params": "57.8M", "GFLOPs": "32", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.7, "Top 5 Accuracy": null, "Number of params": 57800000.0, "GFLOPs": 32.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042514, "title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios", "url": "/paper/next-vit-next-generation-vision-transformer", "published": "2022-07-12T00:00:00.000000", "code": true, "review_url": "/paper/next-vit-next-generation-vision-transformer/review/?hl=63703"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 55678, "rank": 197, "method": "LITv2-B|384", "mlmodel": {}, "method_short": "LITv2-B|384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "84.7%", "Top 5 Accuracy": null, "Number of params": "87M", "GFLOPs": "39.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.7, "Top 5 Accuracy": null, "Number of params": 87000000.0, "GFLOPs": 39.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016651, "title": "Fast Vision Transformers with HiLo Attention", "url": "/paper/fast-vision-transformers-with-hilo-attention", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/fast-vision-transformers-with-hilo-attention/review/?hl=55678"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 63784, "rank": 198, "method": "SE-CoTNetD-152", "mlmodel": {}, "method_short": "SE-CoTNetD-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-26", "metrics": {"Top 1 Accuracy": "84.6%", "Top 5 Accuracy": "97.1%", "Number of params": "55.8M", "GFLOPs": "26.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.6, "Top 5 Accuracy": 97.1, "Number of params": 55800000.0, "GFLOPs": 26.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841137, "title": "Contextual Transformer Networks for Visual Recognition", "url": "/paper/contextual-transformer-networks-for-visual", "published": "2021-07-26T00:00:00.000000", "code": true, "review_url": "/paper/contextual-transformer-networks-for-visual/review/?hl=63784"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 78961, "rank": 199, "method": "MogaNet-L", "mlmodel": {}, "method_short": "MogaNet-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-07", "metrics": {"Top 1 Accuracy": "84.6%", "Top 5 Accuracy": null, "Number of params": "83", "GFLOPs": "15.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.6, "Top 5 Accuracy": null, "Number of params": 83.0, "GFLOPs": 15.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1106694, "title": "Efficient Multi-order Gated Aggregation Network", "url": "/paper/efficient-multi-order-gated-aggregation", "published": "2022-11-07T00:00:00.000000", "code": true, "review_url": "/paper/efficient-multi-order-gated-aggregation/review/?hl=78961"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 53852, "rank": 200, "method": "Sequencer2D-L\u2191392", "mlmodel": {}, "method_short": "Sequencer2D-L\u2191392", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-04", "metrics": {"Top 1 Accuracy": "84.6%", "Top 5 Accuracy": null, "Number of params": "54M", "GFLOPs": "50.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.6, "Top 5 Accuracy": null, "Number of params": 54000000.0, "GFLOPs": 50.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1004136, "title": "Sequencer: Deep LSTM for Image Classification", "url": "/paper/sequencer-deep-lstm-for-image-classification", "published": "2022-05-04T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 51585, "rank": 201, "method": "DaViT-B", "mlmodel": {}, "method_short": "DaViT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "84.6%", "Top 5 Accuracy": null, "Number of params": "87.9M", "GFLOPs": "15.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.6, "Top 5 Accuracy": null, "Number of params": 87900000.0, "GFLOPs": 15.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=51585"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 59086, "rank": 202, "method": "GC ViT-B", "mlmodel": {}, "method_short": "GC ViT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-20", "metrics": {"Top 1 Accuracy": "84.5%", "Top 5 Accuracy": null, "Number of params": "90M", "GFLOPs": "14.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.5, "Top 5 Accuracy": null, "Number of params": 90000000.0, "GFLOPs": 14.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029616, "title": "Global Context Vision Transformers", "url": "/paper/global-context-vision-transformers", "published": "2022-06-20T00:00:00.000000", "code": true, "review_url": "/paper/global-context-vision-transformers/review/?hl=59086"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10906, "rank": 203, "method": "ResNeSt-269", "mlmodel": {}, "method_short": "ResNeSt-269", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-04-19", "metrics": {"Top 1 Accuracy": "84.5%", "Top 5 Accuracy": null, "Number of params": "111M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.5, "Top 5 Accuracy": null, "Number of params": 111000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 191448, "title": "ResNeSt: Split-Attention Networks", "url": "/paper/resnest-split-attention-networks", "published": "2020-04-19T00:00:00.000000", "code": true, "review_url": "/paper/resnest-split-attention-networks/review/?hl=10906"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 71539, "rank": 204, "method": "MaxViT-S (224res)", "mlmodel": {}, "method_short": "MaxViT-S ", "method_details": "224res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "84.45%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.45, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71539"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 3084, "rank": 205, "method": "EfficientNet-B7", "mlmodel": {}, "method_short": "EfficientNet-B7", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "84.4%", "Top 5 Accuracy": "97.1", "Number of params": "66M", "GFLOPs": "37", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.4, "Top 5 Accuracy": 97.1, "Number of params": 66000000.0, "GFLOPs": 37.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=3084"}, "external_source_url": null, "tags": [{"id": 334, "name": "\u56fe\u50cf\u5206\u7c7b\u6570\u636e", "color": "#c8d327"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 5154, "rank": 206, "method": "GPIPE", "mlmodel": {}, "method_short": "GPIPE", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-11-16", "metrics": {"Top 1 Accuracy": "84.4%", "Top 5 Accuracy": "97%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.4, "Top 5 Accuracy": 97.0, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 62516, "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism", "url": "/paper/gpipe-efficient-training-of-giant-neural", "published": "2018-11-16T00:00:00.000000", "code": true, "review_url": "/paper/gpipe-efficient-training-of-giant-neural/review/?hl=5154"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 70628, "rank": 207, "method": "DiNAT-Base", "mlmodel": {}, "method_short": "DiNAT-Base", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "84.4%", "Top 5 Accuracy": null, "Number of params": "90M", "GFLOPs": "13.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.4, "Top 5 Accuracy": null, "Number of params": 90000000.0, "GFLOPs": 13.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=70628"}, "external_source_url": null, "tags": [{"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28377, "rank": 208, "method": "ResNet-RS-50 (160 image res)", "mlmodel": {}, "method_short": "ResNet-RS-50 ", "method_details": "160 image res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-13", "metrics": {"Top 1 Accuracy": "84.4%", "Top 5 Accuracy": null, "Number of params": "192M", "GFLOPs": "4.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.4, "Top 5 Accuracy": null, "Number of params": 192000000.0, "GFLOPs": 4.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 754198, "title": "Revisiting ResNets: Improved Training and Scaling Strategies", "url": "/paper/revisiting-resnets-improved-training-and", "published": "2021-03-13T00:00:00.000000", "code": true, "review_url": "/paper/revisiting-resnets-improved-training-and/review/?hl=28377"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 75981, "rank": 209, "method": "ColorNet (RHYLH with Conv Layer)", "mlmodel": {}, "method_short": "ColorNet ", "method_details": "RHYLH with Conv Layer", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-01", "metrics": {"Top 1 Accuracy": "84.32%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.32, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 93126, "title": "ColorNet: Investigating the importance of color spaces for image classification", "url": "/paper/colornet-investigating-the-importance-of", "published": "2019-02-01T00:00:00.000000", "code": true, "review_url": "/paper/colornet-investigating-the-importance-of/review/?hl=75981"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 8299, "rank": 210, "method": "ResNeXt-101 32x8d (semi-weakly sup.)", "mlmodel": {}, "method_short": "ResNeXt-101 32x8d ", "method_details": "semi-weakly sup.", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-02", "metrics": {"Top 1 Accuracy": "84.3%", "Top 5 Accuracy": "97.2", "Number of params": "88M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.3, "Top 5 Accuracy": 97.2, "Number of params": 88000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 113449, "title": "Billion-scale semi-supervised learning for image classification", "url": "/paper/billion-scale-semi-supervised-learning-for", "published": "2019-05-02T00:00:00.000000", "code": true, "review_url": "/paper/billion-scale-semi-supervised-learning-for/review/?hl=8299"}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 44121, "rank": 211, "method": "SReT-S (512 res, ImageNet-1K only)", "mlmodel": {}, "method_short": "SReT-S ", "method_details": "512 res, ImageNet-1K only", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-09", "metrics": {"Top 1 Accuracy": "84.3%", "Top 5 Accuracy": "97.0", "Number of params": "21.3M", "GFLOPs": "42.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.3, "Top 5 Accuracy": 97.0, "Number of params": 21300000.0, "GFLOPs": 42.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 907412, "title": "Sliced Recursive Transformer", "url": "/paper/sliced-recursive-transformer-1", "published": "2021-11-09T00:00:00.000000", "code": true, "review_url": "/paper/sliced-recursive-transformer-1/review/?hl=44121"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 49539, "rank": 212, "method": "ViT-B@384 (attn finetune)", "mlmodel": {}, "method_short": "ViT-B@384 ", "method_details": "attn finetune", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "84.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49539"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 21051, "rank": 213, "method": "LambdaResNet200", "mlmodel": {}, "method_short": "LambdaResNet200", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-17", "metrics": {"Top 1 Accuracy": "84.3%", "Top 5 Accuracy": null, "Number of params": "42M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.3, "Top 5 Accuracy": null, "Number of params": 42000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 746268, "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention", "url": "/paper/lambdanetworks-modeling-long-range-1", "published": "2021-02-17T00:00:00.000000", "code": true, "review_url": "/paper/lambdanetworks-modeling-long-range-1/review/?hl=21051"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10496, "rank": 214, "method": "TResNet-XL", "mlmodel": {}, "method_short": "TResNet-XL", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "84.3%", "Top 5 Accuracy": null, "Number of params": "77M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.3, "Top 5 Accuracy": null, "Number of params": 77000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188636, "title": "TResNet: High Performance GPU-Dedicated Architecture", "url": "/paper/tresnet-high-performance-gpu-dedicated", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/tresnet-high-performance-gpu-dedicated/review/?hl=10496"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 52492, "rank": 215, "method": "NAT-Base", "mlmodel": {}, "method_short": "NAT-Base", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "84.3%", "Top 5 Accuracy": null, "Number of params": "90M", "GFLOPs": "13.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.3, "Top 5 Accuracy": null, "Number of params": 90000000.0, "GFLOPs": 13.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994419, "title": "Neighborhood Attention Transformer", "url": "/paper/neighborhood-attention-transformer", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/neighborhood-attention-transformer/review/?hl=52492"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}], "reports": []}, {"table_id": 116, "row_id": 60761, "rank": 216, "method": "ResNeXt-101 32\u00d716d", "mlmodel": {}, "method_short": "ResNeXt-101 32\u00d716d", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-05-02", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": "97.2%", "Number of params": "194M", "GFLOPs": "72", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": 97.2, "Number of params": 194000000.0, "GFLOPs": 72.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 4635, "title": "Exploring the Limits of Weakly Supervised Pretraining", "url": "/paper/exploring-the-limits-of-weakly-supervised", "published": "2018-05-02T00:00:00.000000", "code": true, "review_url": "/paper/exploring-the-limits-of-weakly-supervised/review/?hl=60761"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 24941, "rank": 217, "method": "BoTNet T7-320", "mlmodel": {}, "method_short": "BoTNet T7-320", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": "96.9%", "Number of params": "75.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": 96.9, "Number of params": 75100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24941"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 78960, "rank": 218, "method": "MogaNet-B", "mlmodel": {}, "method_short": "MogaNet-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-07", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": "44", "GFLOPs": "9.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": 44.0, "GFLOPs": 9.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1106694, "title": "Efficient Multi-order Gated Aggregation Network", "url": "/paper/efficient-multi-order-gated-aggregation", "published": "2022-11-07T00:00:00.000000", "code": true, "review_url": "/paper/efficient-multi-order-gated-aggregation/review/?hl=78960"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 11200, "rank": 219, "method": "Assemble-ResNet152", "mlmodel": {}, "method_short": "Assemble-ResNet152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-01-17", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "15.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 15.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 180140, "title": "Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network", "url": "/paper/compounding-the-performance-improvements-of", "published": "2020-01-17T00:00:00.000000", "code": true, "review_url": "/paper/compounding-the-performance-improvements-of/review/?hl=11200"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 38754, "rank": 220, "method": "ViP-B|384", "mlmodel": {}, "method_short": "ViP-B|384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-13", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 834924, "title": "Visual Parser: Representing Part-whole Hierarchies with Transformers", "url": "/paper/visual-parser-representing-part-whole", "published": "2021-07-13T00:00:00.000000", "code": true, "review_url": "/paper/visual-parser-representing-part-whole/review/?hl=38754"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 55710, "rank": 221, "method": "Bamboo (Bamboo-B)", "mlmodel": {}, "method_short": "Bamboo ", "method_details": "Bamboo-B", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-21", "metrics": {"Top 1 Accuracy": "84.2", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1014071, "title": "Deeper vs Wider: A Revisit of Transformer Configuration", "url": "/paper/deeper-vs-wider-a-revisit-of-transformer", "published": "2022-05-21T00:00:00.000000", "code": false, "review_url": "/paper/deeper-vs-wider-a-revisit-of-transformer/review/?hl=55710"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 51586, "rank": 222, "method": "DaViT-S", "mlmodel": {}, "method_short": "DaViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": "49.7M", "GFLOPs": "8.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": 49700000.0, "GFLOPs": 8.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=51586"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 48308, "rank": 223, "method": "UniNet-B4", "mlmodel": {}, "method_short": "UniNet-B4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-08", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": "73.5M", "GFLOPs": "9.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": 73500000.0, "GFLOPs": 9.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 881795, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with", "published": "2021-10-08T00:00:00.000000", "code": false, "review_url": "/paper/uninet-unified-architecture-search-with/review/?hl=48308"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 23828, "rank": 224, "method": "DeiT-B", "mlmodel": {}, "method_short": "DeiT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-12-23", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": "86M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": 86000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 731001, "title": "Training data-efficient image transformers \u0026 distillation through attention", "url": "/paper/training-data-efficient-image-transformers", "published": "2020-12-23T00:00:00.000000", "code": true, "review_url": "/paper/training-data-efficient-image-transformers/review/?hl=23828"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 64206, "rank": 225, "method": "RevBiFPN-S6", "mlmodel": {}, "method_short": "RevBiFPN-S6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": null, "Number of params": "142.3M", "GFLOPs": "38.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": null, "Number of params": 142300000.0, "GFLOPs": 38.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64206"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11201, "rank": 226, "method": "NoisyStudent (EfficientNet-B3)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": "96.9%", "Number of params": "12M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": 96.9, "Number of params": 12000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11201"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 60748, "rank": 227, "method": "FBNetV5-F-CLS", "mlmodel": {}, "method_short": "FBNetV5-F-CLS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-19", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "2.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 2.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912986, "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run", "url": "/paper/fbnetv5-neural-architecture-search-for", "published": "2021-11-19T00:00:00.000000", "code": false, "review_url": "/paper/fbnetv5-neural-architecture-search-for/review/?hl=60748"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 49535, "rank": 228, "method": "ViT-B-36x1", "mlmodel": {}, "method_short": "ViT-B-36x1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49535"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 49536, "rank": 229, "method": "ViT-B-18x2", "mlmodel": {}, "method_short": "ViT-B-18x2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49536"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29207, "rank": 230, "method": "CAIT-XS-24", "mlmodel": {}, "method_short": "CAIT-XS-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": "26.6M", "GFLOPs": "19.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": 26600000.0, "GFLOPs": 19.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29207"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 30487, "rank": 231, "method": "LV-ViT-M", "mlmodel": {}, "method_short": "LV-ViT-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-22", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": "56M", "GFLOPs": "16", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": 56000000.0, "GFLOPs": 16.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 787097, "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers", "url": "/paper/token-labeling-training-a-85-5-top-1-accuracy", "published": "2021-04-22T00:00:00.000000", "code": true, "review_url": "/paper/token-labeling-training-a-85-5-top-1-accuracy/review/?hl=30487"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 32090, "rank": 232, "method": "Conformer-B", "mlmodel": {}, "method_short": "Conformer-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-09", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": "83.3M", "GFLOPs": "46.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": 83300000.0, "GFLOPs": 46.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 796024, "title": "Conformer: Local Features Coupling Global Representations for Visual Recognition", "url": "/paper/conformer-local-features-coupling-global", "published": "2021-05-09T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 44963, "rank": 233, "method": "PatchConvNet-B120", "mlmodel": {}, "method_short": "PatchConvNet-B120", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "84.1%", "Top 5 Accuracy": null, "Number of params": "188.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.1, "Top 5 Accuracy": null, "Number of params": 188600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}], "reports": []}, {"table_id": 116, "row_id": 10365, "rank": 234, "method": "FixEfficientNetB4", "mlmodel": {}, "method_short": "FixEfficientNetB4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "84.0%", "Top 5 Accuracy": "97.0%", "Number of params": "19M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": 97.0, "Number of params": 19000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10365"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 18195, "rank": 235, "method": "Fix_ResNet50_vd_ssld", "mlmodel": {}, "method_short": "Fix_ResNet50_vd_ssld", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-18", "metrics": {"Top 1 Accuracy": "84.0%", "Top 5 Accuracy": "97.0%", "Number of params": "25.58M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": 97.0, "Number of params": 25580000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 203194, "title": "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset", "url": "/paper/semi-supervised-recognition-under-a-noisy-and", "published": "2020-06-18T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6181, "rank": 236, "method": "EfficientNet-B6", "mlmodel": {}, "method_short": "EfficientNet-B6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "84%", "Top 5 Accuracy": "96.9", "Number of params": "43M", "GFLOPs": "19", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": 96.9, "Number of params": 43000000.0, "GFLOPs": 19.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6181"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 24940, "rank": 237, "method": "BoTNet T6", "mlmodel": {}, "method_short": "BoTNet T6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "84%", "Top 5 Accuracy": "96.7%", "Number of params": "53.9M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": 96.7, "Number of params": 53900000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24940"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 70345, "rank": 238, "method": "GPaCo (Vit-B)", "mlmodel": {}, "method_short": "GPaCo ", "method_details": "Vit-B", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-26", "metrics": {"Top 1 Accuracy": "84.0%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1080045, "title": "Generalized Parametric Contrastive Learning", "url": "/paper/generalized-parametric-contrastive-learning", "published": "2022-09-26T00:00:00.000000", "code": true, "review_url": "/paper/generalized-parametric-contrastive-learning/review/?hl=70345"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 21050, "rank": 239, "method": "LambdaResNet152", "mlmodel": {}, "method_short": "LambdaResNet152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-17", "metrics": {"Top 1 Accuracy": "84.0%", "Top 5 Accuracy": null, "Number of params": "35M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": null, "Number of params": 35000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 746268, "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention", "url": "/paper/lambdanetworks-modeling-long-range-1", "published": "2021-02-17T00:00:00.000000", "code": true, "review_url": "/paper/lambdanetworks-modeling-long-range-1/review/?hl=21050"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 59085, "rank": 240, "method": "GC ViT-S", "mlmodel": {}, "method_short": "GC ViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-20", "metrics": {"Top 1 Accuracy": "84.0%", "Top 5 Accuracy": null, "Number of params": "51M", "GFLOPs": "8.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": null, "Number of params": 51000000.0, "GFLOPs": 8.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029616, "title": "Global Context Vision Transformers", "url": "/paper/global-context-vision-transformers", "published": "2022-06-20T00:00:00.000000", "code": true, "review_url": "/paper/global-context-vision-transformers/review/?hl=59085"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29410, "rank": 241, "method": "PiT-B", "mlmodel": {}, "method_short": "PiT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-30", "metrics": {"Top 1 Accuracy": "84%", "Top 5 Accuracy": null, "Number of params": "73.8M", "GFLOPs": "12.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 84.0, "Top 5 Accuracy": null, "Number of params": 73800000.0, "GFLOPs": 12.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 771780, "title": "Rethinking Spatial Dimensions of Vision Transformers", "url": "/paper/rethinking-spatial-dimensions-of-vision", "published": "2021-03-30T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-spatial-dimensions-of-vision/review/?hl=29410"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 63782, "rank": 242, "method": "Wave-ViT-S", "mlmodel": {}, "method_short": "Wave-ViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-11", "metrics": {"Top 1 Accuracy": "83.9%", "Top 5 Accuracy": "96.6%", "Number of params": "22.7M", "GFLOPs": "4.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.9, "Top 5 Accuracy": 96.6, "Number of params": 22700000.0, "GFLOPs": 4.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042136, "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning", "url": "/paper/wave-vit-unifying-wavelet-and-transformers", "published": "2022-07-11T00:00:00.000000", "code": true, "review_url": "/paper/wave-vit-unifying-wavelet-and-transformers/review/?hl=63782"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 4109, "rank": 243, "method": "AmoebaNet-A", "mlmodel": {}, "method_short": "AmoebaNet-A", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-02-05", "metrics": {"Top 1 Accuracy": "83.9%", "Top 5 Accuracy": "96.6", "Number of params": "469M", "GFLOPs": "208", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.9, "Top 5 Accuracy": 96.6, "Number of params": 469000000.0, "GFLOPs": 208.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 9162, "title": "Regularized Evolution for Image Classifier Architecture Search", "url": "/paper/regularized-evolution-for-image-classifier", "published": "2018-02-05T00:00:00.000000", "code": true, "review_url": "/paper/regularized-evolution-for-image-classifier/review/?hl=4109"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29225, "rank": 244, "method": "EfficientNetV2-S", "mlmodel": {}, "method_short": "EfficientNetV2-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-01", "metrics": {"Top 1 Accuracy": "83.9%", "Top 5 Accuracy": null, "Number of params": "24M", "GFLOPs": "8.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.9, "Top 5 Accuracy": null, "Number of params": 24000000.0, "GFLOPs": 8.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 773495, "title": "EfficientNetV2: Smaller Models and Faster Training", "url": "/paper/efficientnetv2-smaller-models-and-faster", "published": "2021-04-01T00:00:00.000000", "code": true, "review_url": "/paper/efficientnetv2-smaller-models-and-faster/review/?hl=29225"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 34303, "rank": 245, "method": "DynamicViT-LV-M/0.8", "mlmodel": {}, "method_short": "DynamicViT-LV-M/0.8", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-03", "metrics": {"Top 1 Accuracy": "83.9", "Top 5 Accuracy": null, "Number of params": "57.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.9, "Top 5 Accuracy": null, "Number of params": 57100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 810914, "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification", "url": "/paper/dynamicvit-efficient-vision-transformers-with", "published": "2021-06-03T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 27416, "rank": 246, "method": "TNT-B", "mlmodel": {}, "method_short": "TNT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-27", "metrics": {"Top 1 Accuracy": "83.9%", "Top 5 Accuracy": null, "Number of params": "65.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.9, "Top 5 Accuracy": null, "Number of params": 65600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 749686, "title": "Transformer in Transformer", "url": "/paper/transformer-in-transformer", "published": "2021-02-27T00:00:00.000000", "code": true, "review_url": "/paper/transformer-in-transformer/review/?hl=27416"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 10905, "rank": 247, "method": "ResNeSt-200", "mlmodel": {}, "method_short": "ResNeSt-200", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-04-19", "metrics": {"Top 1 Accuracy": "83.9%", "Top 5 Accuracy": null, "Number of params": "70M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.9, "Top 5 Accuracy": null, "Number of params": 70000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 191448, "title": "ResNeSt: Split-Attention Networks", "url": "/paper/resnest-split-attention-networks", "published": "2020-04-19T00:00:00.000000", "code": true, "review_url": "/paper/resnest-split-attention-networks/review/?hl=10905"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 55044, "rank": 248, "method": "CLCNet (S:B4+D:B7)", "mlmodel": {}, "method_short": "CLCNet ", "method_details": "S:B4+D:B7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-19", "metrics": {"Top 1 Accuracy": "83.88%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "18.58", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.88, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 18.58, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1012740, "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network", "url": "/paper/clcnet-rethinking-of-ensemble-modeling-with", "published": "2022-05-19T00:00:00.000000", "code": true, "review_url": "/paper/clcnet-rethinking-of-ensemble-modeling-with/review/?hl=55044"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 44120, "rank": 249, "method": "SReT-S (384 res, ImageNet-1K only)", "mlmodel": {}, "method_short": "SReT-S ", "method_details": "384 res, ImageNet-1K only", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-09", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": "96.8", "Number of params": "21M", "GFLOPs": "18.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": 96.8, "Number of params": 21000000.0, "GFLOPs": 18.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 907412, "title": "Sliced Recursive Transformer", "url": "/paper/sliced-recursive-transformer-1", "published": "2021-11-09T00:00:00.000000", "code": true, "review_url": "/paper/sliced-recursive-transformer-1/review/?hl=44120"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 24939, "rank": 250, "method": "SENet-350", "mlmodel": {}, "method_short": "SENet-350", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": "96.6%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": 96.6, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24939"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 28383, "rank": 251, "method": "ResNet-RS-270 (256 image res)", "mlmodel": {}, "method_short": "ResNet-RS-270 ", "method_details": "256 image res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-13", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "54", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 54.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 754198, "title": "Revisiting ResNets: Improved Training and Scaling Strategies", "url": "/paper/revisiting-resnets-improved-training-and", "published": "2021-03-13T00:00:00.000000", "code": true, "review_url": "/paper/revisiting-resnets-improved-training-and/review/?hl=28383"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 52538, "rank": 252, "method": "ViT-B @224 (DeiT III)", "mlmodel": {}, "method_short": "ViT-B @224 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52538"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 70627, "rank": 253, "method": "DiNAT-Small", "mlmodel": {}, "method_short": "DiNAT-Small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": null, "Number of params": "51M", "GFLOPs": "7.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": null, "Number of params": 51000000.0, "GFLOPs": 7.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=70627"}, "external_source_url": null, "tags": [{"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 33775, "rank": 254, "method": "Transformer local-attention (NesT-B)", "mlmodel": {}, "method_short": "Transformer local-attention ", "method_details": "NesT-B", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-26", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": null, "Number of params": "68M", "GFLOPs": "17.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": null, "Number of params": 68000000.0, "GFLOPs": 17.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 805906, "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding", "url": "/paper/aggregating-nested-transformers", "published": "2021-05-26T00:00:00.000000", "code": true, "review_url": "/paper/aggregating-nested-transformers/review/?hl=33775"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 36913, "rank": 255, "method": "PVTv2-B4", "mlmodel": {}, "method_short": "PVTv2-B4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-25", "metrics": {"Top 1 Accuracy": "83.8%", "Top 5 Accuracy": null, "Number of params": "82M", "GFLOPs": "11.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.8, "Top 5 Accuracy": null, "Number of params": 82000000.0, "GFLOPs": 11.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 825108, "title": "PVT v2: Improved Baselines with Pyramid Vision Transformer", "url": "/paper/pvtv2-improved-baselines-with-pyramid-vision", "published": "2021-06-25T00:00:00.000000", "code": true, "review_url": "/paper/pvtv2-improved-baselines-with-pyramid-vision/review/?hl=36913"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5977, "rank": 256, "method": "FixPNASNet-5", "mlmodel": {}, "method_short": "FixPNASNet-5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-14", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": "96.8", "Number of params": "86.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": 96.8, "Number of params": 86100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 142997, "title": "Fixing the train-test resolution discrepancy", "url": "/paper/fixing-the-train-test-resolution-discrepancy", "published": "2019-06-14T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy/review/?hl=5977"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 77457, "rank": 257, "method": "Pyramid ViG-B", "mlmodel": {}, "method_short": "Pyramid ViG-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-01", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1019895, "title": "Vision GNN: An Image is Worth Graph of Nodes", "url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes", "published": "2022-06-01T00:00:00.000000", "code": true, "review_url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes/review/?hl=77457"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71445, "rank": 258, "method": "DAT-S", "mlmodel": {}, "method_short": "DAT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-03", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": null, "Number of params": "50M", "GFLOPs": "9.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": null, "Number of params": 50000000.0, "GFLOPs": 9.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 934161, "title": "Vision Transformer with Deformable Attention", "url": "/paper/vision-transformer-with-deformable-attention", "published": "2022-01-03T00:00:00.000000", "code": true, "review_url": "/paper/vision-transformer-with-deformable-attention/review/?hl=71445"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 52491, "rank": 259, "method": "NAT-Small", "mlmodel": {}, "method_short": "NAT-Small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": null, "Number of params": "51M", "GFLOPs": "7.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": null, "Number of params": 51000000.0, "GFLOPs": 7.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994419, "title": "Neighborhood Attention Transformer", "url": "/paper/neighborhood-attention-transformer", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/neighborhood-attention-transformer/review/?hl=52491"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}], "reports": []}, {"table_id": 116, "row_id": 52669, "rank": 260, "method": "QnA-ViT-Base", "mlmodel": {}, "method_short": "QnA-ViT-Base", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-21", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": null, "Number of params": "56M", "GFLOPs": "9.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": null, "Number of params": 56000000.0, "GFLOPs": 9.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933167, "title": "Learned Queries for Efficient Local Attention", "url": "/paper/learned-queries-for-efficient-local-attention", "published": "2021-12-21T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 64205, "rank": 261, "method": "RevBiFPN-S5", "mlmodel": {}, "method_short": "RevBiFPN-S5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": null, "Number of params": "82M", "GFLOPs": "21.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": null, "Number of params": 82000000.0, "GFLOPs": 21.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64205"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 32184, "rank": 262, "method": "Twins-SVT-L", "mlmodel": {}, "method_short": "Twins-SVT-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-28", "metrics": {"Top 1 Accuracy": "83.7%", "Top 5 Accuracy": null, "Number of params": "99.2M", "GFLOPs": "15.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.7, "Top 5 Accuracy": null, "Number of params": 99200000.0, "GFLOPs": 15.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 788788, "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers", "url": "/paper/twins-revisiting-spatial-attention-design-in", "published": "2021-04-28T00:00:00.000000", "code": true, "review_url": "/paper/twins-revisiting-spatial-attention-design-in/review/?hl=32184"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71930, "rank": 263, "method": "TransBoost-ViT-S", "mlmodel": {}, "method_short": "TransBoost-ViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "83.67%", "Top 5 Accuracy": null, "Number of params": "22.05M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.67, "Top 5 Accuracy": null, "Number of params": 22050000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71930"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71538, "rank": 264, "method": "MaxViT-T (224res)", "mlmodel": {}, "method_short": "MaxViT-T ", "method_details": "224res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-04", "metrics": {"Top 1 Accuracy": "83.62%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.62, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 988369, "title": "MaxViT: Multi-Axis Vision Transformer", "url": "/paper/maxvit-multi-axis-vision-transformer", "published": "2022-04-04T00:00:00.000000", "code": true, "review_url": "/paper/maxvit-multi-axis-vision-transformer/review/?hl=71538"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10370, "rank": 265, "method": "FixEfficientNet-B2", "mlmodel": {}, "method_short": "FixEfficientNet-B2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": "96.9%", "Number of params": "9.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": 96.9, "Number of params": 9200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10370"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25393, "rank": 266, "method": "NFNet-F0", "mlmodel": {}, "method_short": "NFNet-F0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-11", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": "96.8%", "Number of params": "71.5M", "GFLOPs": "12.38", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": 96.8, "Number of params": 71500000.0, "GFLOPs": 12.38, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 744370, "title": "High-Performance Large-Scale Image Recognition Without Normalization", "url": "/paper/high-performance-large-scale-image", "published": "2021-02-11T00:00:00.000000", "code": true, "review_url": "/paper/high-performance-large-scale-image/review/?hl=25393"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5008, "rank": 267, "method": "MultiGrain PNASNet (500px)", "mlmodel": {}, "method_short": "MultiGrain PNASNet ", "method_details": "500px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": "96.7%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": 96.7, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=5008"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60627, "rank": 268, "method": "SE-ResNeXt-101, 64x4d, S=2(320px)", "mlmodel": {}, "method_short": "SE-ResNeXt-101, 64x4d, S=2", "method_details": "320px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-30", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": "96.69%", "Number of params": "98M", "GFLOPs": "38.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": 96.69, "Number of params": 98000000.0, "GFLOPs": 38.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237979, "title": "Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training", "url": "/paper/splitnet-divide-and-co-training", "published": "2020-11-30T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 34008, "rank": 269, "method": "ResT-Large", "mlmodel": {}, "method_short": "ResT-Large", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-28", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": "96.3", "Number of params": "51.63M", "GFLOPs": "7.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": 96.3, "Number of params": 51630000.0, "GFLOPs": 7.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 807341, "title": "ResT: An Efficient Transformer for Visual Recognition", "url": "/paper/rest-an-efficient-transformer-for-visual", "published": "2021-05-28T00:00:00.000000", "code": true, "review_url": "/paper/rest-an-efficient-transformer-for-visual/review/?hl=34008"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 55677, "rank": 270, "method": "LITv2-B", "mlmodel": {}, "method_short": "LITv2-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "13.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 13.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016651, "title": "Fast Vision Transformers with HiLo Attention", "url": "/paper/fast-vision-transformers-with-hilo-attention", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/fast-vision-transformers-with-hilo-attention/review/?hl=55677"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 44084, "rank": 271, "method": "MAE (ViT-L)", "mlmodel": {}, "method_short": "MAE ", "method_details": "ViT-L", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-11", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 908690, "title": "Masked Autoencoders Are Scalable Vision Learners", "url": "/paper/masked-autoencoders-are-scalable-vision", "published": "2021-11-11T00:00:00.000000", "code": true, "review_url": "/paper/masked-autoencoders-are-scalable-vision/review/?hl=44084"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 36768, "rank": 272, "method": "ViTAE-B-Stage", "mlmodel": {}, "method_short": "ViTAE-B-Stage", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": null, "Number of params": "48.5M", "GFLOPs": "27.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": null, "Number of params": 48500000.0, "GFLOPs": 27.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812332, "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "url": "/paper/vitae-vision-transformer-advanced-by", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/vitae-vision-transformer-advanced-by/review/?hl=36768"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 49599, "rank": 273, "method": "ActiveMLP-L", "mlmodel": {}, "method_short": "ActiveMLP-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-11", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": null, "Number of params": "76.4M", "GFLOPs": "12.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": null, "Number of params": 76400000.0, "GFLOPs": 12.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 975658, "title": "ActiveMLP: An MLP-like Architecture with Active Token Mixer", "url": "/paper/activemlp-an-mlp-like-architecture-with", "published": "2022-03-11T00:00:00.000000", "code": true, "review_url": "/paper/activemlp-an-mlp-like-architecture-with/review/?hl=49599"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 34939, "rank": 274, "method": "ResMLP-B24/8", "mlmodel": {}, "method_short": "ResMLP-B24/8", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "83.6%", "Top 5 Accuracy": null, "Number of params": "116M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.6, "Top 5 Accuracy": null, "Number of params": 116000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795412, "title": "ResMLP: Feedforward networks for image classification with data-efficient training", "url": "/paper/resmlp-feedforward-networks-for-image", "published": "2021-05-07T00:00:00.000000", "code": true, "review_url": "/paper/resmlp-feedforward-networks-for-image/review/?hl=34939"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 24938, "rank": 275, "method": "BoTNet T5", "mlmodel": {}, "method_short": "BoTNet T5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "83.5%", "Top 5 Accuracy": "96.5%", "Number of params": "75.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.5, "Top 5 Accuracy": 96.5, "Number of params": 75100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24938"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 44962, "rank": 276, "method": "PatchConvNet-B60", "mlmodel": {}, "method_short": "PatchConvNet-B60", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "83.5%", "Top 5 Accuracy": null, "Number of params": "99.4M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.5, "Top 5 Accuracy": null, "Number of params": 99400000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}], "reports": []}, {"table_id": 116, "row_id": 8300, "rank": 277, "method": "ResNeXt-101 32x4d (semi-weakly sup.)", "mlmodel": {}, "method_short": "ResNeXt-101 32x4d ", "method_details": "semi-weakly sup.", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-02", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": "96.8", "Number of params": "42M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": 96.8, "Number of params": 42000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 113449, "title": "Billion-scale semi-supervised learning for image classification", "url": "/paper/billion-scale-semi-supervised-learning-for", "published": "2019-05-02T00:00:00.000000", "code": true, "review_url": "/paper/billion-scale-semi-supervised-learning-for/review/?hl=8300"}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 78959, "rank": 278, "method": "MogaNet-S", "mlmodel": {}, "method_short": "MogaNet-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-07", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": "25", "GFLOPs": "5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": 25.0, "GFLOPs": 5.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1106694, "title": "Efficient Multi-order Gated Aggregation Network", "url": "/paper/efficient-multi-order-gated-aggregation", "published": "2022-11-07T00:00:00.000000", "code": true, "review_url": "/paper/efficient-multi-order-gated-aggregation/review/?hl=78959"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 49541, "rank": 279, "method": "ViT-B (hMLP + BeiT)", "mlmodel": {}, "method_short": "ViT-B ", "method_details": "hMLP + BeiT", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49541"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60754, "rank": 280, "method": "UniFormer-S", "mlmodel": {}, "method_short": "UniFormer-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-24", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": "22M", "GFLOPs": "3.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": 22000000.0, "GFLOPs": 3.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 949483, "title": "UniFormer: Unifying Convolution and Self-attention for Visual Recognition", "url": "/paper/uniformer-unifying-convolution-and-self", "published": "2022-01-24T00:00:00.000000", "code": true, "review_url": "/paper/uniformer-unifying-convolution-and-self/review/?hl=60754"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 52537, "rank": 281, "method": "ViT-S @384 (DeiT III)", "mlmodel": {}, "method_short": "ViT-S @384 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": "22M", "GFLOPs": "15.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": 22000000.0, "GFLOPs": 15.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52537"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 59084, "rank": 282, "method": "GC ViT-T", "mlmodel": {}, "method_short": "GC ViT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-20", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "4.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 4.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029616, "title": "Global Context Vision Transformers", "url": "/paper/global-context-vision-transformers", "published": "2022-06-20T00:00:00.000000", "code": true, "review_url": "/paper/global-context-vision-transformers/review/?hl=59084"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 53849, "rank": 283, "method": "Sequencer2D-L", "mlmodel": {}, "method_short": "Sequencer2D-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-04", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": "54M", "GFLOPs": "16.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": 54000000.0, "GFLOPs": 16.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1004136, "title": "Sequencer: Deep LSTM for Image Classification", "url": "/paper/sequencer-deep-lstm-for-image-classification", "published": "2022-05-04T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 39382, "rank": 284, "method": "sMLPNet-B (ImageNet-1k)", "mlmodel": {}, "method_short": "sMLPNet-B ", "method_details": "ImageNet-1k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-12", "metrics": {"Top 1 Accuracy": "83.4%", "Top 5 Accuracy": null, "Number of params": "65.9M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.4, "Top 5 Accuracy": null, "Number of params": 65900000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 865635, "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?", "url": "/paper/sparse-mlp-for-image-recognition-is-self", "published": "2021-09-12T00:00:00.000000", "code": true, "review_url": "/paper/sparse-mlp-for-image-recognition-is-self/review/?hl=39382"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 22069, "rank": 285, "method": "SE-ResNeXt-101, 64x4d, S=2(416px)", "mlmodel": {}, "method_short": "SE-ResNeXt-101, 64x4d, S=2", "method_details": "416px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-30", "metrics": {"Top 1 Accuracy": "83.34%", "Top 5 Accuracy": "96.61%", "Number of params": "98M", "GFLOPs": "61.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.34, "Top 5 Accuracy": 96.61, "Number of params": 98000000.0, "GFLOPs": 61.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237979, "title": "Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training", "url": "/paper/splitnet-divide-and-co-training", "published": "2020-11-30T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 6182, "rank": 286, "method": "EfficientNet-B5", "mlmodel": {}, "method_short": "EfficientNet-B5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": "96.7", "Number of params": "30M", "GFLOPs": "9.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": 96.7, "Number of params": 30000000.0, "GFLOPs": 9.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6182"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 28491, "rank": 287, "method": "CeiT-S (384 finetune res)", "mlmodel": {}, "method_short": "CeiT-S ", "method_details": "384 finetune res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-22", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": "96.5", "Number of params": "24.2M", "GFLOPs": "12.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": 96.5, "Number of params": 24200000.0, "GFLOPs": 12.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755729, "title": "Incorporating Convolution Designs into Visual Transformers", "url": "/paper/incorporating-convolution-designs-into-visual", "published": "2021-03-22T00:00:00.000000", "code": true, "review_url": "/paper/incorporating-convolution-designs-into-visual/review/?hl=28491"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29399, "rank": 288, "method": "T2T-ViT-14|384", "mlmodel": {}, "method_short": "T2T-ViT-14|384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-28", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "34.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 34.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740117, "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "url": "/paper/tokens-to-token-vit-training-vision", "published": "2021-01-28T00:00:00.000000", "code": true, "review_url": "/paper/tokens-to-token-vit-training-vision/review/?hl=29399"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60759, "rank": 289, "method": "LV-ViT-S", "mlmodel": {}, "method_short": "LV-ViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-22", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "26M", "GFLOPs": "6.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 26000000.0, "GFLOPs": 6.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 787097, "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers", "url": "/paper/token-labeling-training-a-85-5-top-1-accuracy", "published": "2021-04-22T00:00:00.000000", "code": true, "review_url": "/paper/token-labeling-training-a-85-5-top-1-accuracy/review/?hl=60759"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 75969, "rank": 290, "method": "MOAT-0 1K only", "mlmodel": {}, "method_short": "MOAT-0 1K only", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-10-04", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "27.8M", "GFLOPs": "5.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 27800000.0, "GFLOPs": 5.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1087041, "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models", "url": "/paper/moat-alternating-mobile-convolution-and", "published": "2022-10-04T00:00:00.000000", "code": true, "review_url": "/paper/moat-alternating-mobile-convolution-and/review/?hl=75969"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29250, "rank": 291, "method": "CvT-21 (384 res)", "mlmodel": {}, "method_short": "CvT-21 ", "method_details": "384 res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "32M", "GFLOPs": "24.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 32000000.0, "GFLOPs": 24.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758429, "title": "CvT: Introducing Convolutions to Vision Transformers", "url": "/paper/cvt-introducing-convolutions-to-vision", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/cvt-introducing-convolutions-to-vision/review/?hl=29250"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 33774, "rank": 292, "method": "Transformer local-attention (NesT-S)", "mlmodel": {}, "method_short": "Transformer local-attention ", "method_details": "NesT-S", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-26", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "38M", "GFLOPs": "10.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 38000000.0, "GFLOPs": 10.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 805906, "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding", "url": "/paper/aggregating-nested-transformers", "published": "2021-05-26T00:00:00.000000", "code": true, "review_url": "/paper/aggregating-nested-transformers/review/?hl=33774"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29123, "rank": 293, "method": "ViL-Medium-D", "mlmodel": {}, "method_short": "ViL-Medium-D", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "39.7M", "GFLOPs": "8.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 39700000.0, "GFLOPs": 8.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=29123"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 55676, "rank": 294, "method": "LITv2-M", "mlmodel": {}, "method_short": "LITv2-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "49M", "GFLOPs": "7.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 49000000.0, "GFLOPs": 7.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016651, "title": "Fast Vision Transformers with HiLo Attention", "url": "/paper/fast-vision-transformers-with-hilo-attention", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/fast-vision-transformers-with-hilo-attention/review/?hl=55676"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60628, "rank": 295, "method": "Shift-B", "mlmodel": {}, "method_short": "Shift-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-26", "metrics": {"Top 1 Accuracy": "83.3%", "Top 5 Accuracy": null, "Number of params": "88M", "GFLOPs": "15.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.3, "Top 5 Accuracy": null, "Number of params": 88000000.0, "GFLOPs": 15.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 951213, "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism", "url": "/paper/when-shift-operation-meets-vision-transformer", "published": "2022-01-26T00:00:00.000000", "code": true, "review_url": "/paper/when-shift-operation-meets-vision-transformer/review/?hl=60628"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 57971, "rank": 296, "method": "Meta Pseudo Labels (ResNet-50)", "mlmodel": {}, "method_short": "Meta Pseudo Labels ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-23", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": "96.5", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": 96.5, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188189, "title": "Meta Pseudo Labels", "url": "/paper/meta-pseudo-labels", "published": "2020-03-23T00:00:00.000000", "code": true, "review_url": "/paper/meta-pseudo-labels/review/?hl=57971"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60267, "rank": 297, "method": "TinyViT-11M-distill (21k)", "mlmodel": {}, "method_short": "TinyViT-11M-distill ", "method_details": "21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": "96.5%", "Number of params": "11M", "GFLOPs": "2.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": 96.5, "Number of params": 11000000.0, "GFLOPs": 2.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60267"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 63783, "rank": 298, "method": "SE-CoTNetD-101", "mlmodel": {}, "method_short": "SE-CoTNetD-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-26", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": "96.5%", "Number of params": "40.9M", "GFLOPs": "8.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": 96.5, "Number of params": 40900000.0, "GFLOPs": 8.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841137, "title": "Contextual Transformer Networks for Visual Recognition", "url": "/paper/contextual-transformer-networks-for-visual", "published": "2021-07-26T00:00:00.000000", "code": true, "review_url": "/paper/contextual-transformer-networks-for-visual/review/?hl=63783"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11202, "rank": 299, "method": "MultiGrain PNASNet (450px)", "mlmodel": {}, "method_short": "MultiGrain PNASNet ", "method_details": "450px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=11202"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 52668, "rank": 300, "method": "QnA-ViT-Small", "mlmodel": {}, "method_short": "QnA-ViT-Small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-21", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "25M", "GFLOPs": "4.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 25000000.0, "GFLOPs": 4.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933167, "title": "Learned Queries for Efficient Local Attention", "url": "/paper/learned-queries-for-efficient-local-attention", "published": "2021-12-21T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 52490, "rank": 301, "method": "NAT-Tiny", "mlmodel": {}, "method_short": "NAT-Tiny", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "4.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 4.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994419, "title": "Neighborhood Attention Transformer", "url": "/paper/neighborhood-attention-transformer", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/neighborhood-attention-transformer/review/?hl=52490"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}], "reports": []}, {"table_id": 116, "row_id": 63702, "rank": 302, "method": "Next-ViT-B", "mlmodel": {}, "method_short": "Next-ViT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-12", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "44.8M", "GFLOPs": "8.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 44800000.0, "GFLOPs": 8.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042514, "title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios", "url": "/paper/next-vit-next-generation-vision-transformer", "published": "2022-07-12T00:00:00.000000", "code": true, "review_url": "/paper/next-vit-next-generation-vision-transformer/review/?hl=63702"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 36912, "rank": 303, "method": "PVTv2-B3", "mlmodel": {}, "method_short": "PVTv2-B3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-25", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "45.2M", "GFLOPs": "6.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 45200000.0, "GFLOPs": 6.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 825108, "title": "PVT v2: Improved Baselines with Pyramid Vision Transformer", "url": "/paper/pvtv2-improved-baselines-with-pyramid-vision", "published": "2021-06-25T00:00:00.000000", "code": true, "review_url": "/paper/pvtv2-improved-baselines-with-pyramid-vision/review/?hl=36912"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 44961, "rank": 304, "method": "PatchConvNet-S120", "mlmodel": {}, "method_short": "PatchConvNet-S120", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "47.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 47700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}], "reports": []}, {"table_id": 116, "row_id": 29125, "rank": 305, "method": "ViL-Base-D", "mlmodel": {}, "method_short": "ViL-Base-D", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "55.7M", "GFLOPs": "13.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 55700000.0, "GFLOPs": 13.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=29125"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 37317, "rank": 306, "method": "CycleMLP-B5", "mlmodel": {}, "method_short": "CycleMLP-B5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-21", "metrics": {"Top 1 Accuracy": "83.2%", "Top 5 Accuracy": null, "Number of params": "76M", "GFLOPs": "12.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.2, "Top 5 Accuracy": null, "Number of params": 76000000.0, "GFLOPs": 12.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 839264, "title": "CycleMLP: A MLP-like Architecture for Dense Prediction", "url": "/paper/cyclemlp-a-mlp-like-architecture-for-dense", "published": "2021-07-21T00:00:00.000000", "code": true, "review_url": "/paper/cyclemlp-a-mlp-like-architecture-for-dense/review/?hl=37317"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 60260, "rank": 307, "method": "TinyViT-21M", "mlmodel": {}, "method_short": "TinyViT-21M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": "96.5%", "Number of params": "21M", "GFLOPs": "4.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": 96.5, "Number of params": 21000000.0, "GFLOPs": 4.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60260"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 11203, "rank": 308, "method": "MultiGrain SENet154 (450px)", "mlmodel": {}, "method_short": "MultiGrain SENet154 ", "method_details": "450px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=11203"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28547, "rank": 309, "method": "DeepVit-L* (DeiT training recipe)", "mlmodel": {}, "method_short": "DeepVit-L* ", "method_details": "DeiT training recipe", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-22", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755709, "title": "DeepViT: Towards Deeper Vision Transformer", "url": "/paper/deepvit-towards-deeper-vision-transformer", "published": "2021-03-22T00:00:00.000000", "code": true, "review_url": "/paper/deepvit-towards-deeper-vision-transformer/review/?hl=28547"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 52543, "rank": 310, "method": "ViT-S @224 (DeiT III, 21k)", "mlmodel": {}, "method_short": "ViT-S @224 ", "method_details": "DeiT III, 21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52543"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 75998, "rank": 311, "method": "MKD ViT-S", "mlmodel": {}, "method_short": "MKD ViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-16", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 962993, "title": "Meta Knowledge Distillation", "url": "/paper/meta-knowledge-distillation", "published": "2022-02-16T00:00:00.000000", "code": false, "review_url": "/paper/meta-knowledge-distillation/review/?hl=75998"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 77456, "rank": 312, "method": "Pyramid ViG-M", "mlmodel": {}, "method_short": "Pyramid ViG-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-01", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1019895, "title": "Vision GNN: An Image is Worth Graph of Nodes", "url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes", "published": "2022-06-01T00:00:00.000000", "code": true, "review_url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes/review/?hl=77456"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 39381, "rank": 313, "method": "sMLPNet-S (ImageNet-1k)", "mlmodel": {}, "method_short": "sMLPNet-S ", "method_details": "ImageNet-1k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-12", "metrics": {"Top 1 Accuracy": "83.1%", "Top 5 Accuracy": null, "Number of params": "48.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.1, "Top 5 Accuracy": null, "Number of params": 48600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 865635, "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?", "url": "/paper/sparse-mlp-for-image-recognition-is-self", "published": "2021-09-12T00:00:00.000000", "code": true, "review_url": "/paper/sparse-mlp-for-image-recognition-is-self/review/?hl=39381"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 66005, "rank": 314, "method": "gSwin-S", "mlmodel": {}, "method_short": "gSwin-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-08-24", "metrics": {"Top 1 Accuracy": "83.01%", "Top 5 Accuracy": "96.32%", "Number of params": "39.8M", "GFLOPs": "7.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.01, "Top 5 Accuracy": 96.32, "Number of params": 39800000.0, "GFLOPs": 7.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1064636, "title": "gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window", "url": "/paper/gswin-gated-mlp-vision-model-with", "published": "2022-08-24T00:00:00.000000", "code": false, "review_url": "/paper/gswin-gated-mlp-vision-model-with/review/?hl=66005"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 48, "name": "Swin-Transformer", "color": "#f75c2f"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 5009, "rank": 315, "method": "MultiGrain SENet154 (400px)", "mlmodel": {}, "method_short": "MultiGrain SENet154 ", "method_details": "400px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "83.0%", "Top 5 Accuracy": "96.5%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": 96.5, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=5009"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 18193, "rank": 316, "method": "ResNet50_vd_ssld", "mlmodel": {}, "method_short": "ResNet50_vd_ssld", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-18", "metrics": {"Top 1 Accuracy": "83.0%", "Top 5 Accuracy": "96.4%", "Number of params": "25.58M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": 96.4, "Number of params": 25580000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 203194, "title": "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset", "url": "/paper/semi-supervised-recognition-under-a-noisy-and", "published": "2020-06-18T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29249, "rank": 317, "method": "CvT-13 (384 res)", "mlmodel": {}, "method_short": "CvT-13 ", "method_details": "384 res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "83%", "Top 5 Accuracy": null, "Number of params": "20M", "GFLOPs": "16.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": null, "Number of params": 20000000.0, "GFLOPs": 16.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758429, "title": "CvT: Introducing Convolutions to Vision Transformers", "url": "/paper/cvt-introducing-convolutions-to-vision", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/cvt-introducing-convolutions-to-vision/review/?hl=29249"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 38422, "rank": 318, "method": "MViT-B-16", "mlmodel": {}, "method_short": "MViT-B-16", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-22", "metrics": {"Top 1 Accuracy": "83.0%", "Top 5 Accuracy": null, "Number of params": "37.0M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": null, "Number of params": 37000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 787025, "title": "Multiscale Vision Transformers", "url": "/paper/multiscale-vision-transformers", "published": "2021-04-22T00:00:00.000000", "code": true, "review_url": "/paper/multiscale-vision-transformers/review/?hl=38422"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10904, "rank": 319, "method": "ResNeSt-101", "mlmodel": {}, "method_short": "ResNeSt-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-04-19", "metrics": {"Top 1 Accuracy": "83.0%", "Top 5 Accuracy": null, "Number of params": "48M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": null, "Number of params": 48000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 191448, "title": "ResNeSt: Split-Attention Networks", "url": "/paper/resnest-split-attention-networks", "published": "2020-04-19T00:00:00.000000", "code": true, "review_url": "/paper/resnest-split-attention-networks/review/?hl=10904"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 64204, "rank": 320, "method": "RevBiFPN-S4", "mlmodel": {}, "method_short": "RevBiFPN-S4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "83%", "Top 5 Accuracy": null, "Number of params": "48.7M", "GFLOPs": "10.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": null, "Number of params": 48700000.0, "GFLOPs": 10.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64204"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 54294, "rank": 321, "method": "ZenNAS (0.8ms)", "mlmodel": {}, "method_short": "ZenNAS ", "method_details": "0.8ms", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-01", "metrics": {"Top 1 Accuracy": "83.0%", "Top 5 Accuracy": null, "Number of params": "183M", "GFLOPs": "13.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 83.0, "Top 5 Accuracy": null, "Number of params": 183000000.0, "GFLOPs": 13.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740994, "title": "Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition", "url": "/paper/zen-nas-a-zero-shot-nas-for-high-performance", "published": "2021-02-01T00:00:00.000000", "code": true, "review_url": "/paper/zen-nas-a-zero-shot-nas-for-high-performance/review/?hl=54294"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 4708, "rank": 322, "method": "Oct-ResNet-152 (SE)", "mlmodel": {}, "method_short": "Oct-ResNet-152 ", "method_details": "SE", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-10", "metrics": {"Top 1 Accuracy": "82.9%", "Top 5 Accuracy": "96.3", "Number of params": "66.8M", "GFLOPs": "22.2", "Hardware Burden": "20771G", "Operations per network pass": "2.22G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.9, "Top 5 Accuracy": 96.3, "Number of params": 66800000.0, "GFLOPs": 22.2, "Hardware Burden": 20771.0, "Operations per network pass": 2.22, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 111242, "title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution", "url": "/paper/drop-an-octave-reducing-spatial-redundancy-in", "published": "2019-04-10T00:00:00.000000", "code": true, "review_url": "/paper/drop-an-octave-reducing-spatial-redundancy-in/review/?hl=4708"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 36274, "rank": 323, "method": "GFNet-H-B", "mlmodel": {}, "method_short": "GFNet-H-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-01", "metrics": {"Top 1 Accuracy": "82.9%", "Top 5 Accuracy": "96.2%", "Number of params": "54M", "GFLOPs": "8.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.9, "Top 5 Accuracy": 96.2, "Number of params": 54000000.0, "GFLOPs": 8.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 828703, "title": "Global Filter Networks for Image Classification", "url": "/paper/global-filter-networks-for-image", "published": "2021-07-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6126, "rank": 324, "method": "PNASNet-5", "mlmodel": {}, "method_short": "PNASNet-5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2017-12-02", "metrics": {"Top 1 Accuracy": "82.9%", "Top 5 Accuracy": "96.2", "Number of params": "86.1M", "GFLOPs": "50", "Hardware Burden": null, "Operations per network pass": "2.5G", "Number of parameters (M)": "86.1M"}, "raw_metrics": {"Top 1 Accuracy": 82.9, "Top 5 Accuracy": 96.2, "Number of params": 86100000.0, "GFLOPs": 50.0, "Hardware Burden": null, "Operations per network pass": 2.5, "Number of parameters (M)": 86100000.0}, "uses_additional_data": false, "paper": {"id": 7580, "title": "Progressive Neural Architecture Search", "url": "/paper/progressive-neural-architecture-search", "published": "2017-12-02T00:00:00.000000", "code": true, "review_url": "/paper/progressive-neural-architecture-search/review/?hl=6126"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60738, "rank": 325, "method": "NASViT (supernet)", "mlmodel": {}, "method_short": "NASViT ", "method_details": "supernet", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "82.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "1.881", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 1.881, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29122, "rank": 326, "method": "ViL-Medium-W", "mlmodel": {}, "method_short": "ViL-Medium-W", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "82.9%", "Top 5 Accuracy": null, "Number of params": "39.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.9, "Top 5 Accuracy": null, "Number of params": 39800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=29122"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 9794, "rank": 327, "method": "Harm-SE-RNX-101 64x4d (320x320, Mean-Max Pooling)", "mlmodel": {}, "method_short": "Harm-SE-RNX-101 64x4d ", "method_details": "320x320, Mean-Max Pooling", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-01-18", "metrics": {"Top 1 Accuracy": "82.85%", "Top 5 Accuracy": "96.44", "Number of params": "88.2M", "GFLOPs": "31.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.85, "Top 5 Accuracy": 96.44, "Number of params": 88200000.0, "GFLOPs": 31.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 180364, "title": "Harmonic Convolutional Networks based on Discrete Cosine Transform", "url": "/paper/harmonic-convolutional-networks-based-on", "published": "2020-01-18T00:00:00.000000", "code": true, "review_url": "/paper/harmonic-convolutional-networks-based-on/review/?hl=9794"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 49038, "rank": 328, "method": "ReXNet_3.0", "mlmodel": {}, "method_short": "ReXNet_3.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": "96.3", "Number of params": "34.7M", "GFLOPs": "3.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": 96.3, "Number of params": 34700000.0, "GFLOPs": 3.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49038"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24937, "rank": 329, "method": "BoTNet T4", "mlmodel": {}, "method_short": "BoTNet T4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": "96.3%", "Number of params": "54.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": 96.3, "Number of params": 54700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24937"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 34577, "rank": 330, "method": "FunMatch - T384+224 (ResNet-50)", "mlmodel": {}, "method_short": "FunMatch - T384+224 ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-09", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 814470, "title": "Knowledge distillation: A good teacher is patient and consistent", "url": "/paper/knowledge-distillation-a-good-teacher-is", "published": "2021-06-09T00:00:00.000000", "code": true, "review_url": "/paper/knowledge-distillation-a-good-teacher-is/review/?hl=34577"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 61463, "rank": 331, "method": "VAN-B2", "mlmodel": {}, "method_short": "VAN-B2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": "26.6M", "GFLOPs": "5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": 26600000.0, "GFLOPs": 5.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=61463"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 51587, "rank": 332, "method": "DaViT-T", "mlmodel": {}, "method_short": "DaViT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-07", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": "28.3M", "GFLOPs": "4.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": 28300000.0, "GFLOPs": 4.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 990738, "title": "DaViT: Dual Attention Vision Transformers", "url": "/paper/davit-dual-attention-vision-transformers", "published": "2022-04-07T00:00:00.000000", "code": true, "review_url": "/paper/davit-dual-attention-vision-transformers/review/?hl=51587"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 53850, "rank": 333, "method": "Sequencer2D-M", "mlmodel": {}, "method_short": "Sequencer2D-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-04", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": "38M", "GFLOPs": "11.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": 38000000.0, "GFLOPs": 11.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1004136, "title": "Sequencer: Deep LSTM for Image Classification", "url": "/paper/sequencer-deep-lstm-for-image-classification", "published": "2022-05-04T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29110, "rank": 334, "method": "CrossViT-18+", "mlmodel": {}, "method_short": "CrossViT-18+", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-27", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": "44.3M", "GFLOPs": "9.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": 44300000.0, "GFLOPs": 9.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758441, "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification", "url": "/paper/2103-14899", "published": "2021-03-27T00:00:00.000000", "code": true, "review_url": "/paper/2103-14899/review/?hl=29110"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 46844, "rank": 335, "method": "Shift-S", "mlmodel": {}, "method_short": "Shift-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-26", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": "50M", "GFLOPs": "8.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": 50000000.0, "GFLOPs": 8.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 951213, "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism", "url": "/paper/when-shift-operation-meets-vision-transformer", "published": "2022-01-26T00:00:00.000000", "code": true, "review_url": "/paper/when-shift-operation-meets-vision-transformer/review/?hl=46844"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 58456, "rank": 336, "method": "HRFormer-B", "mlmodel": {}, "method_short": "HRFormer-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-18", "metrics": {"Top 1 Accuracy": "82.8%", "Top 5 Accuracy": null, "Number of params": "50.3M", "GFLOPs": "13.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.8, "Top 5 Accuracy": null, "Number of params": 50300000.0, "GFLOPs": 13.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 890071, "title": "HRFormer: High-Resolution Transformer for Dense Prediction", "url": "/paper/hrformer-high-resolution-transformer-for", "published": "2021-10-18T00:00:00.000000", "code": true, "review_url": "/paper/hrformer-high-resolution-transformer-for/review/?hl=58456"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 38753, "rank": 337, "method": "CCT-14/7x2 | 384", "mlmodel": {}, "method_short": "CCT-14/7x2 | 384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "82.71%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.71, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778781, "title": "Escaping the Big Data Paradigm with Compact Transformers", "url": "/paper/escaping-the-big-data-paradigm-with-compact", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=38753"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 33621, "rank": 338, "method": "RVT-B*", "mlmodel": {}, "method_short": "RVT-B*", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-17", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": "96.5", "Number of params": "91.8M", "GFLOPs": "17.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": 96.5, "Number of params": 91800000.0, "GFLOPs": 17.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 800173, "title": "Towards Robust Vision Transformer", "url": "/paper/rethinking-the-design-principles-of-robust", "published": "2021-05-17T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-the-design-principles-of-robust/review/?hl=33621"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 2049, "rank": 339, "method": "NASNET-A(6)", "mlmodel": {}, "method_short": "NASNET-A", "method_details": "6", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2017-07-21", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": "96.2", "Number of params": "88.9M", "GFLOPs": "23.8", "Hardware Burden": "1648G", "Operations per network pass": "2.38G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": 96.2, "Number of params": 88900000.0, "GFLOPs": 23.8, "Hardware Burden": 1648.0, "Operations per network pass": 2.38, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 6283, "title": "Learning Transferable Architectures for Scalable Image Recognition", "url": "/paper/learning-transferable-architectures-for", "published": "2017-07-21T00:00:00.000000", "code": true, "review_url": "/paper/learning-transferable-architectures-for/review/?hl=2049"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11205, "rank": 340, "method": "MultiGrain SENet154 (500px)", "mlmodel": {}, "method_short": "MultiGrain SENet154 ", "method_details": "500px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=11205"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60639, "rank": 341, "method": "Container Container", "mlmodel": {}, "method_short": "Container Container", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-02", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": null, "Number of params": "22.1M", "GFLOPs": "8.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": null, "Number of params": 22100000.0, "GFLOPs": 8.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 811008, "title": "Container: Context Aggregation Network", "url": "/paper/container-context-aggregation-network", "published": "2021-06-02T00:00:00.000000", "code": true, "review_url": "/paper/container-context-aggregation-network/review/?hl=60639"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 48307, "rank": 342, "method": "UniNet-B2", "mlmodel": {}, "method_short": "UniNet-B2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-08", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": null, "Number of params": "22.5M", "GFLOPs": "2.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": null, "Number of params": 22500000.0, "GFLOPs": 2.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 881795, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with", "published": "2021-10-08T00:00:00.000000", "code": false, "review_url": "/paper/uninet-unified-architecture-search-with/review/?hl=48307"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 70626, "rank": 343, "method": "DiNAT-Tiny", "mlmodel": {}, "method_short": "DiNAT-Tiny", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "4.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 4.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=70626"}, "external_source_url": null, "tags": [{"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 61467, "rank": 344, "method": "ELSA-Swin-T", "mlmodel": {}, "method_short": "ELSA-Swin-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-23", "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "4.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 4.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 932838, "title": "ELSA: Enhanced Local Self-Attention for Vision Transformer", "url": "/paper/elsa-enhanced-local-self-attention-for-vision", "published": "2021-12-23T00:00:00.000000", "code": true, "review_url": "/paper/elsa-enhanced-local-self-attention-for-vision/review/?hl=61467"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10371, "rank": 345, "method": "FixEfficientNet-B1", "mlmodel": {}, "method_short": "FixEfficientNet-B1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": "96.5%", "Number of params": "7.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": 96.5, "Number of params": 7800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10371"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 6183, "rank": 346, "method": "EfficientNet-B4", "mlmodel": {}, "method_short": "EfficientNet-B4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": "96.3", "Number of params": "19M", "GFLOPs": "4.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": 96.3, "Number of params": 19000000.0, "GFLOPs": 4.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6183"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_efficientnet_b4.ipynb"}]}, {"table_id": 116, "row_id": 60747, "rank": 347, "method": "FBNetV5-C-CLS", "mlmodel": {}, "method_short": "FBNetV5-C-CLS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-19", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 1.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912986, "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run", "url": "/paper/fbnetv5-neural-architecture-search-for", "published": "2021-11-19T00:00:00.000000", "code": false, "review_url": "/paper/fbnetv5-neural-architecture-search-for/review/?hl=60747"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11206, "rank": 348, "method": "MultiGrain PNASNet (400px)", "mlmodel": {}, "method_short": "MultiGrain PNASNet ", "method_details": "400px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=11206"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 49537, "rank": 349, "method": "ViT-S-24x2", "mlmodel": {}, "method_short": "ViT-S-24x2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49537"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 23827, "rank": 350, "method": "DeiT-B", "mlmodel": {}, "method_short": "DeiT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-12-23", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": null, "Number of params": "22M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": null, "Number of params": 22000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 731001, "title": "Training data-efficient image transformers \u0026 distillation through attention", "url": "/paper/training-data-efficient-image-transformers", "published": "2020-12-23T00:00:00.000000", "code": true, "review_url": "/paper/training-data-efficient-image-transformers/review/?hl=23827"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 25003, "rank": 351, "method": "T2T-ViTt-24", "mlmodel": {}, "method_short": "T2T-ViTt-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-28", "metrics": {"Top 1 Accuracy": "82.6%", "Top 5 Accuracy": null, "Number of params": "64.4M", "GFLOPs": "30", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.6, "Top 5 Accuracy": null, "Number of params": 64400000.0, "GFLOPs": 30.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740117, "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "url": "/paper/tokens-to-token-vit-training-vision", "published": "2021-01-28T00:00:00.000000", "code": true, "review_url": "/paper/tokens-to-token-vit-training-vision/review/?hl=25003"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60622, "rank": 352, "method": "FixResNet-50 Billion-scale@224", "mlmodel": {}, "method_short": "FixResNet-50 Billion-scale@224", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-14", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": "96.6", "Number of params": "25.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": 96.6, "Number of params": 25600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 142997, "title": "Fixing the train-test resolution discrepancy", "url": "/paper/fixing-the-train-test-resolution-discrepancy", "published": "2019-06-14T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy/review/?hl=60622"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29248, "rank": 353, "method": "CvT-21", "mlmodel": {}, "method_short": "CvT-21", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "7.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 7.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758429, "title": "CvT: Introducing Convolutions to Vision Transformers", "url": "/paper/cvt-introducing-convolutions-to-vision", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/cvt-introducing-convolutions-to-vision/review/?hl=29248"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 63701, "rank": 354, "method": "Next-ViT-S", "mlmodel": {}, "method_short": "Next-ViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-12", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": null, "Number of params": "31.7M", "GFLOPs": "5.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": null, "Number of params": 31700000.0, "GFLOPs": 5.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042514, "title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios", "url": "/paper/next-vit-next-generation-vision-transformer", "published": "2022-07-12T00:00:00.000000", "code": true, "review_url": "/paper/next-vit-next-generation-vision-transformer/review/?hl=63701"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29461, "rank": 355, "method": "LeViT-384", "mlmodel": {}, "method_short": "LeViT-384", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-02", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": null, "Number of params": "39.4M", "GFLOPs": "2.334", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": null, "Number of params": 39400000.0, "GFLOPs": 2.334, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 774248, "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference", "url": "/paper/levit-a-vision-transformer-in-convnet-s", "published": "2021-04-02T00:00:00.000000", "code": true, "review_url": "/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29461"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29109, "rank": 356, "method": "CrossViT-18", "mlmodel": {}, "method_short": "CrossViT-18", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-27", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": null, "Number of params": "43.3M", "GFLOPs": "9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": null, "Number of params": 43300000.0, "GFLOPs": 9.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758441, "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification", "url": "/paper/2103-14899", "published": "2021-03-27T00:00:00.000000", "code": true, "review_url": "/paper/2103-14899/review/?hl=29109"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 43405, "rank": 357, "method": "MetaFormer PoolFormer-M48", "mlmodel": {}, "method_short": "MetaFormer PoolFormer-M48", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-22", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": null, "Number of params": "73M", "GFLOPs": "23.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": null, "Number of params": 73000000.0, "GFLOPs": 23.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 914420, "title": "MetaFormer Is Actually What You Need for Vision", "url": "/paper/metaformer-is-actually-what-you-need-for", "published": "2021-11-22T00:00:00.000000", "code": true, "review_url": "/paper/metaformer-is-actually-what-you-need-for/review/?hl=43405"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28363, "rank": 358, "method": "ConViT-B+", "mlmodel": {}, "method_short": "ConViT-B+", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "82.5%", "Top 5 Accuracy": null, "Number of params": "152M", "GFLOPs": "30", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.5, "Top 5 Accuracy": null, "Number of params": 152000000.0, "GFLOPs": 30.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755434, "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases", "url": "/paper/convit-improving-vision-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/convit-improving-vision-transformers-with/review/?hl=28363"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71929, "rank": 359, "method": "TransBoost-ConvNext-T", "mlmodel": {}, "method_short": "TransBoost-ConvNext-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "82.46%", "Top 5 Accuracy": null, "Number of params": "28.59M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.46, "Top 5 Accuracy": null, "Number of params": 28590000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71929"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11207, "rank": 360, "method": "NoisyStudent (EfficientNet-B2)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "82.4%", "Top 5 Accuracy": "96.3%", "Number of params": "9.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.4, "Top 5 Accuracy": 96.3, "Number of params": 9200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11207"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 37555, "rank": 361, "method": "AutoFormer-base", "mlmodel": {}, "method_short": "AutoFormer-base", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-01", "metrics": {"Top 1 Accuracy": "82.4%", "Top 5 Accuracy": null, "Number of params": "54M", "GFLOPs": "11", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.4, "Top 5 Accuracy": null, "Number of params": 54000000.0, "GFLOPs": 11.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 828740, "title": "AutoFormer: Searching Transformers for Visual Recognition", "url": "/paper/autoformer-searching-transformers-for-visual", "published": "2021-07-01T00:00:00.000000", "code": true, "review_url": "/paper/autoformer-searching-transformers-for-visual/review/?hl=37555"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 40188, "rank": 362, "method": "ResNet-152 (A2 + reg)", "mlmodel": {}, "method_short": "ResNet-152 ", "method_details": "A2 + reg", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-01", "metrics": {"Top 1 Accuracy": "82.4%", "Top 5 Accuracy": null, "Number of params": "60.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.4, "Top 5 Accuracy": null, "Number of params": 60200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 877343, "title": "ResNet strikes back: An improved training procedure in timm", "url": "/paper/resnet-strikes-back-an-improved-training", "published": "2021-10-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28362, "rank": 363, "method": "ConViT-B", "mlmodel": {}, "method_short": "ConViT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "82.4%", "Top 5 Accuracy": null, "Number of params": "86M", "GFLOPs": "17", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.4, "Top 5 Accuracy": null, "Number of params": 86000000.0, "GFLOPs": 17.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755434, "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases", "url": "/paper/convit-improving-vision-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/convit-improving-vision-transformers-with/review/?hl=28362"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 37857, "rank": 364, "method": "DeiT-B with iRPE-K", "mlmodel": {}, "method_short": "DeiT-B with iRPE-K", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-29", "metrics": {"Top 1 Accuracy": "82.4%", "Top 5 Accuracy": null, "Number of params": "87M", "GFLOPs": "35.368", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.4, "Top 5 Accuracy": null, "Number of params": 87000000.0, "GFLOPs": 35.368, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 843509, "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer", "url": "/paper/rethinking-and-improving-relative-position", "published": "2021-07-29T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-and-improving-relative-position/review/?hl=37857"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 70543, "rank": 365, "method": "Mega", "mlmodel": {}, "method_short": "Mega", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-21", "metrics": {"Top 1 Accuracy": "82.4%", "Top 5 Accuracy": null, "Number of params": "90M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.4, "Top 5 Accuracy": null, "Number of params": 90000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1078900, "title": "Mega: Moving Average Equipped Gated Attention", "url": "/paper/mega-moving-average-equipped-gated-attention", "published": "2022-09-21T00:00:00.000000", "code": true, "review_url": "/paper/mega-moving-average-equipped-gated-attention/review/?hl=70543"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 21465, "rank": 366, "method": "ColorNet", "mlmodel": {}, "method_short": "ColorNet", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-01", "metrics": {"Top 1 Accuracy": "82.35%", "Top 5 Accuracy": "94.78", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.35, "Top 5 Accuracy": 94.78, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 93126, "title": "ColorNet: Investigating the importance of color spaces for image classification", "url": "/paper/colornet-investigating-the-importance-of", "published": "2019-02-01T00:00:00.000000", "code": true, "review_url": "/paper/colornet-investigating-the-importance-of/review/?hl=21465"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11208, "rank": 367, "method": "SCARLET-A4", "mlmodel": {}, "method_short": "SCARLET-A4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-16", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": "96", "Number of params": "27.8M", "GFLOPs": "8.4", "Hardware Burden": "12G", "Operations per network pass": "0.42G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": 96.0, "Number of params": 27800000.0, "GFLOPs": 8.4, "Hardware Burden": 12.0, "Operations per network pass": 0.42, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 150158, "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search", "url": "/paper/scarletnas-bridging-the-gap-between", "published": "2019-08-16T00:00:00.000000", "code": true, "review_url": "/paper/scarletnas-bridging-the-gap-between/review/?hl=11208"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 25002, "rank": 368, "method": "T2T-ViT-24", "mlmodel": {}, "method_short": "T2T-ViT-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-28", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "27.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 27.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740117, "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "url": "/paper/tokens-to-token-vit-training-vision", "published": "2021-01-28T00:00:00.000000", "code": true, "review_url": "/paper/tokens-to-token-vit-training-vision/review/?hl=25002"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 49538, "rank": 369, "method": "ViT-S-48x1", "mlmodel": {}, "method_short": "ViT-S-48x1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-18", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979672, "title": "Three things everyone should know about Vision Transformers", "url": "/paper/three-things-everyone-should-know-about", "published": "2022-03-18T00:00:00.000000", "code": true, "review_url": "/paper/three-things-everyone-should-know-about/review/?hl=49538"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60756, "rank": 370, "method": "MViTv2-T", "mlmodel": {}, "method_short": "MViTv2-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": null, "Number of params": "24M", "GFLOPs": "4.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": null, "Number of params": 24000000.0, "GFLOPs": 4.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924692, "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection", "url": "/paper/improved-multiscale-vision-transformers-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/improved-multiscale-vision-transformers-for/review/?hl=60756"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 53851, "rank": 371, "method": "Sequencer2D-S", "mlmodel": {}, "method_short": "Sequencer2D-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-04", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "8.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 8.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1004136, "title": "Sequencer: Deep LSTM for Image Classification", "url": "/paper/sequencer-deep-lstm-for-image-classification", "published": "2022-05-04T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29108, "rank": 372, "method": "CrossViT-15+", "mlmodel": {}, "method_short": "CrossViT-15+", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-27", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": null, "Number of params": "28.2M", "GFLOPs": "6.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": null, "Number of params": 28200000.0, "GFLOPs": 6.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758441, "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification", "url": "/paper/2103-14899", "published": "2021-03-27T00:00:00.000000", "code": true, "review_url": "/paper/2103-14899/review/?hl=29108"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 37538, "rank": 373, "method": "GLiT-Bases", "mlmodel": {}, "method_short": "GLiT-Bases", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-07", "metrics": {"Top 1 Accuracy": "82.3%", "Top 5 Accuracy": null, "Number of params": "96.1M", "GFLOPs": "17", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.3, "Top 5 Accuracy": null, "Number of params": 96100000.0, "GFLOPs": 17.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 831816, "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer", "url": "/paper/glit-neural-architecture-search-for-global", "published": "2021-07-07T00:00:00.000000", "code": true, "review_url": "/paper/glit-neural-architecture-search-for-global/review/?hl=37538"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 8176, "rank": 374, "method": "ResNeXt-101 32x8d", "mlmodel": {}, "method_short": "ResNeXt-101 32x8d", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-05-02", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": "96.4", "Number of params": "88M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": 96.4, "Number of params": 88000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 4635, "title": "Exploring the Limits of Weakly Supervised Pretraining", "url": "/paper/exploring-the-limits-of-weakly-supervised", "published": "2018-05-02T00:00:00.000000", "code": true, "review_url": "/paper/exploring-the-limits-of-weakly-supervised/review/?hl=8176"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_resnext101_32x8d.ipynb"}]}, {"table_id": 116, "row_id": 24936, "rank": 375, "method": "SENet-152", "mlmodel": {}, "method_short": "SENet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": "95.9%", "Number of params": "66.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": 95.9, "Number of params": 66600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24936"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 75982, "rank": 376, "method": "BossNet-T1", "mlmodel": {}, "method_short": "BossNet-T1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-23", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "15.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 15.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 756376, "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search", "url": "/paper/bossnas-exploring-hybrid-cnn-transformers", "published": "2021-03-23T00:00:00.000000", "code": true, "review_url": "/paper/bossnas-exploring-hybrid-cnn-transformers/review/?hl=75982"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29208, "rank": 377, "method": "CAIT-XXS-36", "mlmodel": {}, "method_short": "CAIT-XXS-36", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "17.3M", "GFLOPs": "14.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 17300000.0, "GFLOPs": 14.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29208"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29251, "rank": 378, "method": "CvT-13-NAS", "mlmodel": {}, "method_short": "CvT-13-NAS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "18M", "GFLOPs": "4.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 18000000.0, "GFLOPs": 4.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758429, "title": "CvT: Introducing Convolutions to Vision Transformers", "url": "/paper/cvt-introducing-convolutions-to-vision", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/cvt-introducing-convolutions-to-vision/review/?hl=29251"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 36767, "rank": 379, "method": "ViTAE-S-Stage", "mlmodel": {}, "method_short": "ViTAE-S-Stage", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "19.2M", "GFLOPs": "12.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 19200000.0, "GFLOPs": 12.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812332, "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "url": "/paper/vitae-vision-transformer-advanced-by", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/vitae-vision-transformer-advanced-by/review/?hl=36767"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 25000, "rank": 380, "method": "T2T-ViTt-19", "mlmodel": {}, "method_short": "T2T-ViTt-19", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-28", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "39.2M", "GFLOPs": "19.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 39200000.0, "GFLOPs": 19.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740117, "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "url": "/paper/tokens-to-token-vit-training-vision", "published": "2021-01-28T00:00:00.000000", "code": true, "review_url": "/paper/tokens-to-token-vit-training-vision/review/?hl=25000"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 39473, "rank": 381, "method": "Evo-LeViT-384*", "mlmodel": {}, "method_short": "Evo-LeViT-384*", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-08-03", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "39.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 39600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 846027, "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer", "url": "/paper/evo-vit-slow-fast-token-evolution-for-dynamic", "published": "2021-08-03T00:00:00.000000", "code": true, "review_url": "/paper/evo-vit-slow-fast-token-evolution-for-dynamic/review/?hl=39473"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 30930, "rank": 382, "method": "Visformer-S", "mlmodel": {}, "method_short": "Visformer-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-26", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "40.2M", "GFLOPs": "4.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 40200000.0, "GFLOPs": 4.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 788363, "title": "Visformer: The Vision-friendly Transformer", "url": "/paper/visformer-the-vision-friendly-transformer", "published": "2021-04-26T00:00:00.000000", "code": true, "review_url": "/paper/visformer-the-vision-friendly-transformer/review/?hl=30930"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 28361, "rank": 383, "method": "ConViT-S+", "mlmodel": {}, "method_short": "ConViT-S+", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "48M", "GFLOPs": "10", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 48000000.0, "GFLOPs": 10.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755434, "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases", "url": "/paper/convit-improving-vision-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/convit-improving-vision-transformers-with/review/?hl=28361"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 45782, "rank": 384, "method": "ConvMixer-1536/20", "mlmodel": {}, "method_short": "ConvMixer-1536/20", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "82.20", "Top 5 Accuracy": null, "Number of params": "51.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 51600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 949389, "title": "Patches Are All You Need?", "url": "/paper/patches-are-all-you-need-1", "published": "2022-01-24T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 179, "name": "Mixer", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28546, "rank": 385, "method": "DeepVit-L", "mlmodel": {}, "method_short": "DeepVit-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-22", "metrics": {"Top 1 Accuracy": "82.2%", "Top 5 Accuracy": null, "Number of params": "55M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.2, "Top 5 Accuracy": null, "Number of params": 55000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755709, "title": "DeepViT: Towards Deeper Vision Transformer", "url": "/paper/deepvit-towards-deeper-vision-transformer", "published": "2021-03-22T00:00:00.000000", "code": true, "review_url": "/paper/deepvit-towards-deeper-vision-transformer/review/?hl=28546"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71931, "rank": 386, "method": "TransBoost-Swin-T", "mlmodel": {}, "method_short": "TransBoost-Swin-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "82.16%", "Top 5 Accuracy": null, "Number of params": "71.71M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.16, "Top 5 Accuracy": null, "Number of params": 71710000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71931"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22070, "rank": 387, "method": "ResNeXt-101, 64x4d, S=2(224px)", "mlmodel": {}, "method_short": "ResNeXt-101, 64x4d, S=2", "method_details": "224px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-30", "metrics": {"Top 1 Accuracy": "82.13%", "Top 5 Accuracy": "95.98%", "Number of params": "88.6M", "GFLOPs": "18.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.13, "Top 5 Accuracy": 95.98, "Number of params": 88600000.0, "GFLOPs": 18.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237979, "title": "Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training", "url": "/paper/splitnet-divide-and-co-training", "published": "2020-11-30T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 77455, "rank": 388, "method": "Pyramid ViG-S", "mlmodel": {}, "method_short": "Pyramid ViG-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-01", "metrics": {"Top 1 Accuracy": "82.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1019895, "title": "Vision GNN: An Image is Worth Graph of Nodes", "url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes", "published": "2022-06-01T00:00:00.000000", "code": true, "review_url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes/review/?hl=77455"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 44960, "rank": 389, "method": "PatchConvNet-S60", "mlmodel": {}, "method_short": "PatchConvNet-S60", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-27", "metrics": {"Top 1 Accuracy": "82.1%", "Top 5 Accuracy": null, "Number of params": "25.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.1, "Top 5 Accuracy": null, "Number of params": 25200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933338, "title": "Augmenting Convolutional networks with attention-based aggregation", "url": "/paper/augmenting-convolutional-networks-with", "published": "2021-12-27T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 167, "name": "PatchConvnet", "color": "#e0edd4"}], "reports": []}, {"table_id": 116, "row_id": 61424, "rank": 390, "method": "ConvNeXt-T", "mlmodel": {}, "method_short": "ConvNeXt-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-10", "metrics": {"Top 1 Accuracy": "82.1%", "Top 5 Accuracy": null, "Number of params": "29M", "GFLOPs": "4.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.1, "Top 5 Accuracy": null, "Number of params": 29000000.0, "GFLOPs": 4.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 942596, "title": "A ConvNet for the 2020s", "url": "/paper/a-convnet-for-the-2020s", "published": "2022-01-10T00:00:00.000000", "code": true, "review_url": "/paper/a-convnet-for-the-2020s/review/?hl=61424"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 28488, "rank": 391, "method": "CeiT-S", "mlmodel": {}, "method_short": "CeiT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-22", "metrics": {"Top 1 Accuracy": "82%", "Top 5 Accuracy": "95.9", "Number of params": null, "GFLOPs": "4.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": 95.9, "Number of params": null, "GFLOPs": 4.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755729, "title": "Incorporating Convolution Designs into Visual Transformers", "url": "/paper/incorporating-convolution-designs-into-visual", "published": "2021-03-22T00:00:00.000000", "code": true, "review_url": "/paper/incorporating-convolution-designs-into-visual/review/?hl=28488"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 30878, "rank": 392, "method": "DIFFQ (\u03bb=1e\u22122)", "mlmodel": {}, "method_short": "DIFFQ ", "method_details": "\u03bb=1e\u22122", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-20", "metrics": {"Top 1 Accuracy": "82.0", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 785665, "title": "Differentiable Model Compression via Pseudo Quantization Noise", "url": "/paper/differentiable-model-compression-via-pseudo", "published": "2021-04-20T00:00:00.000000", "code": true, "review_url": "/paper/differentiable-model-compression-via-pseudo/review/?hl=30878"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 59083, "rank": 393, "method": "GC ViT-XT", "mlmodel": {}, "method_short": "GC ViT-XT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-20", "metrics": {"Top 1 Accuracy": "82.0%", "Top 5 Accuracy": null, "Number of params": "20M", "GFLOPs": "2.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 20000000.0, "GFLOPs": 2.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029616, "title": "Global Context Vision Transformers", "url": "/paper/global-context-vision-transformers", "published": "2022-06-20T00:00:00.000000", "code": true, "review_url": "/paper/global-context-vision-transformers/review/?hl=59083"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 34346, "rank": 394, "method": "Container-Light", "mlmodel": {}, "method_short": "Container-Light", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-02", "metrics": {"Top 1 Accuracy": "82%", "Top 5 Accuracy": null, "Number of params": "20M", "GFLOPs": "3.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 20000000.0, "GFLOPs": 3.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 811008, "title": "Container: Context Aggregation Network", "url": "/paper/container-context-aggregation-network", "published": "2021-06-02T00:00:00.000000", "code": true, "review_url": "/paper/container-context-aggregation-network/review/?hl=34346"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29121, "rank": 395, "method": "ViL-Small", "mlmodel": {}, "method_short": "ViL-Small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "82%", "Top 5 Accuracy": null, "Number of params": "24.6M", "GFLOPs": "4.86", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 24600000.0, "GFLOPs": 4.86, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=29121"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 36911, "rank": 396, "method": "PVTv2-B2", "mlmodel": {}, "method_short": "PVTv2-B2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-25", "metrics": {"Top 1 Accuracy": "82%", "Top 5 Accuracy": null, "Number of params": "25.4M", "GFLOPs": "4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 25400000.0, "GFLOPs": 4.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 825108, "title": "PVT v2: Improved Baselines with Pyramid Vision Transformer", "url": "/paper/pvtv2-improved-baselines-with-pyramid-vision", "published": "2021-06-25T00:00:00.000000", "code": true, "review_url": "/paper/pvtv2-improved-baselines-with-pyramid-vision/review/?hl=36911"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 61466, "rank": 397, "method": "ActiveMLP-T", "mlmodel": {}, "method_short": "ActiveMLP-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-11", "metrics": {"Top 1 Accuracy": "82%", "Top 5 Accuracy": null, "Number of params": "27.2M", "GFLOPs": "4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 27200000.0, "GFLOPs": 4.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 975658, "title": "ActiveMLP: An MLP-like Architecture with Active Token Mixer", "url": "/paper/activemlp-an-mlp-like-architecture-with", "published": "2022-03-11T00:00:00.000000", "code": true, "review_url": "/paper/activemlp-an-mlp-like-architecture-with/review/?hl=61466"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 55675, "rank": 398, "method": "LITv2-S", "mlmodel": {}, "method_short": "LITv2-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "82%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "3.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 3.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016651, "title": "Fast Vision Transformers with HiLo Attention", "url": "/paper/fast-vision-transformers-with-hilo-attention", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/fast-vision-transformers-with-hilo-attention/review/?hl=55675"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71446, "rank": 399, "method": "DAT-T", "mlmodel": {}, "method_short": "DAT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-03", "metrics": {"Top 1 Accuracy": "82.0%", "Top 5 Accuracy": null, "Number of params": "29M", "GFLOPs": "4.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 82.0, "Top 5 Accuracy": null, "Number of params": 29000000.0, "GFLOPs": 4.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 934161, "title": "Vision Transformer with Deformable Attention", "url": "/paper/vision-transformer-with-deformable-attention", "published": "2022-01-03T00:00:00.000000", "code": true, "review_url": "/paper/vision-transformer-with-deformable-attention/review/?hl=71446"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 79154, "rank": 400, "method": "Swin-T (SAMix+DM)", "mlmodel": {}, "method_short": "Swin-T ", "method_details": "SAMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "81.97%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.97, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79154"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 79152, "rank": 401, "method": "Swin-T (AutoMix+DM)", "mlmodel": {}, "method_short": "Swin-T ", "method_details": "AutoMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "81.92%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.92, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79152"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 33620, "rank": 402, "method": "RVT-S*", "mlmodel": {}, "method_short": "RVT-S*", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-17", "metrics": {"Top 1 Accuracy": "81.9%", "Top 5 Accuracy": "95.8", "Number of params": "23.3M", "GFLOPs": "4.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.9, "Top 5 Accuracy": 95.8, "Number of params": 23300000.0, "GFLOPs": 4.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 800173, "title": "Towards Robust Vision Transformer", "url": "/paper/rethinking-the-design-principles-of-robust", "published": "2021-05-17T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-the-design-principles-of-robust/review/?hl=33620"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 44126, "rank": 403, "method": "ResNet-101 (224 res, Fast Knowledge Distillation)", "mlmodel": {}, "method_short": "ResNet-101 ", "method_details": "224 res, Fast Knowledge Distillation", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "81.9%", "Top 5 Accuracy": "95.7", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.9, "Top 5 Accuracy": 95.7, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924618, "title": "A Fast Knowledge Distillation Framework for Visual Recognition", "url": "/paper/a-fast-knowledge-distillation-framework-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/a-fast-knowledge-distillation-framework-for/review/?hl=44126"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29398, "rank": 404, "method": "T2T-ViT-19", "mlmodel": {}, "method_short": "T2T-ViT-19", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-28", "metrics": {"Top 1 Accuracy": "81.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "17.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 17.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740117, "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "url": "/paper/tokens-to-token-vit-training-vision", "published": "2021-01-28T00:00:00.000000", "code": true, "review_url": "/paper/tokens-to-token-vit-training-vision/review/?hl=29398"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29409, "rank": 405, "method": "PiT-S", "mlmodel": {}, "method_short": "PiT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-30", "metrics": {"Top 1 Accuracy": "81.9%", "Top 5 Accuracy": null, "Number of params": "23.5M", "GFLOPs": "2.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.9, "Top 5 Accuracy": null, "Number of params": 23500000.0, "GFLOPs": 2.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 771780, "title": "Rethinking Spatial Dimensions of Vision Transformers", "url": "/paper/rethinking-spatial-dimensions-of-vision", "published": "2021-03-30T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-spatial-dimensions-of-vision/review/?hl=29409"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 39380, "rank": 406, "method": "sMLPNet-T (ImageNet-1k)", "mlmodel": {}, "method_short": "sMLPNet-T ", "method_details": "ImageNet-1k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-12", "metrics": {"Top 1 Accuracy": "81.9%", "Top 5 Accuracy": null, "Number of params": "24.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.9, "Top 5 Accuracy": null, "Number of params": 24100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 865635, "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?", "url": "/paper/sparse-mlp-for-image-recognition-is-self", "published": "2021-09-12T00:00:00.000000", "code": true, "review_url": "/paper/sparse-mlp-for-image-recognition-is-self/review/?hl=39380"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 29124, "rank": 407, "method": "ViL-Base-W", "mlmodel": {}, "method_short": "ViL-Base-W", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "81.9%", "Top 5 Accuracy": null, "Number of params": "79M", "GFLOPs": "6.74", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.9, "Top 5 Accuracy": null, "Number of params": 79000000.0, "GFLOPs": 6.74, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=29124"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 11209, "rank": 408, "method": "AOGNet-40M-AN", "mlmodel": {}, "method_short": "AOGNet-40M-AN", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-04", "metrics": {"Top 1 Accuracy": "81.87%", "Top 5 Accuracy": "95.74", "Number of params": null, "GFLOPs": "7.51", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.87, "Top 5 Accuracy": 95.74, "Number of params": null, "GFLOPs": 7.51, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 149150, "title": "Attentive Normalization", "url": "/paper/attentive-normalization", "published": "2019-08-04T00:00:00.000000", "code": true, "review_url": "/paper/attentive-normalization/review/?hl=11209"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 43136, "rank": 409, "method": "FBNetV5", "mlmodel": {}, "method_short": "FBNetV5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-19", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.726", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.726, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912986, "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run", "url": "/paper/fbnetv5-neural-architecture-search-for", "published": "2021-11-19T00:00:00.000000", "code": false, "review_url": "/paper/fbnetv5-neural-architecture-search-for/review/?hl=43136"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60737, "rank": 410, "method": "NASViT-A5", "mlmodel": {}, "method_short": "NASViT-A5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.757", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.757, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 37571, "rank": 411, "method": "ResNet-200", "mlmodel": {}, "method_short": "ResNet-200", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-26", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841133, "title": "Parametric Contrastive Learning", "url": "/paper/parametric-contrastive-learning", "published": "2021-07-26T00:00:00.000000", "code": true, "review_url": "/paper/parametric-contrastive-learning/review/?hl=37571"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 44842, "rank": 412, "method": "RepMLPNet-L256", "mlmodel": {}, "method_short": "RepMLPNet-L256", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-21", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 932764, "title": "RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality", "url": "/paper/repmlpnet-hierarchical-vision-mlp-with-re", "published": "2021-12-21T00:00:00.000000", "code": true, "review_url": "/paper/repmlpnet-hierarchical-vision-mlp-with-re/review/?hl=44842"}, "external_source_url": null, "tags": [{"id": 18, "name": "MLP", "color": "#ffae00"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 52489, "rank": 413, "method": "NAT-Mini", "mlmodel": {}, "method_short": "NAT-Mini", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": "20M", "GFLOPs": "2.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": 20000000.0, "GFLOPs": 2.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994419, "title": "Neighborhood Attention Transformer", "url": "/paper/neighborhood-attention-transformer", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/neighborhood-attention-transformer/review/?hl=52489"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}], "reports": []}, {"table_id": 116, "row_id": 70625, "rank": 414, "method": "DiNAT-Mini", "mlmodel": {}, "method_short": "DiNAT-Mini", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-29", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": "20M", "GFLOPs": "2.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": 20000000.0, "GFLOPs": 2.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1082646, "title": "Dilated Neighborhood Attention Transformer", "url": "/paper/dilated-neighborhood-attention-transformer", "published": "2022-09-29T00:00:00.000000", "code": true, "review_url": "/paper/dilated-neighborhood-attention-transformer/review/?hl=70625"}, "external_source_url": null, "tags": [{"id": 265, "name": "Neighborhood Attention", "color": "#ba1330"}, {"id": 266, "name": "NAT Transformer", "color": "#574896"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 40189, "rank": 415, "method": "ResNet-152 (A2)", "mlmodel": {}, "method_short": "ResNet-152 ", "method_details": "A2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-01", "metrics": {"Top 1 Accuracy": "81.8%", "Top 5 Accuracy": null, "Number of params": "60.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.8, "Top 5 Accuracy": null, "Number of params": 60200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 877343, "title": "ResNet strikes back: An improved training procedure in timm", "url": "/paper/resnet-strikes-back-an-improved-training", "published": "2021-10-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 57973, "rank": 416, "method": "MEAL V2 (ResNet-50) (380 res)", "mlmodel": {}, "method_short": "MEAL V2 ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-09-17", "metrics": {"Top 1 Accuracy": "81.72%", "Top 5 Accuracy": "95.81%", "Number of params": "25.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.72, "Top 5 Accuracy": 95.81, "Number of params": 25600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 218270, "title": "MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks", "url": "/paper/meal-v2-boosting-vanilla-resnet-50-to-80-top", "published": "2020-09-17T00:00:00.000000", "code": true, "review_url": "/paper/meal-v2-boosting-vanilla-resnet-50-to-80-top/review/?hl=57973"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 66004, "rank": 417, "method": "gSwin-T", "mlmodel": {}, "method_short": "gSwin-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-08-24", "metrics": {"Top 1 Accuracy": "81.71%", "Top 5 Accuracy": "95.80%", "Number of params": "21.8M", "GFLOPs": "3.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.71, "Top 5 Accuracy": 95.8, "Number of params": 21800000.0, "GFLOPs": 3.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1064636, "title": "gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window", "url": "/paper/gswin-gated-mlp-vision-model-with", "published": "2022-08-24T00:00:00.000000", "code": false, "review_url": "/paper/gswin-gated-mlp-vision-model-with/review/?hl=66004"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 48, "name": "Swin-Transformer", "color": "#f75c2f"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 24934, "rank": 418, "method": "BoTNet T3", "mlmodel": {}, "method_short": "BoTNet T3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "81.7%", "Top 5 Accuracy": "95.8%", "Number of params": "33.5M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.7, "Top 5 Accuracy": 95.8, "Number of params": 33500000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24934"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 37554, "rank": 419, "method": "AutoFormer-small", "mlmodel": {}, "method_short": "AutoFormer-small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-01", "metrics": {"Top 1 Accuracy": "81.7%", "Top 5 Accuracy": "95.7", "Number of params": "22.9M", "GFLOPs": "5.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.7, "Top 5 Accuracy": 95.7, "Number of params": 22900000.0, "GFLOPs": 5.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 828740, "title": "AutoFormer: Searching Transformers for Visual Recognition", "url": "/paper/autoformer-searching-transformers-for-visual", "published": "2021-07-01T00:00:00.000000", "code": true, "review_url": "/paper/autoformer-searching-transformers-for-visual/review/?hl=37554"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60746, "rank": 420, "method": "FBNetV5-A-CLS", "mlmodel": {}, "method_short": "FBNetV5-A-CLS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-19", "metrics": {"Top 1 Accuracy": "81.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.685", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.685, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912986, "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run", "url": "/paper/fbnetv5-neural-architecture-search-for", "published": "2021-11-19T00:00:00.000000", "code": false, "review_url": "/paper/fbnetv5-neural-architecture-search-for/review/?hl=60746"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 31942, "rank": 421, "method": "T2T-ViT-14", "mlmodel": {}, "method_short": "T2T-ViT-14", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-05", "metrics": {"Top 1 Accuracy": "81.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 794722, "title": "Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks", "url": "/paper/beyond-self-attention-external-attention", "published": "2021-05-05T00:00:00.000000", "code": true, "review_url": "/paper/beyond-self-attention-external-attention/review/?hl=31942"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 52667, "rank": 422, "method": "QnA-ViT-Tiny", "mlmodel": {}, "method_short": "QnA-ViT-Tiny", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-21", "metrics": {"Top 1 Accuracy": "81.7%", "Top 5 Accuracy": null, "Number of params": "16M", "GFLOPs": "2.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.7, "Top 5 Accuracy": null, "Number of params": 16000000.0, "GFLOPs": 2.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 933167, "title": "Learned Queries for Efficient Local Attention", "url": "/paper/learned-queries-for-efficient-local-attention", "published": "2021-12-21T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 46840, "rank": 423, "method": "Shift-T", "mlmodel": {}, "method_short": "Shift-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-01-26", "metrics": {"Top 1 Accuracy": "81.7%", "Top 5 Accuracy": null, "Number of params": "28M", "GFLOPs": "4.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.7, "Top 5 Accuracy": null, "Number of params": 28000000.0, "GFLOPs": 4.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 951213, "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism", "url": "/paper/when-shift-operation-meets-vision-transformer", "published": "2022-01-26T00:00:00.000000", "code": true, "review_url": "/paper/when-shift-operation-meets-vision-transformer/review/?hl=46840"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 63785, "rank": 424, "method": "SE-CoTNetD-50", "mlmodel": {}, "method_short": "SE-CoTNetD-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-26", "metrics": {"Top 1 Accuracy": "81.6%", "Top 5 Accuracy": "95.8%", "Number of params": "23.1M", "GFLOPs": "4.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.6, "Top 5 Accuracy": 95.8, "Number of params": 23100000.0, "GFLOPs": 4.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841137, "title": "Contextual Transformer Networks for Visual Recognition", "url": "/paper/contextual-transformer-networks-for-visual", "published": "2021-07-26T00:00:00.000000", "code": true, "review_url": "/paper/contextual-transformer-networks-for-visual/review/?hl=63785"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 49041, "rank": 425, "method": "ReXNet_2.0", "mlmodel": {}, "method_short": "ReXNet_2.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "81.6%", "Top 5 Accuracy": "95.7", "Number of params": "19M", "GFLOPs": "1.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.6, "Top 5 Accuracy": 95.7, "Number of params": 19000000.0, "GFLOPs": 1.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49041"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 21312, "rank": 426, "method": "ResNet-152 (SAM)", "mlmodel": {}, "method_short": "ResNet-152 ", "method_details": "SAM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-03", "metrics": {"Top 1 Accuracy": "81.6%", "Top 5 Accuracy": "95.65", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.6, "Top 5 Accuracy": 95.65, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 225198, "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization", "url": "/paper/sharpness-aware-minimization-for-efficiently-1", "published": "2020-10-03T00:00:00.000000", "code": true, "review_url": "/paper/sharpness-aware-minimization-for-efficiently-1/review/?hl=21312"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29247, "rank": 427, "method": "CvT-13", "mlmodel": {}, "method_short": "CvT-13", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "81.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "4.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 4.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758429, "title": "CvT: Introducing Convolutions to Vision Transformers", "url": "/paper/cvt-introducing-convolutions-to-vision", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/cvt-introducing-convolutions-to-vision/review/?hl=29247"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29470, "rank": 428, "method": "LeViT-256", "mlmodel": {}, "method_short": "LeViT-256", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-02", "metrics": {"Top 1 Accuracy": "81.6%", "Top 5 Accuracy": null, "Number of params": "17.8M", "GFLOPs": "1.066", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.6, "Top 5 Accuracy": null, "Number of params": 17800000.0, "GFLOPs": 1.066, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 774248, "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference", "url": "/paper/levit-a-vision-transformer-in-convnet-s", "published": "2021-04-02T00:00:00.000000", "code": true, "review_url": "/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29470"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 33111, "rank": 429, "method": "gMLP-B", "mlmodel": {}, "method_short": "gMLP-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-17", "metrics": {"Top 1 Accuracy": "81.6%", "Top 5 Accuracy": null, "Number of params": "73M", "GFLOPs": "31.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.6, "Top 5 Accuracy": null, "Number of params": 73000000.0, "GFLOPs": 31.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 800166, "title": "Pay Attention to MLPs", "url": "/paper/pay-attention-to-mlps", "published": "2021-05-17T00:00:00.000000", "code": true, "review_url": "/paper/pay-attention-to-mlps/review/?hl=33111"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 11211, "rank": 430, "method": "NoisyStudent (EfficientNet-B1)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B1", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "81.5%", "Top 5 Accuracy": "95.8%", "Number of params": "7.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.5, "Top 5 Accuracy": 95.8, "Number of params": 7800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11211"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 60261, "rank": 431, "method": "TinyViT-11M", "mlmodel": {}, "method_short": "TinyViT-11M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "81.5%", "Top 5 Accuracy": "95.8%", "Number of params": "11M", "GFLOPs": "2.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.5, "Top 5 Accuracy": 95.8, "Number of params": 11000000.0, "GFLOPs": 2.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60261"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 38562, "rank": 432, "method": "CoE-Large + CondConv", "mlmodel": {}, "method_short": "CoE-Large + CondConv", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-08", "metrics": {"Top 1 Accuracy": "81.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.214", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.214, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 832426, "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs", "url": "/paper/collaboration-of-experts-achieving-80-top-1", "published": "2021-07-08T00:00:00.000000", "code": false, "review_url": "/paper/collaboration-of-experts-achieving-80-top-1/review/?hl=38562"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 33773, "rank": 433, "method": "Transformer local-attention (NesT-T)", "mlmodel": {}, "method_short": "Transformer local-attention ", "method_details": "NesT-T", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-26", "metrics": {"Top 1 Accuracy": "81.5%", "Top 5 Accuracy": null, "Number of params": "17M", "GFLOPs": "5.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.5, "Top 5 Accuracy": null, "Number of params": 17000000.0, "GFLOPs": 5.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 805906, "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding", "url": "/paper/aggregating-nested-transformers", "published": "2021-05-26T00:00:00.000000", "code": true, "review_url": "/paper/aggregating-nested-transformers/review/?hl=33773"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 24998, "rank": 434, "method": "T2T-ViT-14", "mlmodel": {}, "method_short": "T2T-ViT-14", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-28", "metrics": {"Top 1 Accuracy": "81.5%", "Top 5 Accuracy": null, "Number of params": "21.5M", "GFLOPs": "9.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.5, "Top 5 Accuracy": null, "Number of params": 21500000.0, "GFLOPs": 9.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740117, "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "url": "/paper/tokens-to-token-vit-training-vision", "published": "2021-01-28T00:00:00.000000", "code": true, "review_url": "/paper/tokens-to-token-vit-training-vision/review/?hl=24998"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 29107, "rank": 435, "method": "CrossViT-15", "mlmodel": {}, "method_short": "CrossViT-15", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-27", "metrics": {"Top 1 Accuracy": "81.5%", "Top 5 Accuracy": null, "Number of params": "27.4M", "GFLOPs": "5.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.5, "Top 5 Accuracy": null, "Number of params": 27400000.0, "GFLOPs": 5.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758441, "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification", "url": "/paper/2103-14899", "published": "2021-03-27T00:00:00.000000", "code": true, "review_url": "/paper/2103-14899/review/?hl=29107"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 17490, "rank": 436, "method": "PyConvResNet-101", "mlmodel": {}, "method_short": "PyConvResNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-20", "metrics": {"Top 1 Accuracy": "81.49%", "Top 5 Accuracy": "95.72%", "Number of params": "42.3M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.49, "Top 5 Accuracy": 95.72, "Number of params": 42300000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 205105, "title": "Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition", "url": "/paper/pyramidal-convolution-rethinking", "published": "2020-06-20T00:00:00.000000", "code": true, "review_url": "/paper/pyramidal-convolution-rethinking/review/?hl=17490"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24935, "rank": 437, "method": "SENet-101", "mlmodel": {}, "method_short": "SENet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "81.4%", "Top 5 Accuracy": "95.7%", "Number of params": "49.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.4, "Top 5 Accuracy": 95.7, "Number of params": 49200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24935"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 37856, "rank": 438, "method": "DeiT-S with iRPE-QKV", "mlmodel": {}, "method_short": "DeiT-S with iRPE-QKV", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-29", "metrics": {"Top 1 Accuracy": "81.4%", "Top 5 Accuracy": "95.6", "Number of params": null, "GFLOPs": "9.770", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.4, "Top 5 Accuracy": 95.6, "Number of params": null, "GFLOPs": 9.77, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 843509, "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer", "url": "/paper/rethinking-and-improving-relative-position", "published": "2021-07-29T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-and-improving-relative-position/review/?hl=37856"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60736, "rank": 439, "method": "NASViT-A4", "mlmodel": {}, "method_short": "NASViT-A4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "81.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.591", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.591, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 52536, "rank": 440, "method": "ViT-S @224 (DeiT III)", "mlmodel": {}, "method_short": "ViT-S @224 ", "method_details": "DeiT III", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-04-14", "metrics": {"Top 1 Accuracy": "81.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 994466, "title": "DeiT III: Revenge of the ViT", "url": "/paper/deit-iii-revenge-of-the-vit", "published": "2022-04-14T00:00:00.000000", "code": true, "review_url": "/paper/deit-iii-revenge-of-the-vit/review/?hl=52536"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 61472, "rank": 441, "method": "CCT-14/7x2", "mlmodel": {}, "method_short": "CCT-14/7x2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "81.34%", "Top 5 Accuracy": null, "Number of params": "22.36M", "GFLOPs": "11.06", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.34, "Top 5 Accuracy": null, "Number of params": 22360000.0, "GFLOPs": 11.06, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778781, "title": "Escaping the Big Data Paradigm with Compact Transformers", "url": "/paper/escaping-the-big-data-paradigm-with-compact", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=61472"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22144, "rank": 442, "method": "ResNet-200 (Adversarial Autoaugment)", "mlmodel": {}, "method_short": "ResNet-200 ", "method_details": "Adversarial Autoaugment", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-24", "metrics": {"Top 1 Accuracy": "81.32%", "Top 5 Accuracy": "95.30%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.32, "Top 5 Accuracy": 95.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 178148, "title": "Adversarial AutoAugment", "url": "/paper/adversarial-autoaugment-1", "published": "2019-12-24T00:00:00.000000", "code": false, "review_url": "/paper/adversarial-autoaugment-1/review/?hl=22144"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 37508, "rank": 443, "method": "ResNet-152", "mlmodel": {}, "method_short": "ResNet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-26", "metrics": {"Top 1 Accuracy": "81.3%", "Top 5 Accuracy": "95.4", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.3, "Top 5 Accuracy": 95.4, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841133, "title": "Parametric Contrastive Learning", "url": "/paper/parametric-contrastive-learning", "published": "2021-07-26T00:00:00.000000", "code": true, "review_url": "/paper/parametric-contrastive-learning/review/?hl=37508"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_resnet152.ipynb"}]}, {"table_id": 116, "row_id": 11212, "rank": 444, "method": "MultiGrain PNASNet (300px)", "mlmodel": {}, "method_short": "MultiGrain PNASNet ", "method_details": "300px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "81.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=11212"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28360, "rank": 445, "method": "ConViT-S", "mlmodel": {}, "method_short": "ConViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "81.3%", "Top 5 Accuracy": null, "Number of params": "27M", "GFLOPs": "5.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.3, "Top 5 Accuracy": null, "Number of params": 27000000.0, "GFLOPs": 5.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755434, "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases", "url": "/paper/convit-improving-vision-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/convit-improving-vision-transformers-with/review/?hl=28360"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60638, "rank": 446, "method": "Swin-T", "mlmodel": {}, "method_short": "Swin-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-25", "metrics": {"Top 1 Accuracy": "81.3%", "Top 5 Accuracy": null, "Number of params": "29M", "GFLOPs": "4.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.3, "Top 5 Accuracy": null, "Number of params": 29000000.0, "GFLOPs": 4.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 757245, "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "url": "/paper/swin-transformer-hierarchical-vision", "published": "2021-03-25T00:00:00.000000", "code": true, "review_url": "/paper/swin-transformer-hierarchical-vision/review/?hl=60638"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10252, "rank": 447, "method": "Res2Net-101", "mlmodel": {}, "method_short": "Res2Net-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-02", "metrics": {"Top 1 Accuracy": "81.23%", "Top 5 Accuracy": "94.43%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.23, "Top 5 Accuracy": 94.43, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 110193, "title": "Res2Net: A New Multi-scale Backbone Architecture", "url": "/paper/res2net-a-new-multi-scale-backbone", "published": "2019-04-02T00:00:00.000000", "code": true, "review_url": "/paper/res2net-a-new-multi-scale-backbone/review/?hl=10252"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 20874, "rank": 448, "method": "ResNeXt-101 (Debiased+CutMix)", "mlmodel": {}, "method_short": "ResNeXt-101 ", "method_details": "Debiased+CutMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-04", "metrics": {"Top 1 Accuracy": "81.2", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 227347, "title": "Shape-Texture Debiased Neural Network Training", "url": "/paper/shape-texture-debiased-neural-network-1", "published": "2020-10-12T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 79150, "rank": 449, "method": "Swin-T (PuzzleMix+DM)", "mlmodel": {}, "method_short": "Swin-T ", "method_details": "PuzzleMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "81.16%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.16, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79150"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71928, "rank": 450, "method": "TransBoost-ResNet50-StrikesBack", "mlmodel": {}, "method_short": "TransBoost-ResNet50-StrikesBack", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "81.15%", "Top 5 Accuracy": null, "Number of params": "25.56M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.15, "Top 5 Accuracy": null, "Number of params": 25560000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71928"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60624, "rank": 451, "method": "ResNeSt-50", "mlmodel": {}, "method_short": "ResNeSt-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-04-19", "metrics": {"Top 1 Accuracy": "81.13%", "Top 5 Accuracy": null, "Number of params": "27.5M", "GFLOPs": "5.39", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.13, "Top 5 Accuracy": null, "Number of params": 27500000.0, "GFLOPs": 5.39, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 191448, "title": "ResNeSt: Split-Attention Networks", "url": "/paper/resnest-split-attention-networks", "published": "2020-04-19T00:00:00.000000", "code": true, "review_url": "/paper/resnest-split-attention-networks/review/?hl=60624"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 79153, "rank": 452, "method": "DeiT-S (SAMix+DM)", "mlmodel": {}, "method_short": "DeiT-S ", "method_details": "SAMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "81.12%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.12, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79153"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6184, "rank": 453, "method": "EfficientNet-B3", "mlmodel": {}, "method_short": "EfficientNet-B3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "81.1%", "Top 5 Accuracy": "95.5", "Number of params": "12M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.1, "Top 5 Accuracy": 95.5, "Number of params": 12000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6184"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 37855, "rank": 454, "method": "DeiT-S with iRPE-QK", "mlmodel": {}, "method_short": "DeiT-S with iRPE-QK", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-29", "metrics": {"Top 1 Accuracy": "81.1%", "Top 5 Accuracy": "95.4", "Number of params": null, "GFLOPs": "9.412", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.1, "Top 5 Accuracy": 95.4, "Number of params": null, "GFLOPs": 9.412, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 843509, "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer", "url": "/paper/rethinking-and-improving-relative-position", "published": "2021-07-29T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-and-improving-relative-position/review/?hl=37855"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 61462, "rank": 455, "method": "VAN-B1", "mlmodel": {}, "method_short": "VAN-B1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "81.1%", "Top 5 Accuracy": null, "Number of params": "13.9M", "GFLOPs": "2.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.1, "Top 5 Accuracy": null, "Number of params": 13900000.0, "GFLOPs": 2.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=61462"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 64203, "rank": 456, "method": "RevBiFPN-S3", "mlmodel": {}, "method_short": "RevBiFPN-S3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "81.1%", "Top 5 Accuracy": null, "Number of params": "19.6M", "GFLOPs": "3.33", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.1, "Top 5 Accuracy": null, "Number of params": 19600000.0, "GFLOPs": 3.33, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64203"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 34305, "rank": 457, "method": "ResNet-152x2-SAM", "mlmodel": {}, "method_short": "ResNet-152x2-SAM", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-03", "metrics": {"Top 1 Accuracy": "81.1%", "Top 5 Accuracy": null, "Number of params": "236M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.1, "Top 5 Accuracy": null, "Number of params": 236000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 810994, "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations", "url": "/paper/when-vision-transformers-outperform-resnets", "published": "2021-06-03T00:00:00.000000", "code": true, "review_url": "/paper/when-vision-transformers-outperform-resnets/review/?hl=34305"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 45001, "rank": 458, "method": "ResNet-101 (SAMix)", "mlmodel": {}, "method_short": "ResNet-101 ", "method_details": "SAMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-30", "metrics": {"Top 1 Accuracy": "81.08%", "Top 5 Accuracy": "95.54%", "Number of params": "44.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.08, "Top 5 Accuracy": 95.54, "Number of params": 44600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 923048, "title": "Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup", "url": "/paper/boosting-discriminative-visual-representation", "published": "2021-11-30T00:00:00.000000", "code": true, "review_url": "/paper/boosting-discriminative-visual-representation/review/?hl=45001"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 60735, "rank": 459, "method": "NASViT-A3", "mlmodel": {}, "method_short": "NASViT-A3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "81.0%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.528", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.528, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 35599, "rank": 460, "method": "ViTAE-13M", "mlmodel": {}, "method_short": "ViTAE-13M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "81%", "Top 5 Accuracy": null, "Number of params": "13.2M", "GFLOPs": "6.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 81.0, "Top 5 Accuracy": null, "Number of params": 13200000.0, "GFLOPs": 6.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812332, "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "url": "/paper/vitae-vision-transformer-advanced-by", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/vitae-vision-transformer-advanced-by/review/?hl=35599"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 44995, "rank": 461, "method": "ResNet-101 (AutoMix)", "mlmodel": {}, "method_short": "ResNet-101 ", "method_details": "AutoMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-24", "metrics": {"Top 1 Accuracy": "80.98%", "Top 5 Accuracy": null, "Number of params": "44.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": "44.6M"}, "raw_metrics": {"Top 1 Accuracy": 80.98, "Top 5 Accuracy": null, "Number of params": 44600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": 44600000.0}, "uses_additional_data": false, "paper": {"id": 756879, "title": "AutoMix: Unveiling the Power of Mixup for Stronger Classifiers", "url": "/paper/automix-unveiling-the-power-of-mixup", "published": "2021-03-24T00:00:00.000000", "code": true, "review_url": "/paper/automix-unveiling-the-power-of-mixup/review/?hl=44995"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 79151, "rank": 462, "method": "DeiT-S (AutoMix+DM)", "mlmodel": {}, "method_short": "DeiT-S ", "method_details": "AutoMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "80.91%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.91, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79151"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 2053, "rank": 463, "method": "ResNeXt-101  64x4", "mlmodel": {}, "method_short": "ResNeXt-101  64x4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-11-16", "metrics": {"Top 1 Accuracy": "80.9%", "Top 5 Accuracy": "95.6%", "Number of params": "83.6M", "GFLOPs": "31.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.9, "Top 5 Accuracy": 95.6, "Number of params": 83600000.0, "GFLOPs": 31.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 24264, "title": "Aggregated Residual Transformations for Deep Neural Networks", "url": "/paper/aggregated-residual-transformations-for-deep", "published": "2016-11-16T00:00:00.000000", "code": true, "review_url": "/paper/aggregated-residual-transformations-for-deep/review/?hl=2053"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 37509, "rank": 464, "method": "ResNet-101", "mlmodel": {}, "method_short": "ResNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-26", "metrics": {"Top 1 Accuracy": "80.9%", "Top 5 Accuracy": "95.2", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.9, "Top 5 Accuracy": 95.2, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841133, "title": "Parametric Contrastive Learning", "url": "/paper/parametric-contrastive-learning", "published": "2021-07-26T00:00:00.000000", "code": true, "review_url": "/paper/parametric-contrastive-learning/review/?hl=37509"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29209, "rank": 465, "method": "CAIT-XXS-24", "mlmodel": {}, "method_short": "CAIT-XXS-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-31", "metrics": {"Top 1 Accuracy": "80.9%", "Top 5 Accuracy": null, "Number of params": "12M", "GFLOPs": "9.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.9, "Top 5 Accuracy": null, "Number of params": 12000000.0, "GFLOPs": 9.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 772635, "title": "Going deeper with Image Transformers", "url": "/paper/going-deeper-with-image-transformers", "published": "2021-03-31T00:00:00.000000", "code": true, "review_url": "/paper/going-deeper-with-image-transformers/review/?hl=29209"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 37854, "rank": 466, "method": "DeiT-S with iRPE-K", "mlmodel": {}, "method_short": "DeiT-S with iRPE-K", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-29", "metrics": {"Top 1 Accuracy": "80.9%", "Top 5 Accuracy": null, "Number of params": "22M", "GFLOPs": "9.318", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.9, "Top 5 Accuracy": null, "Number of params": 22000000.0, "GFLOPs": 9.318, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 843509, "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer", "url": "/paper/rethinking-and-improving-relative-position", "published": "2021-07-29T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-and-improving-relative-position/review/?hl=37854"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 61473, "rank": 467, "method": "CentroidViT-S (arXiv, 2021-02)", "mlmodel": {}, "method_short": "CentroidViT-S ", "method_details": "arXiv, 2021-02", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-17", "metrics": {"Top 1 Accuracy": "80.9%", "Top 5 Accuracy": null, "Number of params": "22.3M", "GFLOPs": "9.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.9, "Top 5 Accuracy": null, "Number of params": 22300000.0, "GFLOPs": 9.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 746312, "title": "Centroid Transformers: Learning to Abstract with Attention", "url": "/paper/centroid-transformers-learning-to-abstract", "published": "2021-02-17T00:00:00.000000", "code": false, "review_url": "/paper/centroid-transformers-learning-to-abstract/review/?hl=61473"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 15976, "rank": 468, "method": "ResNet-200 (Supervised Contrastive)", "mlmodel": {}, "method_short": "ResNet-200 ", "method_details": "Supervised Contrastive", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-04-23", "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": "95.6%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": 95.6, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 191948, "title": "Supervised Contrastive Learning", "url": "/paper/supervised-contrastive-learning", "published": "2020-04-23T00:00:00.000000", "code": true, "review_url": "/paper/supervised-contrastive-learning/review/?hl=15976"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29951, "rank": 469, "method": "LocalViT-S", "mlmodel": {}, "method_short": "LocalViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": "95.4", "Number of params": "22.4M", "GFLOPs": "4.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": 95.4, "Number of params": 22400000.0, "GFLOPs": 4.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778779, "title": "LocalViT: Bringing Locality to Vision Transformers", "url": "/paper/localvit-bringing-locality-to-vision", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/localvit-bringing-locality-to-vision/review/?hl=29951"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 57967, "rank": 470, "method": "AlphaNet-A6", "mlmodel": {}, "method_short": "AlphaNet-A6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.709", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.709, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57967"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60743, "rank": 471, "method": "UniNet-B0", "mlmodel": {}, "method_short": "UniNet-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-12", "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": null, "Number of params": "11.5M", "GFLOPs": "0.555", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": null, "Number of params": 11500000.0, "GFLOPs": 0.555, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1042502, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with-1", "published": "2022-07-12T00:00:00.000000", "code": true, "review_url": "/paper/uninet-unified-architecture-search-with-1/review/?hl=60743"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 34941, "rank": 472, "method": "ResMLP-S24", "mlmodel": {}, "method_short": "ResMLP-S24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": null, "Number of params": "30M", "GFLOPs": "6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": null, "Number of params": 30000000.0, "GFLOPs": 6.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795412, "title": "ResMLP: Feedforward networks for image classification with data-efficient training", "url": "/paper/resmlp-feedforward-networks-for-image", "published": "2021-05-07T00:00:00.000000", "code": true, "review_url": "/paper/resmlp-feedforward-networks-for-image/review/?hl=34941"}, "external_source_url": null, "tags": [{"id": 18, "name": "MLP", "color": "#ffae00"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60268, "rank": 473, "method": "TinyViT-5M-distill (21k)", "mlmodel": {}, "method_short": "TinyViT-5M-distill ", "method_details": "21k", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "80.7%", "Top 5 Accuracy": "95.6%", "Number of params": "5.4M", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.7, "Top 5 Accuracy": 95.6, "Number of params": 5400000.0, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60268"}, "external_source_url": null, "tags": [{"id": 105, "name": "ImageNet-22k", "color": "#bc51bd"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 38561, "rank": 474, "method": "CoE-Large", "mlmodel": {}, "method_short": "CoE-Large", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-08", "metrics": {"Top 1 Accuracy": "80.7%", "Top 5 Accuracy": null, "Number of params": "95.3M", "GFLOPs": "0.194", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.7, "Top 5 Accuracy": null, "Number of params": 95300000.0, "GFLOPs": 0.194, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 832426, "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs", "url": "/paper/collaboration-of-experts-achieving-80-top-1", "published": "2021-07-08T00:00:00.000000", "code": false, "review_url": "/paper/collaboration-of-experts-achieving-80-top-1/review/?hl=38561"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 57972, "rank": 475, "method": "MEAL V2 (ResNet-50) (224 res)", "mlmodel": {}, "method_short": "MEAL V2 ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-09-17", "metrics": {"Top 1 Accuracy": "80.67%", "Top 5 Accuracy": "95.09%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.67, "Top 5 Accuracy": 95.09, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 218270, "title": "MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks", "url": "/paper/meal-v2-boosting-vanilla-resnet-50-to-80-top", "published": "2020-09-17T00:00:00.000000", "code": true, "review_url": "/paper/meal-v2-boosting-vanilla-resnet-50-to-80-top/review/?hl=57972"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60625, "rank": 476, "method": "ResNeSt-50-fast", "mlmodel": {}, "method_short": "ResNeSt-50-fast", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-04-19", "metrics": {"Top 1 Accuracy": "80.64%", "Top 5 Accuracy": null, "Number of params": "27.5M", "GFLOPs": "4.34", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.64, "Top 5 Accuracy": null, "Number of params": 27500000.0, "GFLOPs": 4.34, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 191448, "title": "ResNeSt: Split-Attention Networks", "url": "/paper/resnest-split-attention-networks", "published": "2020-04-19T00:00:00.000000", "code": true, "review_url": "/paper/resnest-split-attention-networks/review/?hl=60625"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71925, "rank": 477, "method": "TransBoost-ResNet152", "mlmodel": {}, "method_short": "TransBoost-ResNet152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "80.64%", "Top 5 Accuracy": null, "Number of params": "60.19M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.64, "Top 5 Accuracy": null, "Number of params": 60190000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71925"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 9313, "rank": 478, "method": "ResNet-200 (Fast AA)", "mlmodel": {}, "method_short": "ResNet-200 ", "method_details": "Fast AA", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-01", "metrics": {"Top 1 Accuracy": "80.6%", "Top 5 Accuracy": "95.3%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.6, "Top 5 Accuracy": 95.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 113336, "title": "Fast AutoAugment", "url": "/paper/fast-autoaugment", "published": "2019-05-01T00:00:00.000000", "code": true, "review_url": "/paper/fast-autoaugment/review/?hl=9313"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 11210, "rank": 479, "method": "ResNeXt-101 (CutMix)", "mlmodel": {}, "method_short": "ResNeXt-101 ", "method_details": "CutMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-13", "metrics": {"Top 1 Accuracy": "80.53%", "Top 5 Accuracy": "94.97%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.53, "Top 5 Accuracy": 94.97, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 114316, "title": "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features", "url": "/paper/cutmix-regularization-strategy-to-train", "published": "2019-05-13T00:00:00.000000", "code": true, "review_url": "/paper/cutmix-regularization-strategy-to-train/review/?hl=11210"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 21729, "rank": 480, "method": "Attention-92", "mlmodel": {}, "method_short": "Attention-92", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2017-04-23", "metrics": {"Top 1 Accuracy": "80.5%", "Top 5 Accuracy": "95.2%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.5, "Top 5 Accuracy": 95.2, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 23736, "title": "Residual Attention Network for Image Classification", "url": "/paper/residual-attention-network-for-image", "published": "2017-04-23T00:00:00.000000", "code": true, "review_url": "/paper/residual-attention-network-for-image/review/?hl=21729"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 16312, "rank": 481, "method": "NAT-M4", "mlmodel": {}, "method_short": "NAT-M4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-05-12", "metrics": {"Top 1 Accuracy": "80.5%", "Top 5 Accuracy": "95.2%", "Number of params": "9.1M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.5, "Top 5 Accuracy": 95.2, "Number of params": 9100000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 194717, "title": "Neural Architecture Transfer", "url": "/paper/neural-architecture-transfer", "published": "2020-05-12T00:00:00.000000", "code": true, "review_url": "/paper/neural-architecture-transfer/review/?hl=16312"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 39448, "rank": 482, "method": "HCGNet-C", "mlmodel": {}, "method_short": "HCGNet-C", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-26", "metrics": {"Top 1 Accuracy": "80.5%", "Top 5 Accuracy": "95.2", "Number of params": "42.2M", "GFLOPs": "7.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.5, "Top 5 Accuracy": 95.2, "Number of params": 42200000.0, "GFLOPs": 7.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 151373, "title": "Gated Convolutional Networks with Hybrid Connectivity for Image Classification", "url": "/paper/gated-convolutional-networks-with-hybrid", "published": "2019-08-26T00:00:00.000000", "code": true, "review_url": "/paper/gated-convolutional-networks-with-hybrid/review/?hl=39448"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60734, "rank": 483, "method": "NASViT-A2", "mlmodel": {}, "method_short": "NASViT-A2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "80.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.421", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.421, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 37537, "rank": 484, "method": "GLiT-Smalls", "mlmodel": {}, "method_short": "GLiT-Smalls", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-07", "metrics": {"Top 1 Accuracy": "80.5%", "Top 5 Accuracy": null, "Number of params": "24.6M", "GFLOPs": "4.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.5, "Top 5 Accuracy": null, "Number of params": 24600000.0, "GFLOPs": 4.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 831816, "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer", "url": "/paper/glit-neural-architecture-search-for-global", "published": "2021-07-07T00:00:00.000000", "code": true, "review_url": "/paper/glit-neural-architecture-search-for-global/review/?hl=37537"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 34135, "rank": 485, "method": "DVT (T2T-ViT-12)", "mlmodel": {}, "method_short": "DVT ", "method_details": "T2T-ViT-12", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-31", "metrics": {"Top 1 Accuracy": "80.43%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "1.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.43, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 1.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 808027, "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition", "url": "/paper/not-all-images-are-worth-16x16-words-dynamic", "published": "2021-05-31T00:00:00.000000", "code": true, "review_url": "/paper/not-all-images-are-worth-16x16-words-dynamic/review/?hl=34135"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 48306, "rank": 486, "method": "UniNet-B1", "mlmodel": {}, "method_short": "UniNet-B1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-08", "metrics": {"Top 1 Accuracy": "80.4%", "Top 5 Accuracy": null, "Number of params": "14M", "GFLOPs": "0.99", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.4, "Top 5 Accuracy": null, "Number of params": 14000000.0, "GFLOPs": 0.99, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 881795, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with", "published": "2021-10-08T00:00:00.000000", "code": false, "review_url": "/paper/uninet-unified-architecture-search-with/review/?hl=48306"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 40187, "rank": 487, "method": "DeiT-S (T2)", "mlmodel": {}, "method_short": "DeiT-S ", "method_details": "T2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-01", "metrics": {"Top 1 Accuracy": "80.4%", "Top 5 Accuracy": null, "Number of params": "22M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.4, "Top 5 Accuracy": null, "Number of params": 22000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 877343, "title": "ResNet strikes back: An improved training procedure in timm", "url": "/paper/resnet-strikes-back-an-improved-training", "published": "2021-10-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 40185, "rank": 488, "method": "ResNet50 (A1)", "mlmodel": {}, "method_short": "ResNet50 ", "method_details": "A1", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-01", "metrics": {"Top 1 Accuracy": "80.4%", "Top 5 Accuracy": null, "Number of params": "25M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.4, "Top 5 Accuracy": null, "Number of params": 25000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 877343, "title": "ResNet strikes back: An improved training procedure in timm", "url": "/paper/resnet-strikes-back-an-improved-training", "published": "2021-10-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 66003, "rank": 489, "method": "gSwin-VT", "mlmodel": {}, "method_short": "gSwin-VT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-08-24", "metrics": {"Top 1 Accuracy": "80.32%", "Top 5 Accuracy": "95.08%", "Number of params": "15.5M", "GFLOPs": "2.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.32, "Top 5 Accuracy": 95.08, "Number of params": 15500000.0, "GFLOPs": 2.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1064636, "title": "gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window", "url": "/paper/gswin-gated-mlp-vision-model-with", "published": "2022-08-24T00:00:00.000000", "code": false, "review_url": "/paper/gswin-gated-mlp-vision-model-with/review/?hl=66003"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 48, "name": "Swin-Transformer", "color": "#f75c2f"}, {"id": 18, "name": "MLP", "color": "#ffae00"}], "reports": []}, {"table_id": 116, "row_id": 49042, "rank": 490, "method": "ReXNet_1.5", "mlmodel": {}, "method_short": "ReXNet_1.5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "80.3%", "Top 5 Accuracy": "95.2", "Number of params": "9.7M", "GFLOPs": "0.86", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.3, "Top 5 Accuracy": 95.2, "Number of params": 9700000.0, "GFLOPs": 0.86, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49042"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 57966, "rank": 491, "method": "AlphaNet-A5", "mlmodel": {}, "method_short": "AlphaNet-A5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "80.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.491", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.491, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57966"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 24596, "rank": 492, "method": "ResNet-50+AutoDropout+RandAugment", "mlmodel": {}, "method_short": "ResNet-50+AutoDropout+RandAugment", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-05", "metrics": {"Top 1 Accuracy": "80.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 734166, "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks", "url": "/paper/autodropout-learning-dropout-patterns-to", "published": "2021-01-05T00:00:00.000000", "code": true, "review_url": "/paper/autodropout-learning-dropout-patterns-to/review/?hl=24596"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 36261, "rank": 493, "method": "CCT-16/7x2", "mlmodel": {}, "method_short": "CCT-16/7x2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "80.28%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.28, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778781, "title": "Escaping the Big Data Paradigm with Compact Transformers", "url": "/paper/escaping-the-big-data-paradigm-with-compact", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/escaping-the-big-data-paradigm-with-compact/review/?hl=36261"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 79149, "rank": 494, "method": "DeiT-S (PuzzleMix+DM)", "mlmodel": {}, "method_short": "DeiT-S ", "method_details": "PuzzleMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "80.25%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.25, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79149"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 20325, "rank": 495, "method": "iAFF-ResNeXt-50-32x4d", "mlmodel": {}, "method_short": "iAFF-ResNeXt-50-32x4d", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-09-29", "metrics": {"Top 1 Accuracy": "80.22%", "Top 5 Accuracy": "94.9%", "Number of params": "34.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.22, "Top 5 Accuracy": 94.9, "Number of params": 34700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 219902, "title": "Attentional Feature Fusion", "url": "/paper/attentional-feature-fusion", "published": "2020-09-29T00:00:00.000000", "code": true, "review_url": "/paper/attentional-feature-fusion/review/?hl=20325"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 10372, "rank": 496, "method": "FixEfficientNet-B0", "mlmodel": {}, "method_short": "FixEfficientNet-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-18", "metrics": {"Top 1 Accuracy": "80.2%", "Top 5 Accuracy": "95.4%", "Number of params": "5.3M", "GFLOPs": "1.60", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.2, "Top 5 Accuracy": 95.4, "Number of params": 5300000.0, "GFLOPs": 1.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 187431, "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "url": "/paper/fixing-the-train-test-resolution-discrepancy-2", "published": "2020-03-18T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy-2/review/?hl=10372"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 39229, "rank": 497, "method": "ConvMLP-L", "mlmodel": {}, "method_short": "ConvMLP-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-09", "metrics": {"Top 1 Accuracy": "80.2%", "Top 5 Accuracy": null, "Number of params": "42.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.2, "Top 5 Accuracy": null, "Number of params": 42700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 864188, "title": "ConvMLP: Hierarchical Convolutional MLPs for Vision", "url": "/paper/convmlp-hierarchical-convolutional-mlps-for", "published": "2021-09-09T00:00:00.000000", "code": true, "review_url": "/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39229"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 18, "name": "MLP", "color": "#ffae00"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 2054, "rank": 498, "method": "Inception ResNet V2", "mlmodel": {}, "method_short": "Inception ResNet V2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-02-23", "metrics": {"Top 1 Accuracy": "80.1%", "Top 5 Accuracy": "95.1%", "Number of params": "55.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.1, "Top 5 Accuracy": 95.1, "Number of params": 55800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 31162, "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "url": "/paper/inception-v4-inception-resnet-and-the-impact", "published": "2016-02-23T00:00:00.000000", "code": true, "review_url": "/paper/inception-v4-inception-resnet-and-the-impact/review/?hl=2054"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 44125, "rank": 499, "method": "ResNet-50 (224 res, Fast Knowledge Distillation)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "224 res, Fast Knowledge Distillation", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "80.1%", "Top 5 Accuracy": "94.8", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.1, "Top 5 Accuracy": 94.8, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924618, "title": "A Fast Knowledge Distillation Framework for Visual Recognition", "url": "/paper/a-fast-knowledge-distillation-framework-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/a-fast-knowledge-distillation-framework-for/review/?hl=44125"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 4576, "rank": 500, "method": "RandWire-WS", "mlmodel": {}, "method_short": "RandWire-WS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-02", "metrics": {"Top 1 Accuracy": "80.1%", "Top 5 Accuracy": "94.8", "Number of params": "61.5M", "GFLOPs": "7.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.1, "Top 5 Accuracy": 94.8, "Number of params": 61500000.0, "GFLOPs": 7.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 110191, "title": "Exploring Randomly Wired Neural Networks for Image Recognition", "url": "/paper/exploring-randomly-wired-neural-networks-for", "published": "2019-04-02T00:00:00.000000", "code": true, "review_url": "/paper/exploring-randomly-wired-neural-networks-for/review/?hl=4576"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 37529, "rank": 501, "method": "WideNet-H", "mlmodel": {}, "method_short": "WideNet-H", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-25", "metrics": {"Top 1 Accuracy": "80.09%", "Top 5 Accuracy": null, "Number of params": "63M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.09, "Top 5 Accuracy": null, "Number of params": 63000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841171, "title": "Go Wider Instead of Deeper", "url": "/paper/go-wider-instead-of-deeper", "published": "2021-07-25T00:00:00.000000", "code": true, "review_url": "/paper/go-wider-instead-of-deeper/review/?hl=37529"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24933, "rank": 502, "method": "ResNet-101", "mlmodel": {}, "method_short": "ResNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "80%", "Top 5 Accuracy": "95%", "Number of params": "44.4M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.0, "Top 5 Accuracy": 95.0, "Number of params": 44400000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24933"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 78963, "rank": 503, "method": "MogaNet-T (256res)", "mlmodel": {}, "method_short": "MogaNet-T ", "method_details": "256res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-07", "metrics": {"Top 1 Accuracy": "80%", "Top 5 Accuracy": null, "Number of params": "5.2", "GFLOPs": "1.44", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.0, "Top 5 Accuracy": null, "Number of params": 5.2, "GFLOPs": 1.44, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1106694, "title": "Efficient Multi-order Gated Aggregation Network", "url": "/paper/efficient-multi-order-gated-aggregation", "published": "2022-11-07T00:00:00.000000", "code": true, "review_url": "/paper/efficient-multi-order-gated-aggregation/review/?hl=78963"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 38560, "rank": 504, "method": "CoE-Small + CondConv + PWLU", "mlmodel": {}, "method_short": "CoE-Small + CondConv + PWLU", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-08", "metrics": {"Top 1 Accuracy": "80%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.100", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 832426, "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs", "url": "/paper/collaboration-of-experts-achieving-80-top-1", "published": "2021-07-08T00:00:00.000000", "code": false, "review_url": "/paper/collaboration-of-experts-achieving-80-top-1/review/?hl=38560"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 61474, "rank": 505, "method": "BasisNet-MV3", "mlmodel": {}, "method_short": "BasisNet-MV3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "80%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.198", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.198, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795440, "title": "BasisNet: Two-stage Model Synthesis for Efficient Inference", "url": "/paper/basisnet-two-stage-model-synthesis-for-1", "published": "2021-05-07T00:00:00.000000", "code": false, "review_url": "/paper/basisnet-two-stage-model-synthesis-for-1/review/?hl=61474"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 57965, "rank": 506, "method": "AlphaNet-A4", "mlmodel": {}, "method_short": "AlphaNet-A4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "80.0%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.444", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.444, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57965"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29479, "rank": 507, "method": "LeViT-192", "mlmodel": {}, "method_short": "LeViT-192", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-02", "metrics": {"Top 1 Accuracy": "80%", "Top 5 Accuracy": null, "Number of params": "10.4M", "GFLOPs": "0.624", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 80.0, "Top 5 Accuracy": null, "Number of params": 10400000.0, "GFLOPs": 0.624, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 774248, "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference", "url": "/paper/levit-a-vision-transformer-in-convnet-s", "published": "2021-04-02T00:00:00.000000", "code": true, "review_url": "/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29479"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 6135, "rank": 508, "method": "ResNet-200", "mlmodel": {}, "method_short": "ResNet-200", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-03-16", "metrics": {"Top 1 Accuracy": "79.9%", "Top 5 Accuracy": "95.2%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.9, "Top 5 Accuracy": 95.2, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 31943, "title": "Identity Mappings in Deep Residual Networks", "url": "/paper/identity-mappings-in-deep-residual-networks", "published": "2016-03-16T00:00:00.000000", "code": true, "review_url": "/paper/identity-mappings-in-deep-residual-networks/review/?hl=6135"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 10590, "rank": 509, "method": "RegNetY-8.0GF", "mlmodel": {}, "method_short": "RegNetY-8.0GF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "79.9%", "Top 5 Accuracy": null, "Number of params": "39.2M", "GFLOPs": "8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.9, "Top 5 Accuracy": null, "Number of params": 39200000.0, "GFLOPs": 8.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188625, "title": "Designing Network Design Spaces", "url": "/paper/designing-network-design-spaces", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/designing-network-design-spaces/review/?hl=10590"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 34310, "rank": 510, "method": "ViT-B/16-SAM", "mlmodel": {}, "method_short": "ViT-B/16-SAM", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-03", "metrics": {"Top 1 Accuracy": "79.9%", "Top 5 Accuracy": null, "Number of params": "87M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.9, "Top 5 Accuracy": null, "Number of params": 87000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 810994, "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations", "url": "/paper/when-vision-transformers-outperform-resnets", "published": "2021-06-03T00:00:00.000000", "code": true, "review_url": "/paper/when-vision-transformers-outperform-resnets/review/?hl=34310"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71924, "rank": 511, "method": "TransBoost-ResNet101", "mlmodel": {}, "method_short": "TransBoost-ResNet101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "79.86%", "Top 5 Accuracy": null, "Number of params": "44.55M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.86, "Top 5 Accuracy": null, "Number of params": 44550000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71924"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 5139, "rank": 512, "method": "SKNet-101", "mlmodel": {}, "method_short": "SKNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-03-15", "metrics": {"Top 1 Accuracy": "79.81%", "Top 5 Accuracy": null, "Number of params": "48.9M", "GFLOPs": "8.46", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.81, "Top 5 Accuracy": null, "Number of params": 48900000.0, "GFLOPs": 8.46, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 108643, "title": "Selective Kernel Networks", "url": "/paper/selective-kernel-networks", "published": "2019-03-15T00:00:00.000000", "code": true, "review_url": "/paper/selective-kernel-networks/review/?hl=5139"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 9596, "rank": 513, "method": "CSPResNeXt-50 + Mish", "mlmodel": {}, "method_short": "CSPResNeXt-50 + Mish", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-23", "metrics": {"Top 1 Accuracy": "79.8%", "Top 5 Accuracy": "95.2%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.8, "Top 5 Accuracy": 95.2, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 151086, "title": "Mish: A Self Regularized Non-Monotonic Activation Function", "url": "/paper/mish-a-self-regularized-non-monotonic-neural", "published": "2019-08-23T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 9800, "rank": 514, "method": "CSPResNeXt-50 (Mish+Aug)", "mlmodel": {}, "method_short": "CSPResNeXt-50 ", "method_details": "Mish+Aug", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-27", "metrics": {"Top 1 Accuracy": "79.8%", "Top 5 Accuracy": "95.2%", "Number of params": "20.5M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.8, "Top 5 Accuracy": 95.2, "Number of params": 20500000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174547, "title": "CSPNet: A New Backbone that can Enhance Learning Capability of CNN", "url": "/paper/cspnet-a-new-backbone-that-can-enhance", "published": "2019-11-27T00:00:00.000000", "code": true, "review_url": "/paper/cspnet-a-new-backbone-that-can-enhance/review/?hl=9800"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5976, "rank": 515, "method": "FixResNet-50 CutMix", "mlmodel": {}, "method_short": "FixResNet-50 CutMix", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-14", "metrics": {"Top 1 Accuracy": "79.8%", "Top 5 Accuracy": "94.9", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.8, "Top 5 Accuracy": 94.9, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 142997, "title": "Fixing the train-test resolution discrepancy", "url": "/paper/fixing-the-train-test-resolution-discrepancy", "published": "2019-06-14T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy/review/?hl=5976"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6185, "rank": 516, "method": "EfficientNet-B2", "mlmodel": {}, "method_short": "EfficientNet-B2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "79.8%", "Top 5 Accuracy": "94.9", "Number of params": "9.2M", "GFLOPs": "1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.8, "Top 5 Accuracy": 94.9, "Number of params": 9200000.0, "GFLOPs": 1.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6185"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 60739, "rank": 517, "method": "GC ViT-XXT", "mlmodel": {}, "method_short": "GC ViT-XXT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-20", "metrics": {"Top 1 Accuracy": "79.8%", "Top 5 Accuracy": null, "Number of params": "12M", "GFLOPs": "2.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.8, "Top 5 Accuracy": null, "Number of params": 12000000.0, "GFLOPs": 2.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029616, "title": "Global Context Vision Transformers", "url": "/paper/global-context-vision-transformers", "published": "2022-06-20T00:00:00.000000", "code": true, "review_url": "/paper/global-context-vision-transformers/review/?hl=60739"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 34134, "rank": 518, "method": "DVT (T2T-ViT-10)", "mlmodel": {}, "method_short": "DVT ", "method_details": "T2T-ViT-10", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-31", "metrics": {"Top 1 Accuracy": "79.74%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.74, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 808027, "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition", "url": "/paper/not-all-images-are-worth-16x16-words-dynamic", "published": "2021-05-31T00:00:00.000000", "code": true, "review_url": "/paper/not-all-images-are-worth-16x16-words-dynamic/review/?hl=34134"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60733, "rank": 519, "method": "NASViT-A1", "mlmodel": {}, "method_short": "NASViT-A1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "79.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.309", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.309, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 70350, "rank": 520, "method": "GPaCo (ResNet-50)", "mlmodel": {}, "method_short": "GPaCo ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-26", "metrics": {"Top 1 Accuracy": "79.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1080045, "title": "Generalized Parametric Contrastive Learning", "url": "/paper/generalized-parametric-contrastive-learning", "published": "2022-09-26T00:00:00.000000", "code": true, "review_url": "/paper/generalized-parametric-contrastive-learning/review/?hl=70350"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 31818, "rank": 521, "method": "ResMLP-36", "mlmodel": {}, "method_short": "ResMLP-36", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "79.7%", "Top 5 Accuracy": null, "Number of params": "45M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.7, "Top 5 Accuracy": null, "Number of params": 45000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795412, "title": "ResMLP: Feedforward networks for image classification with data-efficient training", "url": "/paper/resmlp-feedforward-networks-for-image", "published": "2021-05-07T00:00:00.000000", "code": true, "review_url": "/paper/resmlp-feedforward-networks-for-image/review/?hl=31818"}, "external_source_url": null, "tags": [{"id": 18, "name": "MLP", "color": "#ffae00"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 61464, "rank": 522, "method": "ResT-Small", "mlmodel": {}, "method_short": "ResT-Small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-28", "metrics": {"Top 1 Accuracy": "79.6%", "Top 5 Accuracy": "94.9", "Number of params": "13.66M", "GFLOPs": "1.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.6, "Top 5 Accuracy": 94.9, "Number of params": 13660000.0, "GFLOPs": 1.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 807341, "title": "ResT: An Efficient Transformer for Visual Recognition", "url": "/paper/rest-an-efficient-transformer-for-visual", "published": "2021-05-28T00:00:00.000000", "code": true, "review_url": "/paper/rest-an-efficient-transformer-for-visual/review/?hl=61464"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22083, "rank": 523, "method": "Grafit (ResNet-50)", "mlmodel": {}, "method_short": "Grafit ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "79.6%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.6, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237887, "title": "Grafit: Learning fine-grained image representations with coarse labels", "url": "/paper/grafit-learning-fine-grained-image", "published": "2020-11-25T00:00:00.000000", "code": false, "review_url": "/paper/grafit-learning-fine-grained-image/review/?hl=22083"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29488, "rank": 524, "method": "LeViT-128", "mlmodel": {}, "method_short": "LeViT-128", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-02", "metrics": {"Top 1 Accuracy": "79.6%", "Top 5 Accuracy": null, "Number of params": "8.8M", "GFLOPs": "0.376", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.6, "Top 5 Accuracy": null, "Number of params": 8800000.0, "GFLOPs": 0.376, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 774248, "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference", "url": "/paper/levit-a-vision-transformer-in-convnet-s", "published": "2021-04-02T00:00:00.000000", "code": true, "review_url": "/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29488"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 49039, "rank": 525, "method": "ReXNet_1.3", "mlmodel": {}, "method_short": "ReXNet_1.3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "79.5%", "Top 5 Accuracy": "94.7", "Number of params": "7.6M", "GFLOPs": "0.66", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.5, "Top 5 Accuracy": 94.7, "Number of params": 7600000.0, "GFLOPs": 0.66, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49039"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 37528, "rank": 526, "method": "WideNet-L", "mlmodel": {}, "method_short": "WideNet-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-25", "metrics": {"Top 1 Accuracy": "79.49%", "Top 5 Accuracy": null, "Number of params": "40M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.49, "Top 5 Accuracy": null, "Number of params": 40000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841171, "title": "Go Wider Instead of Deeper", "url": "/paper/go-wider-instead-of-deeper", "published": "2021-07-25T00:00:00.000000", "code": true, "review_url": "/paper/go-wider-instead-of-deeper/review/?hl=37528"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 45002, "rank": 527, "method": "ResNet-50 (SAMix)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "SAMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-30", "metrics": {"Top 1 Accuracy": "79.41%", "Top 5 Accuracy": "94.77%", "Number of params": "25.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.41, "Top 5 Accuracy": 94.77, "Number of params": 25600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 923048, "title": "Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup", "url": "/paper/boosting-discriminative-visual-representation", "published": "2021-11-30T00:00:00.000000", "code": true, "review_url": "/paper/boosting-discriminative-visual-representation/review/?hl=45002"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 4619, "rank": 528, "method": "MultiGrain R50-AA-500", "mlmodel": {}, "method_short": "MultiGrain R50-AA-500", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": "94.8%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": 94.8, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=4619"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24932, "rank": 529, "method": "SENet-50", "mlmodel": {}, "method_short": "SENet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": "94.6%", "Number of params": "28.02M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": 94.6, "Number of params": 28020000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24932"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60596, "rank": 530, "method": "TinyNet (GhostNet-A)", "mlmodel": {}, "method_short": "TinyNet ", "method_details": "GhostNet-A", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-28", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": "94.5", "Number of params": "11.9", "GFLOPs": "0.591", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": 94.5, "Number of params": 11.9, "GFLOPs": 0.591, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 231372, "title": "Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets", "url": "/paper/model-rubik-s-cube-twisting-resolution-depth", "published": "2020-10-28T00:00:00.000000", "code": true, "review_url": "/paper/model-rubik-s-cube-twisting-resolution-depth/review/?hl=60596"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22143, "rank": 531, "method": "ResNet-50 (Adversarial Autoaugment)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "Adversarial Autoaugment", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-24", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": "94.47%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": 94.47, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 178148, "title": "Adversarial AutoAugment", "url": "/paper/adversarial-autoaugment-1", "published": "2019-12-24T00:00:00.000000", "code": false, "review_url": "/paper/adversarial-autoaugment-1/review/?hl=22143"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 57964, "rank": 532, "method": "AlphaNet-A3", "mlmodel": {}, "method_short": "AlphaNet-A3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.357", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.357, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57964"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 31817, "rank": 533, "method": "ResMLP-24", "mlmodel": {}, "method_short": "ResMLP-24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795412, "title": "ResMLP: Feedforward networks for image classification with data-efficient training", "url": "/paper/resmlp-feedforward-networks-for-image", "published": "2021-05-07T00:00:00.000000", "code": true, "review_url": "/paper/resmlp-feedforward-networks-for-image/review/?hl=31817"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 57374, "rank": 534, "method": "EdgeNeXt-S", "mlmodel": {}, "method_short": "EdgeNeXt-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-21", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": null, "Number of params": "5.6M", "GFLOPs": "2.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": null, "Number of params": 5600000.0, "GFLOPs": 2.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029687, "title": "EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications", "url": "/paper/edgenext-efficiently-amalgamated-cnn", "published": "2022-06-21T00:00:00.000000", "code": true, "review_url": "/paper/edgenext-efficiently-amalgamated-cnn/review/?hl=57374"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 252, "name": "CrossCovarianceAttention", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 10591, "rank": 535, "method": "RegNetY-4.0GF", "mlmodel": {}, "method_short": "RegNetY-4.0GF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "79.4%", "Top 5 Accuracy": null, "Number of params": "20.6M", "GFLOPs": "4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.4, "Top 5 Accuracy": null, "Number of params": 20600000.0, "GFLOPs": 4.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188625, "title": "Designing Network Design Spaces", "url": "/paper/designing-network-design-spaces", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/designing-network-design-spaces/review/?hl=10591"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5195, "rank": 536, "method": "ScaleNet-152", "mlmodel": {}, "method_short": "ScaleNet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-20", "metrics": {"Top 1 Accuracy": "79.38%", "Top 5 Accuracy": "94.82", "Number of params": null, "GFLOPs": "11.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.38, "Top 5 Accuracy": 94.82, "Number of params": null, "GFLOPs": 11.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 112304, "title": "Data-Driven Neuron Allocation for Scale Aggregation Networks", "url": "/paper/190409460", "published": "2019-04-20T00:00:00.000000", "code": true, "review_url": "/paper/190409460/review/?hl=5195"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11214, "rank": 537, "method": "LIP-ResNet-101", "mlmodel": {}, "method_short": "LIP-ResNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-12", "metrics": {"Top 1 Accuracy": "79.33%", "Top 5 Accuracy": "94.6%", "Number of params": "42.9M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.33, "Top 5 Accuracy": 94.6, "Number of params": 42900000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 149671, "title": "LIP: Local Importance-based Pooling", "url": "/paper/lip-local-importance-based-pooling", "published": "2019-08-12T00:00:00.000000", "code": true, "review_url": "/paper/lip-local-importance-based-pooling/review/?hl=11214"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 73070, "rank": 538, "method": "MobileViTv3-S", "mlmodel": {}, "method_short": "MobileViTv3-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-30", "metrics": {"Top 1 Accuracy": "79.3%", "Top 5 Accuracy": null, "Number of params": "5.8 M", "GFLOPs": "1.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.3, "Top 5 Accuracy": null, "Number of params": 5.8, "GFLOPs": 1.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1084483, "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features", "url": "/paper/mobilevitv3-mobile-friendly-vision", "published": "2022-09-30T00:00:00.000000", "code": true, "review_url": "/paper/mobilevitv3-mobile-friendly-vision/review/?hl=73070"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 27940, "rank": 539, "method": "RedNet-152", "mlmodel": {}, "method_short": "RedNet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-10", "metrics": {"Top 1 Accuracy": "79.3%", "Top 5 Accuracy": null, "Number of params": "34M", "GFLOPs": "6.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.3, "Top 5 Accuracy": null, "Number of params": 34000000.0, "GFLOPs": 6.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 752660, "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition", "url": "/paper/involution-inverting-the-inherence-of", "published": "2021-03-10T00:00:00.000000", "code": true, "review_url": "/paper/involution-inverting-the-inherence-of/review/?hl=27940"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 44996, "rank": 540, "method": "ResNet-50 (AutoMix)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "AutoMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-24", "metrics": {"Top 1 Accuracy": "79.25%", "Top 5 Accuracy": "94.65", "Number of params": "25.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": "25.6M"}, "raw_metrics": {"Top 1 Accuracy": 79.25, "Top 5 Accuracy": 94.65, "Number of params": 25600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": 25600000.0}, "uses_additional_data": false, "paper": {"id": 756879, "title": "AutoMix: Unveiling the Power of Mixup for Stronger Classifiers", "url": "/paper/automix-unveiling-the-power-of-mixup", "published": "2021-03-24T00:00:00.000000", "code": true, "review_url": "/paper/automix-unveiling-the-power-of-mixup/review/?hl=44996"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 40514, "rank": 541, "method": "PS-KD (ResNet-152 + CutMix)", "mlmodel": {}, "method_short": "PS-KD ", "method_details": "ResNet-152 + CutMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-22", "metrics": {"Top 1 Accuracy": "79.24%", "Top 5 Accuracy": "94.66%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.24, "Top 5 Accuracy": 94.66, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 204960, "title": "Self-Knowledge Distillation with Progressive Refinement of Targets", "url": "/paper/self-knowledge-distillation-a-simple-way-for", "published": "2020-06-22T00:00:00.000000", "code": true, "review_url": "/paper/self-knowledge-distillation-a-simple-way-for/review/?hl=40514"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 11215, "rank": 542, "method": "ResNet-101 (JFT-300M Finetuning)", "mlmodel": {}, "method_short": "ResNet-101 ", "method_details": "JFT-300M Finetuning", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2017-07-10", "metrics": {"Top 1 Accuracy": "79.2%", "Top 5 Accuracy": "94.7%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.2, "Top 5 Accuracy": 94.7, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 19305, "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "url": "/paper/revisiting-unreasonable-effectiveness-of-data", "published": "2017-07-10T00:00:00.000000", "code": true, "review_url": "/paper/revisiting-unreasonable-effectiveness-of-data/review/?hl=11215"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 33619, "rank": 543, "method": "RVT-Ti*", "mlmodel": {}, "method_short": "RVT-Ti*", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-17", "metrics": {"Top 1 Accuracy": "79.2%", "Top 5 Accuracy": "94.7", "Number of params": "10.9M", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.2, "Top 5 Accuracy": 94.7, "Number of params": 10900000.0, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 800173, "title": "Towards Robust Vision Transformer", "url": "/paper/rethinking-the-design-principles-of-robust", "published": "2021-05-17T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-the-design-principles-of-robust/review/?hl=33619"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 17400, "rank": 544, "method": "Multiscale DEQ (MDEQ-XL)", "mlmodel": {}, "method_short": "Multiscale DEQ ", "method_details": "MDEQ-XL", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-15", "metrics": {"Top 1 Accuracy": "79.2%", "Top 5 Accuracy": null, "Number of params": "81M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.2, "Top 5 Accuracy": null, "Number of params": 81000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 202827, "title": "Multiscale Deep Equilibrium Models", "url": "/paper/multiscale-deep-equilibrium-models", "published": "2020-06-15T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60262, "rank": 545, "method": "TinyViT-5M", "mlmodel": {}, "method_short": "TinyViT-5M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-07-21", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": "94.8%", "Number of params": "5.4M", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": 94.8, "Number of params": 5400000.0, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1047551, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "url": "/paper/tinyvit-fast-pretraining-distillation-for", "published": "2022-07-21T00:00:00.000000", "code": true, "review_url": "/paper/tinyvit-fast-pretraining-distillation-for/review/?hl=60262"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 4812, "rank": 546, "method": "AA-ResNet-152", "mlmodel": {}, "method_short": "AA-ResNet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-22", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": "94.6%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": 94.6, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 112308, "title": "Attention Augmented Convolutional Networks", "url": "/paper/190409925", "published": "2019-04-22T00:00:00.000000", "code": true, "review_url": "/paper/190409925/review/?hl=4812"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5975, "rank": 547, "method": "FixResNet-50", "mlmodel": {}, "method_short": "FixResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-14", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": "94.6", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": 94.6, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 142997, "title": "Fixing the train-test resolution discrepancy", "url": "/paper/fixing-the-train-test-resolution-discrepancy", "published": "2019-06-14T00:00:00.000000", "code": true, "review_url": "/paper/fixing-the-train-test-resolution-discrepancy/review/?hl=5975"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 57963, "rank": 548, "method": "AlphaNet-A2", "mlmodel": {}, "method_short": "AlphaNet-A2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.317", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.317, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57963"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29408, "rank": 549, "method": "PiT-XS", "mlmodel": {}, "method_short": "PiT-XS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-30", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": null, "Number of params": "10.6M", "GFLOPs": "1.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": null, "Number of params": 10600000.0, "GFLOPs": 1.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 771780, "title": "Rethinking Spatial Dimensions of Vision Transformers", "url": "/paper/rethinking-spatial-dimensions-of-vision", "published": "2021-03-30T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-spatial-dimensions-of-vision/review/?hl=29408"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 48305, "rank": 550, "method": "UniNet-B0", "mlmodel": {}, "method_short": "UniNet-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-08", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": null, "Number of params": "11.9M", "GFLOPs": "0.56", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": null, "Number of params": 11900000.0, "GFLOPs": 0.56, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 881795, "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", "url": "/paper/uninet-unified-architecture-search-with", "published": "2021-10-08T00:00:00.000000", "code": false, "review_url": "/paper/uninet-unified-architecture-search-with/review/?hl=48305"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27939, "rank": 551, "method": "RedNet-101", "mlmodel": {}, "method_short": "RedNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-10", "metrics": {"Top 1 Accuracy": "79.1%", "Top 5 Accuracy": null, "Number of params": "25.6M", "GFLOPs": "4.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.1, "Top 5 Accuracy": null, "Number of params": 25600000.0, "GFLOPs": 4.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 752660, "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition", "url": "/paper/involution-inverting-the-inherence-of", "published": "2021-03-10T00:00:00.000000", "code": true, "review_url": "/paper/involution-inverting-the-inherence-of/review/?hl=27939"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11216, "rank": 552, "method": "ResNet-50 (UDA)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "UDA", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-29", "metrics": {"Top 1 Accuracy": "79.04%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.04, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 112981, "title": "Unsupervised Data Augmentation for Consistency Training", "url": "/paper/unsupervised-data-augmentation-1", "published": "2019-04-29T00:00:00.000000", "code": true, "review_url": "/paper/unsupervised-data-augmentation-1/review/?hl=11216"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 5196, "rank": 553, "method": "ScaleNet-101", "mlmodel": {}, "method_short": "ScaleNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-20", "metrics": {"Top 1 Accuracy": "79.03%", "Top 5 Accuracy": "94.58", "Number of params": null, "GFLOPs": "7.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.03, "Top 5 Accuracy": 94.58, "Number of params": null, "GFLOPs": 7.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 112304, "title": "Data-Driven Neuron Allocation for Scale Aggregation Networks", "url": "/paper/190409460", "published": "2019-04-20T00:00:00.000000", "code": true, "review_url": "/paper/190409460/review/?hl=5196"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 54152, "rank": 554, "method": "Co-ResNet-152", "mlmodel": {}, "method_short": "Co-ResNet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-08-17", "metrics": {"Top 1 Accuracy": "79.03%", "Top 5 Accuracy": "94.52%", "Number of params": "60M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.03, "Top 5 Accuracy": 94.52, "Number of params": 60000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 852792, "title": "Contextual Convolutional Neural Networks", "url": "/paper/contextual-convolutional-neural-networks", "published": "2021-08-17T00:00:00.000000", "code": true, "review_url": "/paper/contextual-convolutional-neural-networks/review/?hl=54152"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71923, "rank": 555, "method": "TransBoost-ResNet50", "mlmodel": {}, "method_short": "TransBoost-ResNet50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "79.03%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.03, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71923"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 18194, "rank": 556, "method": "MobileNetV3_large_x1_0_ssld", "mlmodel": {}, "method_short": "MobileNetV3_large_x1_0_ssld", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-18", "metrics": {"Top 1 Accuracy": "79.0%", "Top 5 Accuracy": "94.5%", "Number of params": "5.47M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": 94.5, "Number of params": 5470000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 203194, "title": "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset", "url": "/paper/semi-supervised-recognition-under-a-noisy-and", "published": "2020-06-18T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 2055, "rank": 557, "method": "Xception", "mlmodel": {}, "method_short": "Xception", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-10-07", "metrics": {"Top 1 Accuracy": "79%", "Top 5 Accuracy": "94.5", "Number of params": "22.855952M", "GFLOPs": null, "Hardware Burden": "87G", "Operations per network pass": "0.838G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": 94.5, "Number of params": 22855952.0, "GFLOPs": null, "Hardware Burden": 87.0, "Operations per network pass": 0.838, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 24504, "title": "Xception: Deep Learning with Depthwise Separable Convolutions", "url": "/paper/xception-deep-learning-with-depthwise", "published": "2016-10-07T00:00:00.000000", "code": true, "review_url": "/paper/xception-deep-learning-with-depthwise/review/?hl=2055"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 23972, "rank": 558, "method": "SpineNet-143", "mlmodel": {}, "method_short": "SpineNet-143", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-10", "metrics": {"Top 1 Accuracy": "79%", "Top 5 Accuracy": "94.4", "Number of params": "60.5M", "GFLOPs": "9.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": 94.4, "Number of params": 60500000.0, "GFLOPs": 9.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 176141, "title": "SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization", "url": "/paper/spinenet-learning-scale-permuted-backbone-for", "published": "2019-12-10T00:00:00.000000", "code": true, "review_url": "/paper/spinenet-learning-scale-permuted-backbone-for/review/?hl=23972"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 78958, "rank": 559, "method": "MogaNet-T", "mlmodel": {}, "method_short": "MogaNet-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-07", "metrics": {"Top 1 Accuracy": "79%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "1.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 1.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1106694, "title": "Efficient Multi-order Gated Aggregation Network", "url": "/paper/efficient-multi-order-gated-aggregation", "published": "2022-11-07T00:00:00.000000", "code": true, "review_url": "/paper/efficient-multi-order-gated-aggregation/review/?hl=78958"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 64202, "rank": 560, "method": "RevBiFPN-S2", "mlmodel": {}, "method_short": "RevBiFPN-S2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "79%", "Top 5 Accuracy": null, "Number of params": "10.6M", "GFLOPs": "1.37", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": null, "Number of params": 10600000.0, "GFLOPs": 1.37, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64202"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 39228, "rank": 561, "method": "ConvMLP-M", "mlmodel": {}, "method_short": "ConvMLP-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-09", "metrics": {"Top 1 Accuracy": "79%", "Top 5 Accuracy": null, "Number of params": "17.4M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": null, "Number of params": 17400000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 864188, "title": "ConvMLP: Hierarchical Convolutional MLPs for Vision", "url": "/paper/convmlp-hierarchical-convolutional-mlps-for", "published": "2021-09-09T00:00:00.000000", "code": true, "review_url": "/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39228"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 34315, "rank": 562, "method": "Mixer-B/8-SAM", "mlmodel": {}, "method_short": "Mixer-B/8-SAM", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-03", "metrics": {"Top 1 Accuracy": "79%", "Top 5 Accuracy": null, "Number of params": "64M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 79.0, "Top 5 Accuracy": null, "Number of params": 64000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 810994, "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations", "url": "/paper/when-vision-transformers-outperform-resnets", "published": "2021-06-03T00:00:00.000000", "code": true, "review_url": "/paper/when-vision-transformers-outperform-resnets/review/?hl=34315"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 8971, "rank": 563, "method": "InceptionV3 (FRN layer)", "mlmodel": {}, "method_short": "InceptionV3 ", "method_details": "FRN layer", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-21", "metrics": {"Top 1 Accuracy": "78.95%", "Top 5 Accuracy": "94.49%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.95, "Top 5 Accuracy": 94.49, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174026, "title": "Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks", "url": "/paper/filter-response-normalization-layer", "published": "2019-11-21T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 4166, "rank": 564, "method": "ResNet-152 + SWA", "mlmodel": {}, "method_short": "ResNet-152 + SWA", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-03-14", "metrics": {"Top 1 Accuracy": "78.94%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.94, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 8278, "title": "Averaging Weights Leads to Wider Optima and Better Generalization", "url": "/paper/averaging-weights-leads-to-wider-optima-and", "published": "2018-03-14T00:00:00.000000", "code": true, "review_url": "/paper/averaging-weights-leads-to-wider-optima-and/review/?hl=4166"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 8264, "rank": 565, "method": "ECA-Net (ResNet-152)", "mlmodel": {}, "method_short": "ECA-Net ", "method_details": "ResNet-152", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-10-08", "metrics": {"Top 1 Accuracy": "78.92%", "Top 5 Accuracy": "94.55", "Number of params": "57.40M", "GFLOPs": "10.83", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.92, "Top 5 Accuracy": 94.55, "Number of params": 57400000.0, "GFLOPs": 10.83, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 157542, "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks", "url": "/paper/eca-net-efficient-channel-attention-for-deep", "published": "2019-10-08T00:00:00.000000", "code": true, "review_url": "/paper/eca-net-efficient-channel-attention-for-deep/review/?hl=8264"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6228, "rank": 566, "method": "MixNet-L", "mlmodel": {}, "method_short": "MixNet-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-22", "metrics": {"Top 1 Accuracy": "78.9%", "Top 5 Accuracy": "94.2", "Number of params": "7.3M", "GFLOPs": "0.565", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.9, "Top 5 Accuracy": 94.2, "Number of params": 7300000.0, "GFLOPs": 0.565, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 146640, "title": "MixConv: Mixed Depthwise Convolutional Kernels", "url": "/paper/mixnet-mixed-depthwise-convolutional-kernels", "published": "2019-07-22T00:00:00.000000", "code": true, "review_url": "/paper/mixnet-mixed-depthwise-convolutional-kernels/review/?hl=6228"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 57962, "rank": 567, "method": "AlphaNet-A1", "mlmodel": {}, "method_short": "AlphaNet-A1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "78.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.279", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.279, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57962"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11217, "rank": 568, "method": "NoisyStudent (EfficientNet-B0)", "mlmodel": {}, "method_short": "NoisyStudent ", "method_details": "EfficientNet-B0", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-11", "metrics": {"Top 1 Accuracy": "78.8%", "Top 5 Accuracy": "94.5%", "Number of params": "5.3M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.8, "Top 5 Accuracy": 94.5, "Number of params": 5300000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 170047, "title": "Self-training with Noisy Student improves ImageNet classification", "url": "/paper/self-training-with-noisy-student-improves", "published": "2019-11-11T00:00:00.000000", "code": true, "review_url": "/paper/self-training-with-noisy-student-improves/review/?hl=11217"}, "external_source_url": null, "tags": [{"id": 104, "name": "JFT-300M", "color": "#c4bc00"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 24931, "rank": 569, "method": "ResNet-50", "mlmodel": {}, "method_short": "ResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-27", "metrics": {"Top 1 Accuracy": "78.8%", "Top 5 Accuracy": "94.5%", "Number of params": "25.5M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.8, "Top 5 Accuracy": 94.5, "Number of params": 25500000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 739590, "title": "Bottleneck Transformers for Visual Recognition", "url": "/paper/bottleneck-transformers-for-visual", "published": "2021-01-27T00:00:00.000000", "code": true, "review_url": "/paper/bottleneck-transformers-for-visual/review/?hl=24931"}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 2056, "rank": 570, "method": "Inception V3", "mlmodel": {}, "method_short": "Inception V3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-12-02", "metrics": {"Top 1 Accuracy": "78.8%", "Top 5 Accuracy": "94.4%", "Number of params": null, "GFLOPs": "4.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.8, "Top 5 Accuracy": 94.4, "Number of params": null, "GFLOPs": 4.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 37097, "title": "Rethinking the Inception Architecture for Computer Vision", "url": "/paper/rethinking-the-inception-architecture-for", "published": "2015-12-02T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-the-inception-architecture-for/review/?hl=2056"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6186, "rank": 571, "method": "EfficientNet-B1", "mlmodel": {}, "method_short": "EfficientNet-B1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "78.8%", "Top 5 Accuracy": "94.4", "Number of params": "7.8M", "GFLOPs": "0.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.8, "Top 5 Accuracy": 94.4, "Number of params": 7800000.0, "GFLOPs": 0.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6186"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 28490, "rank": 572, "method": "CeiT-T (384 finetune res)", "mlmodel": {}, "method_short": "CeiT-T ", "method_details": "384 finetune res", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-22", "metrics": {"Top 1 Accuracy": "78.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "3.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 3.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755729, "title": "Incorporating Convolution Designs into Visual Transformers", "url": "/paper/incorporating-convolution-designs-into-visual", "published": "2021-03-22T00:00:00.000000", "code": true, "review_url": "/paper/incorporating-convolution-designs-into-visual/review/?hl=28490"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 5137, "rank": 573, "method": "SGE-ResNet101", "mlmodel": {}, "method_short": "SGE-ResNet101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-23", "metrics": {"Top 1 Accuracy": "78.798%", "Top 5 Accuracy": "94.368", "Number of params": "44.55M", "GFLOPs": "7.858", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.798, "Top 5 Accuracy": 94.368, "Number of params": 44550000.0, "GFLOPs": 7.858, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 116805, "title": "Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks", "url": "/paper/spatial-group-wise-enhance-improving-semantic", "published": "2019-05-23T00:00:00.000000", "code": true, "review_url": "/paper/spatial-group-wise-enhance-improving-semantic/review/?hl=5137"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24704, "rank": 574, "method": "RepVGG-B2", "mlmodel": {}, "method_short": "RepVGG-B2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-11", "metrics": {"Top 1 Accuracy": "78.78%", "Top 5 Accuracy": null, "Number of params": "80.31M", "GFLOPs": "18.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.78, "Top 5 Accuracy": null, "Number of params": 80310000.0, "GFLOPs": 18.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 734898, "title": "RepVGG: Making VGG-style ConvNets Great Again", "url": "/paper/repvgg-making-vgg-style-convnets-great-again", "published": "2021-01-11T00:00:00.000000", "code": true, "review_url": "/paper/repvgg-making-vgg-style-convnets-great-again/review/?hl=24704"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 19216, "rank": 575, "method": "ResNet-50", "mlmodel": {}, "method_short": "ResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-09-15", "metrics": {"Top 1 Accuracy": "78.76%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.76, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 217984, "title": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup", "url": "/paper/puzzle-mix-exploiting-saliency-and-local-1", "published": "2020-09-15T00:00:00.000000", "code": true, "review_url": "/paper/puzzle-mix-exploiting-saliency-and-local-1/review/?hl=19216"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 79148, "rank": 576, "method": "SAMix+DM (ResNet-50 RSB A3)", "mlmodel": {}, "method_short": "SAMix+DM ", "method_details": "ResNet-50 RSB A3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "78.75%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.75, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79148"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 24593, "rank": 577, "method": "ResNet-50", "mlmodel": {}, "method_short": "ResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-05", "metrics": {"Top 1 Accuracy": "78.7%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.7, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 734166, "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks", "url": "/paper/autodropout-learning-dropout-patterns-to", "published": "2021-01-05T00:00:00.000000", "code": true, "review_url": "/paper/autodropout-learning-dropout-patterns-to/review/?hl=24593"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 44128, "rank": 578, "method": "SReT-LT (Fast Knowledge Distillation)", "mlmodel": {}, "method_short": "SReT-LT ", "method_details": "Fast Knowledge Distillation", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-12-02", "metrics": {"Top 1 Accuracy": "78.7%", "Top 5 Accuracy": null, "Number of params": "5M", "GFLOPs": "1.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.7, "Top 5 Accuracy": null, "Number of params": 5000000.0, "GFLOPs": 1.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 924618, "title": "A Fast Knowledge Distillation Framework for Visual Recognition", "url": "/paper/a-fast-knowledge-distillation-framework-for", "published": "2021-12-02T00:00:00.000000", "code": true, "review_url": "/paper/a-fast-knowledge-distillation-framework-for/review/?hl=44128"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 36910, "rank": 579, "method": "PVTv2-B1", "mlmodel": {}, "method_short": "PVTv2-B1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-25", "metrics": {"Top 1 Accuracy": "78.7%", "Top 5 Accuracy": null, "Number of params": "13.1M", "GFLOPs": "2.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.7, "Top 5 Accuracy": null, "Number of params": 13100000.0, "GFLOPs": 2.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 825108, "title": "PVT v2: Improved Baselines with Pyramid Vision Transformer", "url": "/paper/pvtv2-improved-baselines-with-pyramid-vision", "published": "2021-06-25T00:00:00.000000", "code": true, "review_url": "/paper/pvtv2-improved-baselines-with-pyramid-vision/review/?hl=36910"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 8265, "rank": 580, "method": "ECA-Net (ResNet-101)", "mlmodel": {}, "method_short": "ECA-Net ", "method_details": "ResNet-101", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-10-08", "metrics": {"Top 1 Accuracy": "78.65%", "Top 5 Accuracy": "94.34", "Number of params": "42.49M", "GFLOPs": "7.35", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.65, "Top 5 Accuracy": 94.34, "Number of params": 42490000.0, "GFLOPs": 7.35, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 157542, "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks", "url": "/paper/eca-net-efficient-channel-attention-for-deep", "published": "2019-10-08T00:00:00.000000", "code": true, "review_url": "/paper/eca-net-efficient-channel-attention-for-deep/review/?hl=8265"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 73073, "rank": 581, "method": "MobileViTv3-1.0", "mlmodel": {}, "method_short": "MobileViTv3-1.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-30", "metrics": {"Top 1 Accuracy": "78.64%", "Top 5 Accuracy": null, "Number of params": "5.1 M", "GFLOPs": "1.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.64, "Top 5 Accuracy": null, "Number of params": 5.1, "GFLOPs": 1.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1084483, "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features", "url": "/paper/mobilevitv3-mobile-friendly-vision", "published": "2022-09-30T00:00:00.000000", "code": true, "review_url": "/paper/mobilevitv3-mobile-friendly-vision/review/?hl=73073"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 49676, "rank": 582, "method": "EdgeFormer-S", "mlmodel": {}, "method_short": "EdgeFormer-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-08", "metrics": {"Top 1 Accuracy": "78.63%", "Top 5 Accuracy": null, "Number of params": "5M", "GFLOPs": "3.48", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.63, "Top 5 Accuracy": null, "Number of params": 5000000.0, "GFLOPs": 3.48, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 973514, "title": "ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer", "url": "/paper/edgeformer-improving-light-weight-convnets-by", "published": "2022-03-08T00:00:00.000000", "code": true, "review_url": "/paper/edgeformer-improving-light-weight-convnets-by/review/?hl=49676"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 79147, "rank": 583, "method": "AutoMix+DM (ResNet-50 RSB A3)", "mlmodel": {}, "method_short": "AutoMix+DM ", "method_details": "ResNet-50 RSB A3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "78.62%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.62, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79147"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 71926, "rank": 584, "method": "TransBoost-EfficientNetB0", "mlmodel": {}, "method_short": "TransBoost-EfficientNetB0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "78.60%", "Top 5 Accuracy": null, "Number of params": "5.29M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.6, "Top 5 Accuracy": null, "Number of params": 5290000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71926"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 30931, "rank": 585, "method": "Visformer-Ti", "mlmodel": {}, "method_short": "Visformer-Ti", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-26", "metrics": {"Top 1 Accuracy": "78.6%", "Top 5 Accuracy": null, "Number of params": "10.3M", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.6, "Top 5 Accuracy": null, "Number of params": 10300000.0, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 788363, "title": "Visformer: The Vision-friendly Transformer", "url": "/paper/visformer-the-vision-friendly-transformer", "published": "2021-04-26T00:00:00.000000", "code": true, "review_url": "/paper/visformer-the-vision-friendly-transformer/review/?hl=30931"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60764, "rank": 586, "method": "ResMLP-12 (distilled, class-MLP)", "mlmodel": {}, "method_short": "ResMLP-12 ", "method_details": "distilled, class-MLP", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "78.6%", "Top 5 Accuracy": null, "Number of params": "17.7M", "GFLOPs": "3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.6, "Top 5 Accuracy": null, "Number of params": 17700000.0, "GFLOPs": 3.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795412, "title": "ResMLP: Feedforward networks for image classification with data-efficient training", "url": "/paper/resmlp-feedforward-networks-for-image", "published": "2021-05-07T00:00:00.000000", "code": true, "review_url": "/paper/resmlp-feedforward-networks-for-image/review/?hl=60764"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 31690, "rank": 587, "method": "RepMLP-Res50", "mlmodel": {}, "method_short": "RepMLP-Res50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-05", "metrics": {"Top 1 Accuracy": "78.60%", "Top 5 Accuracy": null, "Number of params": "52.77M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.6, "Top 5 Accuracy": null, "Number of params": 52770000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 794048, "title": "RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition", "url": "/paper/repmlp-re-parameterizing-convolutions-into", "published": "2021-05-05T00:00:00.000000", "code": true, "review_url": "/paper/repmlp-re-parameterizing-convolutions-into/review/?hl=31690"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11218, "rank": 588, "method": "Res2Net-50-299", "mlmodel": {}, "method_short": "Res2Net-50-299", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-02", "metrics": {"Top 1 Accuracy": "78.59%", "Top 5 Accuracy": "94.12%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.59, "Top 5 Accuracy": 94.12, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 110193, "title": "Res2Net: A New Multi-scale Backbone Architecture", "url": "/paper/res2net-a-new-multi-scale-backbone", "published": "2019-04-02T00:00:00.000000", "code": true, "review_url": "/paper/res2net-a-new-multi-scale-backbone/review/?hl=11218"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60765, "rank": 589, "method": "ResNet-152", "mlmodel": {}, "method_short": "ResNet-152", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-12-10", "metrics": {"Top 1 Accuracy": "78.57%", "Top 5 Accuracy": "94.29", "Number of params": null, "GFLOPs": "11.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.57, "Top 5 Accuracy": 94.29, "Number of params": null, "GFLOPs": 11.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 37118, "title": "Deep Residual Learning for Image Recognition", "url": "/paper/deep-residual-learning-for-image-recognition", "published": "2015-12-10T00:00:00.000000", "code": true, "review_url": "/paper/deep-residual-learning-for-image-recognition/review/?hl=60765"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 21626, "rank": 590, "method": "HCGNet-B", "mlmodel": {}, "method_short": "HCGNet-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-26", "metrics": {"Top 1 Accuracy": "78.5%", "Top 5 Accuracy": "94.2", "Number of params": "12.9M", "GFLOPs": "2.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.5, "Top 5 Accuracy": 94.2, "Number of params": 12900000.0, "GFLOPs": 2.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 151373, "title": "Gated Convolutional Networks with Hybrid Connectivity for Image Classification", "url": "/paper/gated-convolutional-networks-with-hybrid", "published": "2019-08-26T00:00:00.000000", "code": true, "review_url": "/paper/gated-convolutional-networks-with-hybrid/review/?hl=21626"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 9707, "rank": 591, "method": "ResNet-50-DW (Deformable Kernels)", "mlmodel": {}, "method_short": "ResNet-50-DW ", "method_details": "Deformable Kernels", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-10-07", "metrics": {"Top 1 Accuracy": "78.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 157224, "title": "Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation", "url": "/paper/deformable-kernels-adapting-effective", "published": "2019-10-07T00:00:00.000000", "code": true, "review_url": "/paper/deformable-kernels-adapting-effective/review/?hl=9707"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 58455, "rank": 592, "method": "HRFormer-T", "mlmodel": {}, "method_short": "HRFormer-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-18", "metrics": {"Top 1 Accuracy": "78.5%", "Top 5 Accuracy": null, "Number of params": "8.0M", "GFLOPs": "1.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.5, "Top 5 Accuracy": null, "Number of params": 8000000.0, "GFLOPs": 1.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 890071, "title": "HRFormer: High-Resolution Transformer for Dense Prediction", "url": "/paper/hrformer-high-resolution-transformer-for", "published": "2021-10-18T00:00:00.000000", "code": true, "review_url": "/paper/hrformer-high-resolution-transformer-for/review/?hl=58455"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 24703, "rank": 593, "method": "RepVGG-B2g4", "mlmodel": {}, "method_short": "RepVGG-B2g4", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-11", "metrics": {"Top 1 Accuracy": "78.5%", "Top 5 Accuracy": null, "Number of params": "55.77M", "GFLOPs": "11.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.5, "Top 5 Accuracy": null, "Number of params": 55770000.0, "GFLOPs": 11.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 734898, "title": "RepVGG: Making VGG-style ConvNets Great Again", "url": "/paper/repvgg-making-vgg-style-convnets-great-again", "published": "2021-01-11T00:00:00.000000", "code": true, "review_url": "/paper/repvgg-making-vgg-style-convnets-great-again/review/?hl=24703"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 34133, "rank": 594, "method": "DVT (T2T-ViT-7)", "mlmodel": {}, "method_short": "DVT ", "method_details": "T2T-ViT-7", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-31", "metrics": {"Top 1 Accuracy": "78.48%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.48, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 808027, "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition", "url": "/paper/not-all-images-are-worth-16x16-words-dynamic", "published": "2021-05-31T00:00:00.000000", "code": true, "review_url": "/paper/not-all-images-are-worth-16x16-words-dynamic/review/?hl=34133"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 5103, "rank": 595, "method": "SRM-ResNet-101", "mlmodel": {}, "method_short": "SRM-ResNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-03-26", "metrics": {"Top 1 Accuracy": "78.47%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.47, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 109531, "title": "SRM : A Style-based Recalibration Module for Convolutional Neural Networks", "url": "/paper/srm-a-style-based-recalibration-module-for", "published": "2019-03-26T00:00:00.000000", "code": true, "review_url": "/paper/srm-a-style-based-recalibration-module-for/review/?hl=5103"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 4164, "rank": 596, "method": "DenseNet-161 + SWA", "mlmodel": {}, "method_short": "DenseNet-161 + SWA", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-03-14", "metrics": {"Top 1 Accuracy": "78.44%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.44, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 8278, "title": "Averaging Weights Leads to Wider Optima and Better Generalization", "url": "/paper/averaging-weights-leads-to-wider-optima-and", "published": "2018-03-14T00:00:00.000000", "code": true, "review_url": "/paper/averaging-weights-leads-to-wider-optima-and/review/?hl=4164"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_densenet161.ipynb"}]}, {"table_id": 116, "row_id": 5191, "rank": 597, "method": "ResNet-50 (CutMix)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "CutMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-13", "metrics": {"Top 1 Accuracy": "78.4%", "Top 5 Accuracy": "94.10%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.4, "Top 5 Accuracy": 94.1, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 114316, "title": "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features", "url": "/paper/cutmix-regularization-strategy-to-train", "published": "2019-05-13T00:00:00.000000", "code": true, "review_url": "/paper/cutmix-regularization-strategy-to-train/review/?hl=5191"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 45281, "rank": 598, "method": "MobileViT-S", "mlmodel": {}, "method_short": "MobileViT-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-05", "metrics": {"Top 1 Accuracy": "78.4%", "Top 5 Accuracy": "94.1%", "Number of params": "5.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.4, "Top 5 Accuracy": 94.1, "Number of params": 5600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 879564, "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer", "url": "/paper/mobilevit-light-weight-general-purpose-and", "published": "2021-10-05T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60745, "rank": 599, "method": "FBNetV5-AC-CLS", "mlmodel": {}, "method_short": "FBNetV5-AC-CLS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-19", "metrics": {"Top 1 Accuracy": "78.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.280", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.28, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912986, "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run", "url": "/paper/fbnetv5-neural-architecture-search-for", "published": "2021-11-19T00:00:00.000000", "code": false, "review_url": "/paper/fbnetv5-neural-architecture-search-for/review/?hl=60745"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 49040, "rank": 600, "method": "ReXNet_1.0-relabel", "mlmodel": {}, "method_short": "ReXNet_1.0-relabel", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-13", "metrics": {"Top 1 Accuracy": "78.4%", "Top 5 Accuracy": null, "Number of params": "4.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.4, "Top 5 Accuracy": null, "Number of params": 4800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 735765, "title": "Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels", "url": "/paper/re-labeling-imagenet-from-single-to-multi", "published": "2021-01-13T00:00:00.000000", "code": true, "review_url": "/paper/re-labeling-imagenet-from-single-to-multi/review/?hl=49040"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 27938, "rank": 601, "method": "RedNet-50", "mlmodel": {}, "method_short": "RedNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-10", "metrics": {"Top 1 Accuracy": "78.4%", "Top 5 Accuracy": null, "Number of params": "15.5M", "GFLOPs": "2.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.4, "Top 5 Accuracy": null, "Number of params": 15500000.0, "GFLOPs": 2.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 752660, "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition", "url": "/paper/involution-inverting-the-inherence-of", "published": "2021-03-10T00:00:00.000000", "code": true, "review_url": "/paper/involution-inverting-the-inherence-of/review/?hl=27938"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 30496, "rank": 602, "method": "ResNet-50 + DropBlock (0.9 kp, 0.1 label smoothing)", "mlmodel": {}, "method_short": "ResNet-50 + DropBlock ", "method_details": "0.9 kp, 0.1 label smoothing", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-10-30", "metrics": {"Top 1 Accuracy": "78.35%", "Top 5 Accuracy": "94.15%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.35, "Top 5 Accuracy": 94.15, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 60685, "title": "DropBlock: A regularization method for convolutional networks", "url": "/paper/dropblock-a-regularization-method-for", "published": "2018-10-30T00:00:00.000000", "code": true, "review_url": "/paper/dropblock-a-regularization-method-for/review/?hl=30496"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 9302, "rank": 603, "method": "EfficientNet-B0 (CondConv)", "mlmodel": {}, "method_short": "EfficientNet-B0 ", "method_details": "CondConv", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-10", "metrics": {"Top 1 Accuracy": "78.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.826", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.826, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 111267, "title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "url": "/paper/soft-conditional-computation", "published": "2019-04-10T00:00:00.000000", "code": true, "review_url": "/paper/soft-conditional-computation/review/?hl=9302"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 6137, "rank": 604, "method": "ResNet-101", "mlmodel": {}, "method_short": "ResNet-101", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-12-10", "metrics": {"Top 1 Accuracy": "78.25%", "Top 5 Accuracy": "93.95", "Number of params": "40M", "GFLOPs": "7.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.25, "Top 5 Accuracy": 93.95, "Number of params": 40000000.0, "GFLOPs": 7.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 37118, "title": "Deep Residual Learning for Image Recognition", "url": "/paper/deep-residual-learning-for-image-recognition", "published": "2015-12-10T00:00:00.000000", "code": true, "review_url": "/paper/deep-residual-learning-for-image-recognition/review/?hl=6137"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29954, "rank": 605, "method": "LocalViT-PVT", "mlmodel": {}, "method_short": "LocalViT-PVT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "78.2%", "Top 5 Accuracy": "94.2", "Number of params": "13.5M", "GFLOPs": "4.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.2, "Top 5 Accuracy": 94.2, "Number of params": 13500000.0, "GFLOPs": 4.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778779, "title": "LocalViT: Bringing Locality to Vision Transformers", "url": "/paper/localvit-bringing-locality-to-vision", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/localvit-bringing-locality-to-vision/review/?hl=29954"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 4618, "rank": 606, "method": "MultiGrain R50-AA-224", "mlmodel": {}, "method_short": "MultiGrain R50-AA-224", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "78.2%", "Top 5 Accuracy": "93.9%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.2, "Top 5 Accuracy": 93.9, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=4618"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60732, "rank": 607, "method": "NASViT-A0", "mlmodel": {}, "method_short": "NASViT-A0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-29", "metrics": {"Top 1 Accuracy": "78.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.208", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.208, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 882976, "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "url": "/paper/nasvit-neural-architecture-search-for", "published": "2021-09-29T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 77454, "rank": 608, "method": "Pyramid ViG-Ti", "mlmodel": {}, "method_short": "Pyramid ViG-Ti", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-01", "metrics": {"Top 1 Accuracy": "78.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1019895, "title": "Vision GNN: An Image is Worth Graph of Nodes", "url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes", "published": "2022-06-01T00:00:00.000000", "code": true, "review_url": "/paper/vision-gnn-an-image-is-worth-graph-of-nodes/review/?hl=77454"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11220, "rank": 609, "method": "ResNet-50 (LIP Bottleneck-256)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "LIP Bottleneck-256", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-12", "metrics": {"Top 1 Accuracy": "78.15%", "Top 5 Accuracy": "94.02%", "Number of params": "25.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.15, "Top 5 Accuracy": 94.02, "Number of params": 25800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 149671, "title": "LIP: Local Importance-based Pooling", "url": "/paper/lip-local-importance-based-pooling", "published": "2019-08-12T00:00:00.000000", "code": true, "review_url": "/paper/lip-local-importance-based-pooling/review/?hl=11220"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 79146, "rank": 610, "method": "PuzzleMix+DM (ResNet-50 RSB A3)", "mlmodel": {}, "method_short": "PuzzleMix+DM ", "method_details": "ResNet-50 RSB A3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "78.15%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.15, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79146"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6134, "rank": 611, "method": "WRN-50-2-bottleneck", "mlmodel": {}, "method_short": "WRN-50-2-bottleneck", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-05-23", "metrics": {"Top 1 Accuracy": "78.1%", "Top 5 Accuracy": "93.97", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.1, "Top 5 Accuracy": 93.97, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 21610, "title": "Wide Residual Networks", "url": "/paper/wide-residual-networks", "published": "2016-05-23T00:00:00.000000", "code": true, "review_url": "/paper/wide-residual-networks/review/?hl=6134"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 40186, "rank": 612, "method": "ResNet50 (A3)", "mlmodel": {}, "method_short": "ResNet50 ", "method_details": "A3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-01", "metrics": {"Top 1 Accuracy": "78.1%", "Top 5 Accuracy": null, "Number of params": "25M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.1, "Top 5 Accuracy": null, "Number of params": 25000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 877343, "title": "ResNet strikes back: An improved training procedure in timm", "url": "/paper/resnet-strikes-back-an-improved-training", "published": "2021-10-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28357, "rank": 613, "method": "HVT-S-1", "mlmodel": {}, "method_short": "HVT-S-1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "78.00%", "Top 5 Accuracy": "93.83", "Number of params": "21.74M", "GFLOPs": "2.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.0, "Top 5 Accuracy": 93.83, "Number of params": 21740000.0, "GFLOPs": 2.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755458, "title": "Scalable Vision Transformers with Hierarchical Pooling", "url": "/paper/scalable-visual-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/scalable-visual-transformers-with/review/?hl=28357"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 57960, "rank": 614, "method": "ZenNet-400M-SE", "mlmodel": {}, "method_short": "ZenNet-400M-SE", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-01", "metrics": {"Top 1 Accuracy": "78%", "Top 5 Accuracy": null, "Number of params": "5.7M", "GFLOPs": "0.820", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.0, "Top 5 Accuracy": null, "Number of params": 5700000.0, "GFLOPs": 0.82, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 740994, "title": "Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition", "url": "/paper/zen-nas-a-zero-shot-nas-for-high-performance", "published": "2021-02-01T00:00:00.000000", "code": true, "review_url": "/paper/zen-nas-a-zero-shot-nas-for-high-performance/review/?hl=57960"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10592, "rank": 615, "method": "RegNetY-1.6GF", "mlmodel": {}, "method_short": "RegNetY-1.6GF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "78%", "Top 5 Accuracy": null, "Number of params": "11.2M", "GFLOPs": "1.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.0, "Top 5 Accuracy": null, "Number of params": 11200000.0, "GFLOPs": 1.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188625, "title": "Designing Network Design Spaces", "url": "/paper/designing-network-design-spaces", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/designing-network-design-spaces/review/?hl=10592"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60626, "rank": 616, "method": "Perceiver (FF)", "mlmodel": {}, "method_short": "Perceiver ", "method_details": "FF", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-04", "metrics": {"Top 1 Accuracy": "78%", "Top 5 Accuracy": null, "Number of params": "44.9M", "GFLOPs": "707.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 78.0, "Top 5 Accuracy": null, "Number of params": 44900000.0, "GFLOPs": 707.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 750724, "title": "Perceiver: General Perception with Iterative Attention", "url": "/paper/perceiver-general-perception-with-iterative", "published": "2021-03-04T00:00:00.000000", "code": true, "review_url": "/paper/perceiver-general-perception-with-iterative/review/?hl=60626"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 49035, "rank": 617, "method": "ReXNet_1.0", "mlmodel": {}, "method_short": "ReXNet_1.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "77.9%", "Top 5 Accuracy": "93.9", "Number of params": "4.8M", "GFLOPs": "0.40", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.9, "Top 5 Accuracy": 93.9, "Number of params": 4800000.0, "GFLOPs": 0.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49035"}, "external_source_url": null, "tags": [{"id": 37, "name": "", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35598, "rank": 618, "method": "ViTAE-6M", "mlmodel": {}, "method_short": "ViTAE-6M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "77.9%", "Top 5 Accuracy": null, "Number of params": "6.5M", "GFLOPs": "4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.9, "Top 5 Accuracy": null, "Number of params": 6500000.0, "GFLOPs": 4.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812332, "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "url": "/paper/vitae-vision-transformer-advanced-by", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/vitae-vision-transformer-advanced-by/review/?hl=35598"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6153, "rank": 619, "method": "DenseNet-264", "mlmodel": {}, "method_short": "DenseNet-264", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-08-25", "metrics": {"Top 1 Accuracy": "77.85%", "Top 5 Accuracy": "93.88%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.85, "Top 5 Accuracy": 93.88, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 11275, "title": "Densely Connected Convolutional Networks", "url": "/paper/densely-connected-convolutional-networks", "published": "2016-08-25T00:00:00.000000", "code": true, "review_url": "/paper/densely-connected-convolutional-networks/review/?hl=6153"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 57961, "rank": 620, "method": "AlphaNet-A0", "mlmodel": {}, "method_short": "AlphaNet-A0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-16", "metrics": {"Top 1 Accuracy": "77.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.203", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.203, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745792, "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "url": "/paper/alphanet-improved-training-of-supernet-with", "published": "2021-02-16T00:00:00.000000", "code": true, "review_url": "/paper/alphanet-improved-training-of-supernet-with/review/?hl=57961"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60600, "rank": 621, "method": "ScaleNet-50", "mlmodel": {}, "method_short": "ScaleNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-20", "metrics": {"Top 1 Accuracy": "77.8%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "3.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.8, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 3.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 112304, "title": "Data-Driven Neuron Allocation for Scale Aggregation Networks", "url": "/paper/190409460", "published": "2019-04-20T00:00:00.000000", "code": true, "review_url": "/paper/190409460/review/?hl=60600"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60641, "rank": 622, "method": "ResMLP-S12", "mlmodel": {}, "method_short": "ResMLP-S12", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-07", "metrics": {"Top 1 Accuracy": "77.8%", "Top 5 Accuracy": null, "Number of params": "15.4M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.8, "Top 5 Accuracy": null, "Number of params": 15400000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 795412, "title": "ResMLP: Feedforward networks for image classification with data-efficient training", "url": "/paper/resmlp-feedforward-networks-for-image", "published": "2021-05-07T00:00:00.000000", "code": true, "review_url": "/paper/resmlp-feedforward-networks-for-image/review/?hl=60641"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60595, "rank": 623, "method": "TinyNet-A + RA", "mlmodel": {}, "method_short": "TinyNet-A + RA", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-10-28", "metrics": {"Top 1 Accuracy": "77.7%", "Top 5 Accuracy": "93.5", "Number of params": "5.1M", "GFLOPs": "0.339", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.7, "Top 5 Accuracy": 93.5, "Number of params": 5100000.0, "GFLOPs": 0.339, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 231372, "title": "Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets", "url": "/paper/model-rubik-s-cube-twisting-resolution-depth", "published": "2020-10-28T00:00:00.000000", "code": true, "review_url": "/paper/model-rubik-s-cube-twisting-resolution-depth/review/?hl=60595"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 9310, "rank": 624, "method": "ResNet-50 (Fast AA)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "Fast AA", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-01", "metrics": {"Top 1 Accuracy": "77.6%", "Top 5 Accuracy": "95.3%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.6, "Top 5 Accuracy": 95.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 113336, "title": "Fast AutoAugment", "url": "/paper/fast-autoaugment", "published": "2019-05-01T00:00:00.000000", "code": true, "review_url": "/paper/fast-autoaugment/review/?hl=9310"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 60636, "rank": 625, "method": "SReT-T", "mlmodel": {}, "method_short": "SReT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-09", "metrics": {"Top 1 Accuracy": "77.6%", "Top 5 Accuracy": null, "Number of params": "4.8M", "GFLOPs": "1.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.6, "Top 5 Accuracy": null, "Number of params": 4800000.0, "GFLOPs": 1.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 907412, "title": "Sliced Recursive Transformer", "url": "/paper/sliced-recursive-transformer-1", "published": "2021-11-09T00:00:00.000000", "code": true, "review_url": "/paper/sliced-recursive-transformer-1/review/?hl=60636"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27937, "rank": 626, "method": "RedNet-38", "mlmodel": {}, "method_short": "RedNet-38", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-10", "metrics": {"Top 1 Accuracy": "77.6%", "Top 5 Accuracy": null, "Number of params": "12.4M", "GFLOPs": "2.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.6, "Top 5 Accuracy": null, "Number of params": 12400000.0, "GFLOPs": 2.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 752660, "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition", "url": "/paper/involution-inverting-the-inherence-of", "published": "2021-03-10T00:00:00.000000", "code": true, "review_url": "/paper/involution-inverting-the-inherence-of/review/?hl=27937"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60598, "rank": 627, "method": "SGE-ResNet50", "mlmodel": {}, "method_short": "SGE-ResNet50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-23", "metrics": {"Top 1 Accuracy": "77.584%", "Top 5 Accuracy": "93.664", "Number of params": "25.56M", "GFLOPs": "4.127", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.584, "Top 5 Accuracy": 93.664, "Number of params": 25560000.0, "GFLOPs": 4.127, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 116805, "title": "Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks", "url": "/paper/spatial-group-wise-enhance-improving-semantic", "published": "2019-05-23T00:00:00.000000", "code": true, "review_url": "/paper/spatial-group-wise-enhance-improving-semantic/review/?hl=60598"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 37527, "rank": 628, "method": "WideNet-B", "mlmodel": {}, "method_short": "WideNet-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-25", "metrics": {"Top 1 Accuracy": "77.54%", "Top 5 Accuracy": null, "Number of params": "29M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.54, "Top 5 Accuracy": null, "Number of params": 29000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 841171, "title": "Go Wider Instead of Deeper", "url": "/paper/go-wider-instead-of-deeper", "published": "2021-07-25T00:00:00.000000", "code": true, "review_url": "/paper/go-wider-instead-of-deeper/review/?hl=37527"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24594, "rank": 629, "method": "EfficientNet-B0", "mlmodel": {}, "method_short": "EfficientNet-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-05", "metrics": {"Top 1 Accuracy": "77.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 734166, "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks", "url": "/paper/autodropout-learning-dropout-patterns-to", "published": "2021-01-05T00:00:00.000000", "code": true, "review_url": "/paper/autodropout-learning-dropout-patterns-to/review/?hl=24594"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 11221, "rank": 630, "method": "ACNet (ResNet-50)", "mlmodel": {}, "method_short": "ACNet ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-07", "metrics": {"Top 1 Accuracy": "77.5%", "Top 5 Accuracy": null, "Number of params": "29.38M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.5, "Top 5 Accuracy": null, "Number of params": 29380000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 110903, "title": "Adaptively Connected Neural Networks", "url": "/paper/adaptively-connected-neural-networks", "published": "2019-04-07T00:00:00.000000", "code": true, "review_url": "/paper/adaptively-connected-neural-networks/review/?hl=11221"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 8266, "rank": 631, "method": "ECA-Net (ResNet-50)", "mlmodel": {}, "method_short": "ECA-Net ", "method_details": "ResNet-50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-10-08", "metrics": {"Top 1 Accuracy": "77.48%", "Top 5 Accuracy": "93.68", "Number of params": "24.37M", "GFLOPs": "3.86", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.48, "Top 5 Accuracy": 93.68, "Number of params": 24370000.0, "GFLOPs": 3.86, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 157542, "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks", "url": "/paper/eca-net-efficient-channel-attention-for-deep", "published": "2019-10-08T00:00:00.000000", "code": true, "review_url": "/paper/eca-net-efficient-channel-attention-for-deep/review/?hl=8266"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6154, "rank": 632, "method": "DenseNet-201", "mlmodel": {}, "method_short": "DenseNet-201", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-08-25", "metrics": {"Top 1 Accuracy": "77.42%", "Top 5 Accuracy": "93.66%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.42, "Top 5 Accuracy": 93.66, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 11275, "title": "Densely Connected Convolutional Networks", "url": "/paper/densely-connected-convolutional-networks", "published": "2016-08-25T00:00:00.000000", "code": true, "review_url": "/paper/densely-connected-convolutional-networks/review/?hl=6154"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 8970, "rank": 633, "method": "ResnetV2 50 (FRN layer)", "mlmodel": {}, "method_short": "ResnetV2 50 ", "method_details": "FRN layer", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-21", "metrics": {"Top 1 Accuracy": "77.21%", "Top 5 Accuracy": "93.57%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.21, "Top 5 Accuracy": 93.57, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174026, "title": "Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks", "url": "/paper/filter-response-normalization-layer", "published": "2019-11-21T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 49044, "rank": 634, "method": "ReXNet_0.9", "mlmodel": {}, "method_short": "ReXNet_0.9", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "77.2%", "Top 5 Accuracy": "93.5", "Number of params": "4.1M", "GFLOPs": "0.35", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.2, "Top 5 Accuracy": 93.5, "Number of params": 4100000.0, "GFLOPs": 0.35, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49044"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60744, "rank": 635, "method": "FBNetV5-AR-CLS", "mlmodel": {}, "method_short": "FBNetV5-AR-CLS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-19", "metrics": {"Top 1 Accuracy": "77.2%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.215", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.2, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.215, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 912986, "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run", "url": "/paper/fbnetv5-neural-architecture-search-for", "published": "2021-11-19T00:00:00.000000", "code": false, "review_url": "/paper/fbnetv5-neural-architecture-search-for/review/?hl=60744"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27348, "rank": 636, "method": "Prodpoly", "mlmodel": {}, "method_short": "Prodpoly", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-06-20", "metrics": {"Top 1 Accuracy": "77.17%", "Top 5 Accuracy": "93.56%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.17, "Top 5 Accuracy": 93.56, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 205264, "title": "Deep Polynomial Neural Networks", "url": "/paper/deep-polynomial-neural-networks", "published": "2020-06-20T00:00:00.000000", "code": true, "review_url": "/paper/deep-polynomial-neural-networks/review/?hl=27348"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6226, "rank": 637, "method": "ResNet-50-D", "mlmodel": {}, "method_short": "ResNet-50-D", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-12-04", "metrics": {"Top 1 Accuracy": "77.16%", "Top 5 Accuracy": "93.52%", "Number of params": "25M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.16, "Top 5 Accuracy": 93.52, "Number of params": 25000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 64092, "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks", "url": "/paper/bag-of-tricks-for-image-classification-with", "published": "2018-12-04T00:00:00.000000", "code": true, "review_url": "/paper/bag-of-tricks-for-image-classification-with/review/?hl=6226"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 24242, "rank": 638, "method": "Inception v3", "mlmodel": {}, "method_short": "Inception v3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-03-22", "metrics": {"Top 1 Accuracy": "77.12%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.12, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 7743, "title": "What do Deep Networks Like to See?", "url": "/paper/what-do-deep-networks-like-to-see", "published": "2018-03-22T00:00:00.000000", "code": true, "review_url": "/paper/what-do-deep-networks-like-to-see/review/?hl=24242"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60583, "rank": 639, "method": "SkipblockNet-L", "mlmodel": {}, "method_short": "SkipblockNet-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-23", "metrics": {"Top 1 Accuracy": "77.1%", "Top 5 Accuracy": "93.4", "Number of params": "7.1M", "GFLOPs": "0.364", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.1, "Top 5 Accuracy": 93.4, "Number of params": 7100000.0, "GFLOPs": 0.364, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 840600, "title": "Bias Loss for Mobile Neural Networks", "url": "/paper/bias-loss-for-mobile-neural-networks", "published": "2021-07-23T00:00:00.000000", "code": true, "review_url": "/paper/bias-loss-for-mobile-neural-networks/review/?hl=60583"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 10461, "rank": 640, "method": "GreedyNAS-A", "mlmodel": {}, "method_short": "GreedyNAS-A", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-25", "metrics": {"Top 1 Accuracy": "77.1%", "Top 5 Accuracy": "93.3", "Number of params": "6.5M", "GFLOPs": "0.366", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.1, "Top 5 Accuracy": 93.3, "Number of params": 6500000.0, "GFLOPs": 0.366, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188284, "title": "GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet", "url": "/paper/greedynas-towards-fast-one-shot-nas-with", "published": "2020-03-25T00:00:00.000000", "code": false, "review_url": "/paper/greedynas-towards-fast-one-shot-nas-with/review/?hl=10461"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 75997, "rank": 641, "method": "MKD ViT-T", "mlmodel": {}, "method_short": "MKD ViT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-16", "metrics": {"Top 1 Accuracy": "77.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 962993, "title": "Meta Knowledge Distillation", "url": "/paper/meta-knowledge-distillation", "published": "2022-02-16T00:00:00.000000", "code": false, "review_url": "/paper/meta-knowledge-distillation/review/?hl=75997"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6229, "rank": 642, "method": "MixNet-M", "mlmodel": {}, "method_short": "MixNet-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-22", "metrics": {"Top 1 Accuracy": "77%", "Top 5 Accuracy": "93.3", "Number of params": "5.0M", "GFLOPs": "0.360", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.0, "Top 5 Accuracy": 93.3, "Number of params": 5000000.0, "GFLOPs": 0.36, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 146640, "title": "MixConv: Mixed Depthwise Convolutional Kernels", "url": "/paper/mixnet-mixed-depthwise-convolutional-kernels", "published": "2019-07-22T00:00:00.000000", "code": true, "review_url": "/paper/mixnet-mixed-depthwise-convolutional-kernels/review/?hl=6229"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 24314, "rank": 643, "method": "SSAL-Resnet50", "mlmodel": {}, "method_short": "SSAL-Resnet50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-01-07", "metrics": {"Top 1 Accuracy": "77.0%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 77.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 734609, "title": "Contextual Classification Using Self-Supervised Auxiliary Models for Deep Neural Networks", "url": "/paper/contextual-classification-using-self", "published": "2021-01-07T00:00:00.000000", "code": true, "review_url": "/paper/contextual-classification-using-self/review/?hl=24314"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6364, "rank": 644, "method": "SCARLET-A", "mlmodel": {}, "method_short": "SCARLET-A", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-16", "metrics": {"Top 1 Accuracy": "76.9%", "Top 5 Accuracy": "93.4", "Number of params": "6.7M", "GFLOPs": "0.730", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.9, "Top 5 Accuracy": 93.4, "Number of params": 6700000.0, "GFLOPs": 0.73, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 150158, "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search", "url": "/paper/scarletnas-bridging-the-gap-between", "published": "2019-08-16T00:00:00.000000", "code": true, "review_url": "/paper/scarletnas-bridging-the-gap-between/review/?hl=6364"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 71927, "rank": 645, "method": "TransBoost-MobileNetV3-L", "mlmodel": {}, "method_short": "TransBoost-MobileNetV3-L", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "76.81%", "Top 5 Accuracy": null, "Number of params": "5.48M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.81, "Top 5 Accuracy": null, "Number of params": 5480000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71927"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 36766, "rank": 646, "method": "ViTAE-T-Stage", "mlmodel": {}, "method_short": "ViTAE-T-Stage", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "76.8%", "Top 5 Accuracy": "93.5", "Number of params": "4.8M", "GFLOPs": "4.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.8, "Top 5 Accuracy": 93.5, "Number of params": 4800000.0, "GFLOPs": 4.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812332, "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "url": "/paper/vitae-vision-transformer-advanced-by", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/vitae-vision-transformer-advanced-by/review/?hl=36766"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10462, "rank": 647, "method": "GreedyNAS-B", "mlmodel": {}, "method_short": "GreedyNAS-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-25", "metrics": {"Top 1 Accuracy": "76.8%", "Top 5 Accuracy": "93", "Number of params": "5.2M", "GFLOPs": "0.324", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.8, "Top 5 Accuracy": 93.0, "Number of params": 5200000.0, "GFLOPs": 0.324, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188284, "title": "GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet", "url": "/paper/greedynas-towards-fast-one-shot-nas-with", "published": "2020-03-25T00:00:00.000000", "code": false, "review_url": "/paper/greedynas-towards-fast-one-shot-nas-with/review/?hl=10462"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 39227, "rank": 648, "method": "ConvMLP-S", "mlmodel": {}, "method_short": "ConvMLP-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-09-09", "metrics": {"Top 1 Accuracy": "76.8", "Top 5 Accuracy": null, "Number of params": "9M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.8, "Top 5 Accuracy": null, "Number of params": 9000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 864188, "title": "ConvMLP: Hierarchical Convolutional MLPs for Vision", "url": "/paper/convmlp-hierarchical-convolutional-mlps-for", "published": "2021-09-09T00:00:00.000000", "code": true, "review_url": "/paper/convmlp-hierarchical-convolutional-mlps-for/review/?hl=39227"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 22520, "rank": 649, "method": "Perona Malik (Perona and Malik, 1990)", "mlmodel": {}, "method_short": "Perona Malik ", "method_details": "Perona and Malik, 1990", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-03", "metrics": {"Top 1 Accuracy": "76.71%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.71, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 232518, "title": "Learning Visual Representations for Transfer Learning by Suppressing Texture", "url": "/paper/learning-visual-representations-for-transfer-1", "published": "2020-11-03T00:00:00.000000", "code": true, "review_url": "/paper/learning-visual-representations-for-transfer-1/review/?hl=22520"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6233, "rank": 650, "method": "MnasNet-A3", "mlmodel": {}, "method_short": "MnasNet-A3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-07-31", "metrics": {"Top 1 Accuracy": "76.7%", "Top 5 Accuracy": "93.3", "Number of params": "5.2M", "GFLOPs": "0.806", "Hardware Burden": null, "Operations per network pass": "0.0403G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.7, "Top 5 Accuracy": 93.3, "Number of params": 5200000.0, "GFLOPs": 0.806, "Hardware Burden": null, "Operations per network pass": 0.0403, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 53964, "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile", "url": "/paper/mnasnet-platform-aware-neural-architecture", "published": "2018-07-31T00:00:00.000000", "code": true, "review_url": "/paper/mnasnet-platform-aware-neural-architecture/review/?hl=6233"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 73069, "rank": 651, "method": "MobileViTv3-XS", "mlmodel": {}, "method_short": "MobileViTv3-XS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-30", "metrics": {"Top 1 Accuracy": "76.7%", "Top 5 Accuracy": null, "Number of params": "2.5 M", "GFLOPs": "0.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.7, "Top 5 Accuracy": null, "Number of params": 2.5, "GFLOPs": 0.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1084483, "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features", "url": "/paper/mobilevitv3-mobile-friendly-vision", "published": "2022-09-30T00:00:00.000000", "code": true, "review_url": "/paper/mobilevitv3-mobile-friendly-vision/review/?hl=73069"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60642, "rank": 652, "method": "ViL-Tiny-RPB", "mlmodel": {}, "method_short": "ViL-Tiny-RPB", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": "76.7%", "Top 5 Accuracy": null, "Number of params": "6.7", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.7, "Top 5 Accuracy": null, "Number of params": 6.7, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=60642"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 28359, "rank": 653, "method": "ConViT-Ti+", "mlmodel": {}, "method_short": "ConViT-Ti+", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "76.7%", "Top 5 Accuracy": null, "Number of params": "10M", "GFLOPs": "2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.7, "Top 5 Accuracy": null, "Number of params": 10000000.0, "GFLOPs": 2.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755434, "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases", "url": "/paper/convit-improving-vision-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/convit-improving-vision-transformers-with/review/?hl=28359"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71922, "rank": 654, "method": "TransBoost-ResNet34", "mlmodel": {}, "method_short": "TransBoost-ResNet34", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "76.70%", "Top 5 Accuracy": null, "Number of params": "21.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.7, "Top 5 Accuracy": null, "Number of params": 21800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71922"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 11222, "rank": 655, "method": "LIP-DenseNet-BC-121", "mlmodel": {}, "method_short": "LIP-DenseNet-BC-121", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-12", "metrics": {"Top 1 Accuracy": "76.64%", "Top 5 Accuracy": "93.16%", "Number of params": "8.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.64, "Top 5 Accuracy": 93.16, "Number of params": 8700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 149671, "title": "LIP: Local Importance-based Pooling", "url": "/paper/lip-local-importance-based-pooling", "published": "2019-08-12T00:00:00.000000", "code": true, "review_url": "/paper/lip-local-importance-based-pooling/review/?hl=11222"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 34277, "rank": 656, "method": "ResNet-50 (X-volution, stage3)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "X-volution, stage3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-04", "metrics": {"Top 1 Accuracy": "76.6%", "Top 5 Accuracy": "93.3%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.6, "Top 5 Accuracy": 93.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 811671, "title": "X-volution: On the unification of convolution and self-attention", "url": "/paper/x-volution-on-the-unification-of-convolution", "published": "2021-06-04T00:00:00.000000", "code": false, "review_url": "/paper/x-volution-on-the-unification-of-convolution/review/?hl=34277"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 16116, "rank": 657, "method": "MUXNet-l", "mlmodel": {}, "method_short": "MUXNet-l", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-31", "metrics": {"Top 1 Accuracy": "76.6%", "Top 5 Accuracy": "93.2", "Number of params": "4.0M", "GFLOPs": "0.636", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.6, "Top 5 Accuracy": 93.2, "Number of params": 4000000.0, "GFLOPs": 0.636, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188988, "title": "MUXConv: Information Multiplexing in Convolutional Neural Networks", "url": "/paper/muxconv-information-multiplexing-in", "published": "2020-03-31T00:00:00.000000", "code": true, "review_url": "/paper/muxconv-information-multiplexing-in/review/?hl=16116"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 23826, "rank": 658, "method": "DeiT-B", "mlmodel": {}, "method_short": "DeiT-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-12-23", "metrics": {"Top 1 Accuracy": "76.6%", "Top 5 Accuracy": null, "Number of params": "5M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.6, "Top 5 Accuracy": null, "Number of params": 5000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 731001, "title": "Training data-efficient image transformers \u0026 distillation through attention", "url": "/paper/training-data-efficient-image-transformers", "published": "2020-12-23T00:00:00.000000", "code": true, "review_url": "/paper/training-data-efficient-image-transformers/review/?hl=23826"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 73072, "rank": 659, "method": "MobileViTv3-0.75", "mlmodel": {}, "method_short": "MobileViTv3-0.75", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-30", "metrics": {"Top 1 Accuracy": "76.55%", "Top 5 Accuracy": null, "Number of params": "3 M", "GFLOPs": "1.1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.55, "Top 5 Accuracy": null, "Number of params": 0, "GFLOPs": 1.1, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1084483, "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features", "url": "/paper/mobilevitv3-mobile-friendly-vision", "published": "2022-09-30T00:00:00.000000", "code": true, "review_url": "/paper/mobilevitv3-mobile-friendly-vision/review/?hl=73072"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 78957, "rank": 660, "method": "MogaNet-XT", "mlmodel": {}, "method_short": "MogaNet-XT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-11-07", "metrics": {"Top 1 Accuracy": "76.5%", "Top 5 Accuracy": null, "Number of params": "3", "GFLOPs": "0.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.5, "Top 5 Accuracy": null, "Number of params": 3.0, "GFLOPs": 0.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1106694, "title": "Efficient Multi-order Gated Aggregation Network", "url": "/paper/efficient-multi-order-gated-aggregation", "published": "2022-11-07T00:00:00.000000", "code": true, "review_url": "/paper/efficient-multi-order-gated-aggregation/review/?hl=78957"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 31822, "rank": 661, "method": "Mixer-B/16", "mlmodel": {}, "method_short": "Mixer-B/16", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-04", "metrics": {"Top 1 Accuracy": "76.44%", "Top 5 Accuracy": null, "Number of params": "46M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.44, "Top 5 Accuracy": null, "Number of params": 46000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 793349, "title": "MLP-Mixer: An all-MLP Architecture for Vision", "url": "/paper/mlp-mixer-an-all-mlp-architecture-for-vision", "published": "2021-05-04T00:00:00.000000", "code": true, "review_url": "/paper/mlp-mixer-an-all-mlp-architecture-for-vision/review/?hl=31822"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28486, "rank": 662, "method": "CeiT-T", "mlmodel": {}, "method_short": "CeiT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-22", "metrics": {"Top 1 Accuracy": "76.4%", "Top 5 Accuracy": "93.4", "Number of params": "6.4M", "GFLOPs": "1.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.4, "Top 5 Accuracy": 93.4, "Number of params": 6400000.0, "GFLOPs": 1.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755729, "title": "Incorporating Convolution Designs into Visual Transformers", "url": "/paper/incorporating-convolution-designs-into-visual", "published": "2021-03-22T00:00:00.000000", "code": true, "review_url": "/paper/incorporating-convolution-designs-into-visual/review/?hl=28486"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 28365, "rank": 663, "method": "Perceiver", "mlmodel": {}, "method_short": "Perceiver", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-04", "metrics": {"Top 1 Accuracy": "76.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 750724, "title": "Perceiver: General Perception with Iterative Attention", "url": "/paper/perceiver-general-perception-with-iterative", "published": "2021-03-04T00:00:00.000000", "code": true, "review_url": "/paper/perceiver-general-perception-with-iterative/review/?hl=28365"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 45003, "rank": 664, "method": "ResNet-34 (SAMix)", "mlmodel": {}, "method_short": "ResNet-34 ", "method_details": "SAMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-30", "metrics": {"Top 1 Accuracy": "76.35%", "Top 5 Accuracy": "92.32%", "Number of params": "21.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.35, "Top 5 Accuracy": 92.32, "Number of params": 21800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 923048, "title": "Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup", "url": "/paper/boosting-discriminative-visual-representation", "published": "2021-11-30T00:00:00.000000", "code": true, "review_url": "/paper/boosting-discriminative-visual-representation/review/?hl=45003"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6187, "rank": 665, "method": "EfficientNet-B0", "mlmodel": {}, "method_short": "EfficientNet-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-28", "metrics": {"Top 1 Accuracy": "76.3%", "Top 5 Accuracy": "93.2", "Number of params": "5.3M", "GFLOPs": "0.39", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.3, "Top 5 Accuracy": 93.2, "Number of params": 5300000.0, "GFLOPs": 0.39, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 117456, "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/paper/efficientnet-rethinking-model-scaling-for", "published": "2019-05-28T00:00:00.000000", "code": true, "review_url": "/paper/efficientnet-rethinking-model-scaling-for/review/?hl=6187"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 6365, "rank": 666, "method": "SCARLET-B", "mlmodel": {}, "method_short": "SCARLET-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-16", "metrics": {"Top 1 Accuracy": "76.3%", "Top 5 Accuracy": "93", "Number of params": "6.5M", "GFLOPs": "0.658", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.3, "Top 5 Accuracy": 93.0, "Number of params": 6500000.0, "GFLOPs": 0.658, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 150158, "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search", "url": "/paper/scarletnas-bridging-the-gap-between", "published": "2019-08-16T00:00:00.000000", "code": true, "review_url": "/paper/scarletnas-bridging-the-gap-between/review/?hl=6365"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10593, "rank": 667, "method": "RegNetY-800MF", "mlmodel": {}, "method_short": "RegNetY-800MF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "76.3%", "Top 5 Accuracy": null, "Number of params": "6.3M", "GFLOPs": "0.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.3, "Top 5 Accuracy": null, "Number of params": 6300000.0, "GFLOPs": 0.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188625, "title": "Designing Network Design Spaces", "url": "/paper/designing-network-design-spaces", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/designing-network-design-spaces/review/?hl=10593"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 37536, "rank": 668, "method": "GLiT-Tinys", "mlmodel": {}, "method_short": "GLiT-Tinys", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-07", "metrics": {"Top 1 Accuracy": "76.3%", "Top 5 Accuracy": null, "Number of params": "7.2M", "GFLOPs": "1.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.3, "Top 5 Accuracy": null, "Number of params": 7200000.0, "GFLOPs": 1.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 831816, "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer", "url": "/paper/glit-neural-architecture-search-for-global", "published": "2021-07-07T00:00:00.000000", "code": true, "review_url": "/paper/glit-neural-architecture-search-for-global/review/?hl=37536"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6155, "rank": 669, "method": "DenseNet-169", "mlmodel": {}, "method_short": "DenseNet-169", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-08-25", "metrics": {"Top 1 Accuracy": "76.2%", "Top 5 Accuracy": "93.15%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.2, "Top 5 Accuracy": 93.15, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 11275, "title": "Densely Connected Convolutional Networks", "url": "/paper/densely-connected-convolutional-networks", "published": "2016-08-25T00:00:00.000000", "code": true, "review_url": "/paper/densely-connected-convolutional-networks/review/?hl=6155"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 42225, "rank": 670, "method": "SkipblockNet-M", "mlmodel": {}, "method_short": "SkipblockNet-M", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-23", "metrics": {"Top 1 Accuracy": "76.2%", "Top 5 Accuracy": "92.8", "Number of params": "5.5M", "GFLOPs": "0.246", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.2, "Top 5 Accuracy": 92.8, "Number of params": 5500000.0, "GFLOPs": 0.246, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 840600, "title": "Bias Loss for Mobile Neural Networks", "url": "/paper/bias-loss-for-mobile-neural-networks", "published": "2021-07-23T00:00:00.000000", "code": true, "review_url": "/paper/bias-loss-for-mobile-neural-networks/review/?hl=42225"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 10463, "rank": 671, "method": "GreedyNAS-C", "mlmodel": {}, "method_short": "GreedyNAS-C", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-25", "metrics": {"Top 1 Accuracy": "76.2%", "Top 5 Accuracy": "92.5", "Number of params": "4.7M", "GFLOPs": "0.284", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.2, "Top 5 Accuracy": 92.5, "Number of params": 4700000.0, "GFLOPs": 0.284, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188284, "title": "GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet", "url": "/paper/greedynas-towards-fast-one-shot-nas-with", "published": "2020-03-25T00:00:00.000000", "code": false, "review_url": "/paper/greedynas-towards-fast-one-shot-nas-with/review/?hl=10463"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 58506, "rank": 672, "method": "ELP (naive ResNet50)", "mlmodel": {}, "method_short": "ELP ", "method_details": "naive ResNet50", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-29", "metrics": {"Top 1 Accuracy": "76.13", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 76.13, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1021878, "title": "A Simple Episodic Linear Probe Improves Visual Recognition in the Wild", "url": "/paper/a-simple-episodic-linear-probe-improves", "published": "2022-01-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 44997, "rank": 673, "method": "ResNet-34 (AutoMix)", "mlmodel": {}, "method_short": "ResNet-34 ", "method_details": "AutoMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-24", "metrics": {"Top 1 Accuracy": "76.1%", "Top 5 Accuracy": "92.4", "Number of params": "21.8M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": "21.8M"}, "raw_metrics": {"Top 1 Accuracy": 76.1, "Top 5 Accuracy": 92.4, "Number of params": 21800000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": 21800000.0}, "uses_additional_data": false, "paper": {"id": 756879, "title": "AutoMix: Unveiling the Power of Mixup for Stronger Classifiers", "url": "/paper/automix-unveiling-the-power-of-mixup", "published": "2021-03-24T00:00:00.000000", "code": true, "review_url": "/paper/automix-unveiling-the-power-of-mixup/review/?hl=44997"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25985, "rank": 674, "method": "ResNet-50 MLPerf v0.7 - 2512 steps", "mlmodel": {}, "method_short": "ResNet-50 MLPerf v0.7 - 2512 steps", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-02-12", "metrics": {"Top 1 Accuracy": "75.92%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.92, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 745083, "title": "A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes", "url": "/paper/a-large-batch-optimizer-reality-check", "published": "2021-02-12T00:00:00.000000", "code": false, "review_url": "/paper/a-large-batch-optimizer-reality-check/review/?hl=25985"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 29953, "rank": 675, "method": "LocalViT-TNT", "mlmodel": {}, "method_short": "LocalViT-TNT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "75.9%", "Top 5 Accuracy": "93", "Number of params": "6.3M", "GFLOPs": "1.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.9, "Top 5 Accuracy": 93.0, "Number of params": 6300000.0, "GFLOPs": 1.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778779, "title": "LocalViT: Bringing Locality to Vision Transformers", "url": "/paper/localvit-bringing-locality-to-vision", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/localvit-bringing-locality-to-vision/review/?hl=29953"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 7939, "rank": 676, "method": "MoGA-A", "mlmodel": {}, "method_short": "MoGA-A", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-04", "metrics": {"Top 1 Accuracy": "75.9%", "Top 5 Accuracy": "92.8", "Number of params": "5.1M", "GFLOPs": "0.608", "Hardware Burden": null, "Operations per network pass": "0.0304G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.9, "Top 5 Accuracy": 92.8, "Number of params": 5100000.0, "GFLOPs": 0.608, "Hardware Burden": null, "Operations per network pass": 0.0304, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 149130, "title": "MoGA: Searching Beyond MobileNetV3", "url": "/paper/moga-searching-beyond-mobilenetv3", "published": "2019-08-04T00:00:00.000000", "code": true, "review_url": "/paper/moga-searching-beyond-mobilenetv3/review/?hl=7939"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5652, "rank": 677, "method": "DenseNAS-A", "mlmodel": {}, "method_short": "DenseNAS-A", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-23", "metrics": {"Top 1 Accuracy": "75.9%", "Top 5 Accuracy": "92.6", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.9, "Top 5 Accuracy": 92.6, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 143691, "title": "Densely Connected Search Space for More Flexible Neural Architecture Search", "url": "/paper/densely-connected-search-space-for-more", "published": "2019-06-23T00:00:00.000000", "code": true, "review_url": "/paper/densely-connected-search-space-for-more/review/?hl=5652"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 64201, "rank": 678, "method": "RevBiFPN-S1", "mlmodel": {}, "method_short": "RevBiFPN-S1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "75.9%", "Top 5 Accuracy": null, "Number of params": "5.11M", "GFLOPs": "0.62", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.9, "Top 5 Accuracy": null, "Number of params": 5110000.0, "GFLOPs": 0.62, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64201"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 27936, "rank": 679, "method": "RedNet-26", "mlmodel": {}, "method_short": "RedNet-26", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-10", "metrics": {"Top 1 Accuracy": "75.9%", "Top 5 Accuracy": null, "Number of params": "9.2M", "GFLOPs": "1.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.9, "Top 5 Accuracy": null, "Number of params": 9200000.0, "GFLOPs": 1.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 752660, "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition", "url": "/paper/involution-inverting-the-inherence-of", "published": "2021-03-10T00:00:00.000000", "code": true, "review_url": "/paper/involution-inverting-the-inherence-of/review/?hl=27936"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6139, "rank": 680, "method": "FractalNet-34", "mlmodel": {}, "method_short": "FractalNet-34", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-05-24", "metrics": {"Top 1 Accuracy": "75.88%", "Top 5 Accuracy": "92.61", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.88, "Top 5 Accuracy": 92.61, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 22396, "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "url": "/paper/fractalnet-ultra-deep-neural-networks-without", "published": "2016-05-24T00:00:00.000000", "code": true, "review_url": "/paper/fractalnet-ultra-deep-neural-networks-without/review/?hl=6139"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6230, "rank": 681, "method": "MixNet-S", "mlmodel": {}, "method_short": "MixNet-S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-22", "metrics": {"Top 1 Accuracy": "75.8%", "Top 5 Accuracy": "92.8", "Number of params": "4.1M", "GFLOPs": "0.256", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.8, "Top 5 Accuracy": 92.8, "Number of params": 4100000.0, "GFLOPs": 0.256, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 146640, "title": "MixConv: Mixed Depthwise Convolutional Kernels", "url": "/paper/mixnet-mixed-depthwise-convolutional-kernels", "published": "2019-07-22T00:00:00.000000", "code": true, "review_url": "/paper/mixnet-mixed-depthwise-convolutional-kernels/review/?hl=6230"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5940, "rank": 682, "method": "CoordConv ResNet-50", "mlmodel": {}, "method_short": "CoordConv ResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-07-09", "metrics": {"Top 1 Accuracy": "75.74%", "Top 5 Accuracy": "92.75%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.74, "Top 5 Accuracy": 92.75, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 52147, "title": "An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution", "url": "/paper/an-intriguing-failing-of-convolutional-neural", "published": "2018-07-09T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 57968, "rank": 683, "method": "GhostNet \u00d71.3", "mlmodel": {}, "method_short": "GhostNet \u00d71.3", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-27", "metrics": {"Top 1 Accuracy": "75.7%", "Top 5 Accuracy": "92.7", "Number of params": "7.3M", "GFLOPs": "0.226", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.7, "Top 5 Accuracy": 92.7, "Number of params": 7300000.0, "GFLOPs": 0.226, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174553, "title": "GhostNet: More Features from Cheap Operations", "url": "/paper/ghostnet-more-features-from-cheap-operations", "published": "2019-11-27T00:00:00.000000", "code": true, "review_url": "/paper/ghostnet-more-features-from-cheap-operations/review/?hl=57968"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 4856, "rank": 684, "method": "LR-Net-26", "mlmodel": {}, "method_short": "LR-Net-26", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-25", "metrics": {"Top 1 Accuracy": "75.7%", "Top 5 Accuracy": "92.6", "Number of params": "14.7M", "GFLOPs": "2.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.7, "Top 5 Accuracy": 92.6, "Number of params": 14700000.0, "GFLOPs": 2.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 112703, "title": "Local Relation Networks for Image Recognition", "url": "/paper/190411491", "published": "2019-04-25T00:00:00.000000", "code": true, "review_url": "/paper/190411491/review/?hl=4856"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29497, "rank": 685, "method": "LeViT-128S", "mlmodel": {}, "method_short": "LeViT-128S", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-02", "metrics": {"Top 1 Accuracy": "75.7%", "Top 5 Accuracy": null, "Number of params": "4.7M", "GFLOPs": "0.288", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.7, "Top 5 Accuracy": null, "Number of params": 4700000.0, "GFLOPs": 0.288, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 774248, "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference", "url": "/paper/levit-a-vision-transformer-in-convnet-s", "published": "2021-04-02T00:00:00.000000", "code": true, "review_url": "/paper/levit-a-vision-transformer-in-convnet-s/review/?hl=29497"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 6232, "rank": 686, "method": "MnasNet-A2", "mlmodel": {}, "method_short": "MnasNet-A2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-07-31", "metrics": {"Top 1 Accuracy": "75.6%", "Top 5 Accuracy": "92.7", "Number of params": "4.8M", "GFLOPs": "0.680", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.6, "Top 5 Accuracy": 92.7, "Number of params": 4800000.0, "GFLOPs": 0.68, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 53964, "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile", "url": "/paper/mnasnet-platform-aware-neural-architecture", "published": "2018-07-31T00:00:00.000000", "code": true, "review_url": "/paper/mnasnet-platform-aware-neural-architecture/review/?hl=6232"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6366, "rank": 687, "method": "SCARLET-C", "mlmodel": {}, "method_short": "SCARLET-C", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-08-16", "metrics": {"Top 1 Accuracy": "75.6%", "Top 5 Accuracy": "92.6", "Number of params": "6M", "GFLOPs": "0.560", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.6, "Top 5 Accuracy": 92.6, "Number of params": 6000000.0, "GFLOPs": 0.56, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 150158, "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search", "url": "/paper/scarletnas-bridging-the-gap-between", "published": "2019-08-16T00:00:00.000000", "code": true, "review_url": "/paper/scarletnas-bridging-the-gap-between/review/?hl=6366"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 31363, "rank": 688, "method": "PAWS (ResNet-50, 10% labels)", "mlmodel": {}, "method_short": "PAWS ", "method_details": "ResNet-50, 10% labels", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-28", "metrics": {"Top 1 Accuracy": "75.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 788977, "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples", "url": "/paper/semi-supervised-learning-of-visual-features", "published": "2021-04-28T00:00:00.000000", "code": true, "review_url": "/paper/semi-supervised-learning-of-visual-features/review/?hl=31363"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 10594, "rank": 689, "method": "RegNetY-600MF", "mlmodel": {}, "method_short": "RegNetY-600MF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "75.5%", "Top 5 Accuracy": null, "Number of params": "6.1M", "GFLOPs": "0.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.5, "Top 5 Accuracy": null, "Number of params": 6100000.0, "GFLOPs": 0.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188625, "title": "Designing Network Design Spaces", "url": "/paper/designing-network-design-spaces", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/designing-network-design-spaces/review/?hl=10594"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5679, "rank": 690, "method": "ShuffleNet V2", "mlmodel": {}, "method_short": "ShuffleNet V2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-07-30", "metrics": {"Top 1 Accuracy": "75.4%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.597", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.4, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.597, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 53827, "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design", "url": "/paper/shufflenet-v2-practical-guidelines-for", "published": "2018-07-30T00:00:00.000000", "code": true, "review_url": "/paper/shufflenet-v2-practical-guidelines-for/review/?hl=5679"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 61461, "rank": 691, "method": "VAN-B0", "mlmodel": {}, "method_short": "VAN-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-02-20", "metrics": {"Top 1 Accuracy": "75.4%", "Top 5 Accuracy": null, "Number of params": "4.1M", "GFLOPs": "0.9", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.4, "Top 5 Accuracy": null, "Number of params": 4100000.0, "GFLOPs": 0.9, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 964888, "title": "Visual Attention Network", "url": "/paper/visual-attention-network", "published": "2022-02-20T00:00:00.000000", "code": true, "review_url": "/paper/visual-attention-network/review/?hl=61461"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60688, "rank": 692, "method": "AsymmNet-Large \u00d71.0", "mlmodel": {}, "method_short": "AsymmNet-Large \u00d71.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-15", "metrics": {"Top 1 Accuracy": "75.4%", "Top 5 Accuracy": null, "Number of params": "5.99M", "GFLOPs": "0.4338", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.4, "Top 5 Accuracy": null, "Number of params": 5990000.0, "GFLOPs": 0.4338, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 783606, "title": "AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks", "url": "/paper/asymmnet-towards-ultralight-convolution", "published": "2021-04-15T00:00:00.000000", "code": true, "review_url": "/paper/asymmnet-towards-ultralight-convolution/review/?hl=60688"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 57957, "rank": 693, "method": "FairNAS-A", "mlmodel": {}, "method_short": "FairNAS-A", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-03", "metrics": {"Top 1 Accuracy": "75.34%", "Top 5 Accuracy": "92.38", "Number of params": "4.6M", "GFLOPs": "0.776", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.34, "Top 5 Accuracy": 92.38, "Number of params": 4600000.0, "GFLOPs": 0.776, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 144724, "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search", "url": "/paper/fairnas-rethinking-evaluation-fairness-of", "published": "2019-07-03T00:00:00.000000", "code": true, "review_url": "/paper/fairnas-rethinking-evaluation-fairness-of/review/?hl=57957"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6136, "rank": 694, "method": "ResNet-50", "mlmodel": {}, "method_short": "ResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-12-10", "metrics": {"Top 1 Accuracy": "75.3%", "Top 5 Accuracy": "93.29", "Number of params": "25M", "GFLOPs": "3.8", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.3, "Top 5 Accuracy": 93.29, "Number of params": 25000000.0, "GFLOPs": 3.8, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 37118, "title": "Deep Residual Learning for Image Recognition", "url": "/paper/deep-residual-learning-for-image-recognition", "published": "2015-12-10T00:00:00.000000", "code": true, "review_url": "/paper/deep-residual-learning-for-image-recognition/review/?hl=6136"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 16115, "rank": 695, "method": "MUXNet-m", "mlmodel": {}, "method_short": "MUXNet-m", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-31", "metrics": {"Top 1 Accuracy": "75.3%", "Top 5 Accuracy": "92.5", "Number of params": "3.4M", "GFLOPs": "0.436", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.3, "Top 5 Accuracy": 92.5, "Number of params": 3400000.0, "GFLOPs": 0.436, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188988, "title": "MUXConv: Information Multiplexing in Convolutional Neural Networks", "url": "/paper/muxconv-information-multiplexing-in", "published": "2020-03-31T00:00:00.000000", "code": true, "review_url": "/paper/muxconv-information-multiplexing-in/review/?hl=16115"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 35597, "rank": 696, "method": "ViTAE-T", "mlmodel": {}, "method_short": "ViTAE-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-07", "metrics": {"Top 1 Accuracy": "75.3%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "3.0", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.3, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 3.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 812332, "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "url": "/paper/vitae-vision-transformer-advanced-by", "published": "2021-06-07T00:00:00.000000", "code": true, "review_url": "/paper/vitae-vision-transformer-advanced-by/review/?hl=35597"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6231, "rank": 697, "method": "MnasNet-A1", "mlmodel": {}, "method_short": "MnasNet-A1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-07-31", "metrics": {"Top 1 Accuracy": "75.2%", "Top 5 Accuracy": "92.5", "Number of params": "3.9M", "GFLOPs": "0.624", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.2, "Top 5 Accuracy": 92.5, "Number of params": 3900000.0, "GFLOPs": 0.624, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 53964, "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile", "url": "/paper/mnasnet-platform-aware-neural-architecture", "published": "2018-07-31T00:00:00.000000", "code": true, "review_url": "/paper/mnasnet-platform-aware-neural-architecture/review/?hl=6231"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6235, "rank": 698, "method": "MobileNet V3-Large 1.0", "mlmodel": {}, "method_short": "MobileNet V3-Large 1.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-05-06", "metrics": {"Top 1 Accuracy": "75.2%", "Top 5 Accuracy": null, "Number of params": "5.4M", "GFLOPs": "0.438", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.2, "Top 5 Accuracy": null, "Number of params": 5400000.0, "GFLOPs": 0.438, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 113867, "title": "Searching for MobileNetV3", "url": "/paper/searching-for-mobilenetv3", "published": "2019-05-06T00:00:00.000000", "code": true, "review_url": "/paper/searching-for-mobilenetv3/review/?hl=6235"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_mobilenetv3_large_100.ipynb"}]}, {"table_id": 116, "row_id": 5010, "rank": 699, "method": "MultiGrain NASNet-A-Mobile (350px)", "mlmodel": {}, "method_short": "MultiGrain NASNet-A-Mobile ", "method_details": "350px", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "75.1%", "Top 5 Accuracy": "92.5%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.1, "Top 5 Accuracy": 92.5, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 105902, "title": "MultiGrain: a unified image embedding for classes and instances", "url": "/paper/multigrain-a-unified-image-embedding-for", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/multigrain-a-unified-image-embedding-for/review/?hl=5010"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 57958, "rank": 700, "method": "FairNAS-B", "mlmodel": {}, "method_short": "FairNAS-B", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-03", "metrics": {"Top 1 Accuracy": "75.10%", "Top 5 Accuracy": "92.30", "Number of params": "4.5M", "GFLOPs": "0.690", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.1, "Top 5 Accuracy": 92.3, "Number of params": 4500000.0, "GFLOPs": 0.69, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 144724, "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search", "url": "/paper/fairnas-rethinking-evaluation-fairness-of", "published": "2019-07-03T00:00:00.000000", "code": true, "review_url": "/paper/fairnas-rethinking-evaluation-fairness-of/review/?hl=57958"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 5472, "rank": 701, "method": "DiCENet", "mlmodel": {}, "method_short": "DiCENet", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-06-08", "metrics": {"Top 1 Accuracy": "75.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": "0.553", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": 0.553, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 142131, "title": "DiCENet: Dimension-wise Convolutions for Efficient Networks", "url": "/paper/dicenet-dimension-wise-convolutions-for", "published": "2019-06-08T00:00:00.000000", "code": true, "review_url": "/paper/dicenet-dimension-wise-convolutions-for/review/?hl=5472"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 34276, "rank": 702, "method": "ResNet-34 (X-volution, stage3)", "mlmodel": {}, "method_short": "ResNet-34 ", "method_details": "X-volution, stage3", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-04", "metrics": {"Top 1 Accuracy": "75%", "Top 5 Accuracy": "92.4%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.0, "Top 5 Accuracy": 92.4, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 811671, "title": "X-volution: On the unification of convolution and self-attention", "url": "/paper/x-volution-on-the-unification-of-convolution", "published": "2021-06-04T00:00:00.000000", "code": false, "review_url": "/paper/x-volution-on-the-unification-of-convolution/review/?hl=34276"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 60592, "rank": 703, "method": "Ghost-ResNet-50 (s=2)", "mlmodel": {}, "method_short": "Ghost-ResNet-50 ", "method_details": "s=2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-27", "metrics": {"Top 1 Accuracy": "75%", "Top 5 Accuracy": "92.3", "Number of params": "13M", "GFLOPs": "2.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 75.0, "Top 5 Accuracy": 92.3, "Number of params": 13000000.0, "GFLOPs": 2.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174553, "title": "GhostNet: More Features from Cheap Operations", "url": "/paper/ghostnet-more-features-from-cheap-operations", "published": "2019-11-27T00:00:00.000000", "code": true, "review_url": "/paper/ghostnet-more-features-from-cheap-operations/review/?hl=60592"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6156, "rank": 704, "method": "DenseNet-121", "mlmodel": {}, "method_short": "DenseNet-121", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-08-25", "metrics": {"Top 1 Accuracy": "74.98%", "Top 5 Accuracy": "92.29%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.98, "Top 5 Accuracy": 92.29, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 11275, "title": "Densely Connected Convolutional Networks", "url": "/paper/densely-connected-convolutional-networks", "published": "2016-08-25T00:00:00.000000", "code": true, "review_url": "/paper/densely-connected-convolutional-networks/review/?hl=6156"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6236, "rank": 705, "method": "Single-Path NAS", "mlmodel": {}, "method_short": "Single-Path NAS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-05", "metrics": {"Top 1 Accuracy": "74.96%", "Top 5 Accuracy": "92.21%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.96, "Top 5 Accuracy": 92.21, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 110700, "title": "Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours", "url": "/paper/single-path-nas-designing-hardware-efficient", "published": "2019-04-05T00:00:00.000000", "code": true, "review_url": "/paper/single-path-nas-designing-hardware-efficient/review/?hl=6236"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 31685, "rank": 706, "method": "FF", "mlmodel": {}, "method_short": "FF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-06", "metrics": {"Top 1 Accuracy": "74.9", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 794637, "title": "Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet", "url": "/paper/do-you-even-need-attention-a-stack-of-feed", "published": "2021-05-06T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6225, "rank": 707, "method": "FBNet-C", "mlmodel": {}, "method_short": "FBNet-C", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-12-09", "metrics": {"Top 1 Accuracy": "74.9%", "Top 5 Accuracy": null, "Number of params": "5.5M", "GFLOPs": "0.375", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.9, "Top 5 Accuracy": null, "Number of params": 5500000.0, "GFLOPs": 0.375, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 64847, "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search", "url": "/paper/fbnet-hardware-aware-efficient-convnet-design", "published": "2018-12-09T00:00:00.000000", "code": true, "review_url": "/paper/fbnet-hardware-aware-efficient-convnet-design/review/?hl=6225"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 5482, "rank": 708, "method": "ESPNetv2", "mlmodel": {}, "method_short": "ESPNetv2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-11-28", "metrics": {"Top 1 Accuracy": "74.9%", "Top 5 Accuracy": null, "Number of params": "5.9M", "GFLOPs": "0.602", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.9, "Top 5 Accuracy": null, "Number of params": 5900000.0, "GFLOPs": 0.602, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 63634, "title": "ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network", "url": "/paper/espnetv2-a-light-weight-power-efficient-and", "published": "2018-11-28T00:00:00.000000", "code": true, "review_url": "/paper/espnetv2-a-light-weight-power-efficient-and/review/?hl=5482"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29950, "rank": 709, "method": "LocalViT-T", "mlmodel": {}, "method_short": "LocalViT-T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "74.8%", "Top 5 Accuracy": "92.6", "Number of params": "5.9M", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.8, "Top 5 Accuracy": 92.6, "Number of params": 5900000.0, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778779, "title": "LocalViT: Bringing Locality to Vision Transformers", "url": "/paper/localvit-bringing-locality-to-vision", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/localvit-bringing-locality-to-vision/review/?hl=29950"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60768, "rank": 710, "method": "MobileViT-XS", "mlmodel": {}, "method_short": "MobileViT-XS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-10-05", "metrics": {"Top 1 Accuracy": "74.8%", "Top 5 Accuracy": "92.3%", "Number of params": "2.3 M", "GFLOPs": "0.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.8, "Top 5 Accuracy": 92.3, "Number of params": 2.3, "GFLOPs": 0.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 879564, "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer", "url": "/paper/mobilevit-light-weight-general-purpose-and", "published": "2021-10-05T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 2057, "rank": 711, "method": "Inception V2", "mlmodel": {}, "method_short": "Inception V2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-02-11", "metrics": {"Top 1 Accuracy": "74.8%", "Top 5 Accuracy": "92.2%", "Number of params": "11.2M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.8, "Top 5 Accuracy": 92.2, "Number of params": 11200000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 41878, "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "url": "/paper/batch-normalization-accelerating-deep-network", "published": "2015-02-11T00:00:00.000000", "code": true, "review_url": "/paper/batch-normalization-accelerating-deep-network/review/?hl=2057"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 37553, "rank": 712, "method": "AutoFormer-tiny", "mlmodel": {}, "method_short": "AutoFormer-tiny", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-01", "metrics": {"Top 1 Accuracy": "74.7%", "Top 5 Accuracy": "92.6", "Number of params": "5.7M", "GFLOPs": "1.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.7, "Top 5 Accuracy": 92.6, "Number of params": 5700000.0, "GFLOPs": 1.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 828740, "title": "AutoFormer: Searching Transformers for Visual Recognition", "url": "/paper/autoformer-searching-transformers-for-visual", "published": "2021-07-01T00:00:00.000000", "code": true, "review_url": "/paper/autoformer-searching-transformers-for-visual/review/?hl=37553"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 60603, "rank": 713, "method": "RandWire-WS (small)", "mlmodel": {}, "method_short": "RandWire-WS ", "method_details": "small", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-04-02", "metrics": {"Top 1 Accuracy": "74.7%", "Top 5 Accuracy": "92.2", "Number of params": "5.6M", "GFLOPs": "0.583", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.7, "Top 5 Accuracy": 92.2, "Number of params": 5600000.0, "GFLOPs": 0.583, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 110191, "title": "Exploring Randomly Wired Neural Networks for Image Recognition", "url": "/paper/exploring-randomly-wired-neural-networks-for", "published": "2019-04-02T00:00:00.000000", "code": true, "review_url": "/paper/exploring-randomly-wired-neural-networks-for/review/?hl=60603"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 5387, "rank": 714, "method": "MobileNetV2 (1.4)", "mlmodel": {}, "method_short": "MobileNetV2 ", "method_details": "1.4", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-01-13", "metrics": {"Top 1 Accuracy": "74.7%", "Top 5 Accuracy": null, "Number of params": "6.9M", "GFLOPs": "1.170", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.7, "Top 5 Accuracy": null, "Number of params": 6900000.0, "GFLOPs": 1.17, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 6891, "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "url": "/paper/mobilenetv2-inverted-residuals-and-linear", "published": "2018-01-13T00:00:00.000000", "code": true, "review_url": "/paper/mobilenetv2-inverted-residuals-and-linear/review/?hl=5387"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_mobilenet_v2.ipynb"}]}, {"table_id": 116, "row_id": 57959, "rank": 715, "method": "FairNAS-C", "mlmodel": {}, "method_short": "FairNAS-C", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-03", "metrics": {"Top 1 Accuracy": "74.69%", "Top 5 Accuracy": "92.12", "Number of params": "4.4M", "GFLOPs": "0.642", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.69, "Top 5 Accuracy": 92.12, "Number of params": 4400000.0, "GFLOPs": 0.642, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 144724, "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search", "url": "/paper/fairnas-rethinking-evaluation-fairness-of", "published": "2019-07-03T00:00:00.000000", "code": true, "review_url": "/paper/fairnas-rethinking-evaluation-fairness-of/review/?hl=57959"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 49045, "rank": 716, "method": "ReXNet_0.6", "mlmodel": {}, "method_short": "ReXNet_0.6", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-07-02", "metrics": {"Top 1 Accuracy": "74.6%", "Top 5 Accuracy": "92.1", "Number of params": "2.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.6, "Top 5 Accuracy": 92.1, "Number of params": 2700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 206716, "title": "Rethinking Channel Dimensions for Efficient Model Design", "url": "/paper/rexnet-diminishing-representational", "published": "2020-07-02T00:00:00.000000", "code": true, "review_url": "/paper/rexnet-diminishing-representational/review/?hl=49045"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11223, "rank": 717, "method": "Proxyless", "mlmodel": {}, "method_short": "Proxyless", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-12-02", "metrics": {"Top 1 Accuracy": "74.6%", "Top 5 Accuracy": null, "Number of params": "4.0M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.6, "Top 5 Accuracy": null, "Number of params": 4000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 64252, "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "url": "/paper/proxylessnas-direct-neural-architecture", "published": "2018-12-02T00:00:00.000000", "code": true, "review_url": "/paper/proxylessnas-direct-neural-architecture/review/?hl=11223"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29407, "rank": 718, "method": "PiT-Ti", "mlmodel": {}, "method_short": "PiT-Ti", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-30", "metrics": {"Top 1 Accuracy": "74.6%", "Top 5 Accuracy": null, "Number of params": "4.9M", "GFLOPs": "0.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.6, "Top 5 Accuracy": null, "Number of params": 4900000.0, "GFLOPs": 0.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 771780, "title": "Rethinking Spatial Dimensions of Vision Transformers", "url": "/paper/rethinking-spatial-dimensions-of-vision", "published": "2021-03-30T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-spatial-dimensions-of-vision/review/?hl=29407"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 6130, "rank": 719, "method": "VGG-19", "mlmodel": {}, "method_short": "VGG-19", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2014-09-04", "metrics": {"Top 1 Accuracy": "74.5%", "Top 5 Accuracy": "92.0", "Number of params": "144M", "GFLOPs": null, "Hardware Burden": "31G", "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.5, "Top 5 Accuracy": 92.0, "Number of params": 144000000.0, "GFLOPs": null, "Hardware Burden": 31.0, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 41308, "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "url": "/paper/very-deep-convolutional-networks-for-large", "published": "2014-09-04T00:00:00.000000", "code": true, "review_url": "/paper/very-deep-convolutional-networks-for-large/review/?hl=6130"}, "external_source_url": null, "tags": [], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_vgg19.ipynb"}]}, {"table_id": 116, "row_id": 6131, "rank": 720, "method": "VGG-16", "mlmodel": {}, "method_short": "VGG-16", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2014-09-04", "metrics": {"Top 1 Accuracy": "74.4%", "Top 5 Accuracy": "91.9", "Number of params": "138M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.4, "Top 5 Accuracy": 91.9, "Number of params": 138000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 41308, "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "url": "/paper/very-deep-convolutional-networks-for-large", "published": "2014-09-04T00:00:00.000000", "code": true, "review_url": "/paper/very-deep-convolutional-networks-for-large/review/?hl=6131"}, "external_source_url": null, "tags": [], "reports": [{"url": "https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/evaluate_robustness_imagenet_vgg16.ipynb"}]}, {"table_id": 116, "row_id": 19544, "rank": 721, "method": "DY-MobileNetV2 \u00d71.0", "mlmodel": {}, "method_short": "DY-MobileNetV2 \u00d71.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "74.4%", "Top 5 Accuracy": null, "Number of params": "11.1M", "GFLOPs": "0,626", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.4, "Top 5 Accuracy": null, "Number of params": 11100000.0, "GFLOPs": 0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19544"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60593, "rank": 722, "method": "Ghost-ResNet-50 (s=4)", "mlmodel": {}, "method_short": "Ghost-ResNet-50 ", "method_details": "s=4", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-27", "metrics": {"Top 1 Accuracy": "74.1%", "Top 5 Accuracy": "91.9", "Number of params": "6.5M", "GFLOPs": "1.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.1, "Top 5 Accuracy": 91.9, "Number of params": 6500000.0, "GFLOPs": 1.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174553, "title": "GhostNet: More Features from Cheap Operations", "url": "/paper/ghostnet-more-features-from-cheap-operations", "published": "2019-11-27T00:00:00.000000", "code": true, "review_url": "/paper/ghostnet-more-features-from-cheap-operations/review/?hl=60593"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60585, "rank": 723, "method": "RegNetY-400MF", "mlmodel": {}, "method_short": "RegNetY-400MF", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-30", "metrics": {"Top 1 Accuracy": "74.1%", "Top 5 Accuracy": null, "Number of params": "4.3M", "GFLOPs": "0.4", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.1, "Top 5 Accuracy": null, "Number of params": 4300000.0, "GFLOPs": 0.4, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188625, "title": "Designing Network Design Spaces", "url": "/paper/designing-network-design-spaces", "published": "2020-03-30T00:00:00.000000", "code": true, "review_url": "/paper/designing-network-design-spaces/review/?hl=60585"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 60635, "rank": 724, "method": "SReT-ExT", "mlmodel": {}, "method_short": "SReT-ExT", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-09", "metrics": {"Top 1 Accuracy": "74.0%", "Top 5 Accuracy": null, "Number of params": "4M", "GFLOPs": "0.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 74.0, "Top 5 Accuracy": null, "Number of params": 4000000.0, "GFLOPs": 0.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 907412, "title": "Sliced Recursive Transformer", "url": "/paper/sliced-recursive-transformer-1", "published": "2021-11-09T00:00:00.000000", "code": true, "review_url": "/paper/sliced-recursive-transformer-1/review/?hl=60635"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 57969, "rank": 725, "method": "GhostNet \u00d71.0", "mlmodel": {}, "method_short": "GhostNet \u00d71.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-27", "metrics": {"Top 1 Accuracy": "73.9%", "Top 5 Accuracy": "91.4", "Number of params": "5.2M", "GFLOPs": "0.141", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 73.9, "Top 5 Accuracy": 91.4, "Number of params": 5200000.0, "GFLOPs": 0.141, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174553, "title": "GhostNet: More Features from Cheap Operations", "url": "/paper/ghostnet-more-features-from-cheap-operations", "published": "2019-11-27T00:00:00.000000", "code": true, "review_url": "/paper/ghostnet-more-features-from-cheap-operations/review/?hl=57969"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 37853, "rank": 726, "method": "DeiT-Ti with iRPE-K", "mlmodel": {}, "method_short": "DeiT-Ti with iRPE-K", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-07-29", "metrics": {"Top 1 Accuracy": "73.7%", "Top 5 Accuracy": null, "Number of params": "6M", "GFLOPs": "2.568", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 73.7, "Top 5 Accuracy": null, "Number of params": 6000000.0, "GFLOPs": 2.568, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 843509, "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer", "url": "/paper/rethinking-and-improving-relative-position", "published": "2021-07-29T00:00:00.000000", "code": true, "review_url": "/paper/rethinking-and-improving-relative-position/review/?hl=37853"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 71921, "rank": 727, "method": "TransBoost-ResNet18", "mlmodel": {}, "method_short": "TransBoost-ResNet18", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-05-26", "metrics": {"Top 1 Accuracy": "73.36%", "Top 5 Accuracy": null, "Number of params": "11.69M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 73.36, "Top 5 Accuracy": null, "Number of params": 11690000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1016638, "title": "TransBoost: Improving the Best ImageNet Performance using Deep Transduction", "url": "/paper/transboost-improving-the-best-imagenet", "published": "2022-05-26T00:00:00.000000", "code": true, "review_url": "/paper/transboost-improving-the-best-imagenet/review/?hl=71921"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 9044, "rank": 728, "method": "Wide ResNet-50 (edge-popup)", "mlmodel": {}, "method_short": "Wide ResNet-50 ", "method_details": "edge-popup", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-29", "metrics": {"Top 1 Accuracy": "73.3%", "Top 5 Accuracy": null, "Number of params": "20.6M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 73.3, "Top 5 Accuracy": null, "Number of params": 20600000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174702, "title": "What's Hidden in a Randomly Weighted Neural Network?", "url": "/paper/whats-hidden-in-a-randomly-weighted-neural", "published": "2019-11-29T00:00:00.000000", "code": true, "review_url": "/paper/whats-hidden-in-a-randomly-weighted-neural/review/?hl=9044"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 44127, "rank": 729, "method": "ResNet-18 (MEAL V2)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "MEAL V2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-09-17", "metrics": {"Top 1 Accuracy": "73.19%", "Top 5 Accuracy": "90.82", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 73.19, "Top 5 Accuracy": 90.82, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 218270, "title": "MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks", "url": "/paper/meal-v2-boosting-vanilla-resnet-50-to-80-top", "published": "2020-09-17T00:00:00.000000", "code": true, "review_url": "/paper/meal-v2-boosting-vanilla-resnet-50-to-80-top/review/?hl=44127"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28358, "rank": 730, "method": "ConViT-Ti", "mlmodel": {}, "method_short": "ConViT-Ti", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "73.1%", "Top 5 Accuracy": null, "Number of params": "6M", "GFLOPs": "1", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 73.1, "Top 5 Accuracy": null, "Number of params": 6000000.0, "GFLOPs": 1.0, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755434, "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases", "url": "/paper/convit-improving-vision-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/convit-improving-vision-transformers-with/review/?hl=28358"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 64200, "rank": 731, "method": "RevBiFPN-S0", "mlmodel": {}, "method_short": "RevBiFPN-S0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-28", "metrics": {"Top 1 Accuracy": "72.8%", "Top 5 Accuracy": null, "Number of params": "3.42M", "GFLOPs": "0.31", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.8, "Top 5 Accuracy": null, "Number of params": 3420000.0, "GFLOPs": 0.31, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1034397, "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network", "url": "/paper/revbifpn-the-fully-reversible-bidirectional", "published": "2022-06-28T00:00:00.000000", "code": true, "review_url": "/paper/revbifpn-the-fully-reversible-bidirectional/review/?hl=64200"}, "external_source_url": null, "tags": [{"id": 283, "name": "Reversible", "color": "#3227d3"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 13, "name": "FPN", "color": "#009481"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 19545, "rank": 732, "method": "DY-MobileNetV2 \u00d70.75", "mlmodel": {}, "method_short": "DY-MobileNetV2 \u00d70.75", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "72.8%", "Top 5 Accuracy": null, "Number of params": "7M", "GFLOPs": "0.435", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.8, "Top 5 Accuracy": null, "Number of params": 7000000.0, "GFLOPs": 0.435, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19545"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 19549, "rank": 733, "method": "DY-ResNet-18", "mlmodel": {}, "method_short": "DY-ResNet-18", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "72.7%", "Top 5 Accuracy": null, "Number of params": "42.7M", "GFLOPs": "3.7", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.7, "Top 5 Accuracy": null, "Number of params": 42700000.0, "GFLOPs": 3.7, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19549"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6033, "rank": 734, "method": "MobileNet-224 (CGD)", "mlmodel": {}, "method_short": "MobileNet-224 ", "method_details": "CGD", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-07-23", "metrics": {"Top 1 Accuracy": "72.56%", "Top 5 Accuracy": "90.92", "Number of params": "4.26M", "GFLOPs": "1.198", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.56, "Top 5 Accuracy": 90.92, "Number of params": 4260000.0, "GFLOPs": 1.198, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 146618, "title": "Compact Global Descriptor for Neural Networks", "url": "/paper/compact-global-descriptor-for-neural-networks", "published": "2019-07-23T00:00:00.000000", "code": true, "review_url": "/paper/compact-global-descriptor-for-neural-networks/review/?hl=6033"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 8263, "rank": 735, "method": "ECA-Net (MobileNetV2)", "mlmodel": {}, "method_short": "ECA-Net ", "method_details": "MobileNetV2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-10-08", "metrics": {"Top 1 Accuracy": "72.56%", "Top 5 Accuracy": "90.81", "Number of params": "3.34M", "GFLOPs": "0.320", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.56, "Top 5 Accuracy": 90.81, "Number of params": 3340000.0, "GFLOPs": 0.32, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 157542, "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks", "url": "/paper/eca-net-efficient-channel-attention-for-deep", "published": "2019-10-08T00:00:00.000000", "code": true, "review_url": "/paper/eca-net-efficient-channel-attention-for-deep/review/?hl=8263"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 29952, "rank": 736, "method": "LocalViT-T2T", "mlmodel": {}, "method_short": "LocalViT-T2T", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-12", "metrics": {"Top 1 Accuracy": "72.5%", "Top 5 Accuracy": null, "Number of params": "4.3M", "GFLOPs": "1.2", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.5, "Top 5 Accuracy": null, "Number of params": 4300000.0, "GFLOPs": 1.2, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 778779, "title": "LocalViT: Bringing Locality to Vision Transformers", "url": "/paper/localvit-bringing-locality-to-vision", "published": "2021-04-12T00:00:00.000000", "code": true, "review_url": "/paper/localvit-bringing-locality-to-vision/review/?hl=29952"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 45004, "rank": 737, "method": "ResNet-18 (SAMix)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "SAMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-11-30", "metrics": {"Top 1 Accuracy": "72.33%", "Top 5 Accuracy": "91.80%", "Number of params": "11.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.33, "Top 5 Accuracy": 91.8, "Number of params": 11700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 923048, "title": "Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup", "url": "/paper/boosting-discriminative-visual-representation", "published": "2021-11-30T00:00:00.000000", "code": true, "review_url": "/paper/boosting-discriminative-visual-representation/review/?hl=45004"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 73068, "rank": 738, "method": "MobileViTv3-0.5", "mlmodel": {}, "method_short": "MobileViTv3-0.5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-30", "metrics": {"Top 1 Accuracy": "72.33%", "Top 5 Accuracy": null, "Number of params": "1.4 M", "GFLOPs": "0.5", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.33, "Top 5 Accuracy": null, "Number of params": 1.4, "GFLOPs": 0.5, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1084483, "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features", "url": "/paper/mobilevitv3-mobile-friendly-vision", "published": "2022-09-30T00:00:00.000000", "code": true, "review_url": "/paper/mobilevitv3-mobile-friendly-vision/review/?hl=73068"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 8686, "rank": 739, "method": "ResNet-50", "mlmodel": {}, "method_short": "ResNet-50", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-10-09", "metrics": {"Top 1 Accuracy": "72.1%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.1, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 157871, "title": "On the adequacy of untuned warmup for adaptive optimization", "url": "/paper/on-the-adequacy-of-untuned-warmup-for", "published": "2019-10-09T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 44998, "rank": 740, "method": "ResNet-18 (AutoMix)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "AutoMix", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-24", "metrics": {"Top 1 Accuracy": "72.05%", "Top 5 Accuracy": "91.75", "Number of params": "11.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": "11.7M"}, "raw_metrics": {"Top 1 Accuracy": 72.05, "Top 5 Accuracy": 91.75, "Number of params": 11700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": 11700000.0}, "uses_additional_data": false, "paper": {"id": 756879, "title": "AutoMix: Unveiling the Power of Mixup for Stronger Classifiers", "url": "/paper/automix-unveiling-the-power-of-mixup", "published": "2021-03-24T00:00:00.000000", "code": true, "review_url": "/paper/automix-unveiling-the-power-of-mixup/review/?hl=44998"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 60597, "rank": 741, "method": "MobileNetV2", "mlmodel": {}, "method_short": "MobileNetV2", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2018-01-13", "metrics": {"Top 1 Accuracy": "72%", "Top 5 Accuracy": null, "Number of params": "3.4M", "GFLOPs": "0.600", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 72.0, "Top 5 Accuracy": null, "Number of params": 3400000.0, "GFLOPs": 0.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 6891, "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "url": "/paper/mobilenetv2-inverted-residuals-and-linear", "published": "2018-01-13T00:00:00.000000", "code": true, "review_url": "/paper/mobilenetv2-inverted-residuals-and-linear/review/?hl=60597"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 64242, "rank": 742, "method": "Ours", "mlmodel": {}, "method_short": "Ours", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-09-10", "metrics": {"Top 1 Accuracy": "71.97%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.97, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 217427, "title": "QuantNet: Learning to Quantize by Learning within Fully Differentiable Framework", "url": "/paper/quantnet-learning-to-quantize-by-learning", "published": "2020-09-10T00:00:00.000000", "code": false, "review_url": "/paper/quantnet-learning-to-quantize-by-learning/review/?hl=64242"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22004, "rank": 743, "method": "ResNet-18 (PAD-L2 w/ ResNet-34 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "PAD-L2 w/ ResNet-34 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "71.71%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.71, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=22004"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 16114, "rank": 744, "method": "MUXNet-s", "mlmodel": {}, "method_short": "MUXNet-s", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-31", "metrics": {"Top 1 Accuracy": "71.6%", "Top 5 Accuracy": "90.3", "Number of params": "2.4M", "GFLOPs": "0.234", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.6, "Top 5 Accuracy": 90.3, "Number of params": 2400000.0, "GFLOPs": 0.234, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188988, "title": "MUXConv: Information Multiplexing in Convolutional Neural Networks", "url": "/paper/muxconv-information-multiplexing-in", "published": "2020-03-31T00:00:00.000000", "code": true, "review_url": "/paper/muxconv-information-multiplexing-in/review/?hl=16114"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 64250, "rank": 745, "method": "PDC", "mlmodel": {}, "method_short": "PDC", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-16", "metrics": {"Top 1 Accuracy": "71.6%", "Top 5 Accuracy": null, "Number of params": "11.51M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.6, "Top 5 Accuracy": null, "Number of params": 11510000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 783593, "title": "Augmenting Deep Classifiers with Polynomial Neural Networks", "url": "/paper/polynomial-networks-in-deep-classifiers", "published": "2021-04-16T00:00:00.000000", "code": true, "review_url": "/paper/polynomial-networks-in-deep-classifiers/review/?hl=64250"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 21999, "rank": 746, "method": "ResNet-18 (FT w/ ResNet-34 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "FT w/ ResNet-34 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "71.56%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.56, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=21999"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 21997, "rank": 747, "method": "ResNet-18 (KD w/ ResNet-34 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "KD w/ ResNet-34 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "71.37%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.37, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=21997"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 57375, "rank": 748, "method": "EdgeNeXt-XXS", "mlmodel": {}, "method_short": "EdgeNeXt-XXS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-06-21", "metrics": {"Top 1 Accuracy": "71.2%", "Top 5 Accuracy": null, "Number of params": "1.3M", "GFLOPs": "0.522", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.2, "Top 5 Accuracy": null, "Number of params": 1300000.0, "GFLOPs": 0.522, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1029687, "title": "EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications", "url": "/paper/edgenext-efficiently-amalgamated-cnn", "published": "2022-06-21T00:00:00.000000", "code": true, "review_url": "/paper/edgenext-efficiently-amalgamated-cnn/review/?hl=57375"}, "external_source_url": null, "tags": [{"id": 4, "name": "Transformer", "color": "#0037CC"}, {"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 252, "name": "CrossCovarianceAttention", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 22003, "rank": 749, "method": "ResNet-18 (L2 w/ ResNet-34 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "L2 w/ ResNet-34 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "71.08%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 71.08, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=22003"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 73071, "rank": 750, "method": "MobileViTv3-XXS", "mlmodel": {}, "method_short": "MobileViTv3-XXS", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-30", "metrics": {"Top 1 Accuracy": "70.98%", "Top 5 Accuracy": null, "Number of params": "1.2 M", "GFLOPs": "0.3", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.98, "Top 5 Accuracy": null, "Number of params": 1.2, "GFLOPs": 0.3, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1084483, "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features", "url": "/paper/mobilevitv3-mobile-friendly-vision", "published": "2022-09-30T00:00:00.000000", "code": true, "review_url": "/paper/mobilevitv3-mobile-friendly-vision/review/?hl=73071"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 22000, "rank": 751, "method": "ResNet-18 (CRD w/ ResNet-34 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "CRD w/ ResNet-34 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "70.93%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.93, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=22000"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 2058, "rank": 752, "method": "ShuffleNet", "mlmodel": {}, "method_short": "ShuffleNet", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2017-07-04", "metrics": {"Top 1 Accuracy": "70.9%", "Top 5 Accuracy": "89.8%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.9, "Top 5 Accuracy": 89.8, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 13326, "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices", "url": "/paper/shufflenet-an-extremely-efficient", "published": "2017-07-04T00:00:00.000000", "code": true, "review_url": "/paper/shufflenet-an-extremely-efficient/review/?hl=2058"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 61431, "rank": 753, "method": "MobileNet-224 \u00d71.0", "mlmodel": {}, "method_short": "MobileNet-224 \u00d71.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2017-04-17", "metrics": {"Top 1 Accuracy": "70.6%", "Top 5 Accuracy": null, "Number of params": "4.2M", "GFLOPs": "1.138", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.6, "Top 5 Accuracy": null, "Number of params": 4200000.0, "GFLOPs": 1.138, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 24007, "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications", "url": "/paper/mobilenets-efficient-convolutional-neural", "published": "2017-04-17T00:00:00.000000", "code": true, "review_url": "/paper/mobilenets-efficient-convolutional-neural/review/?hl=61431"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22018, "rank": 754, "method": "ResNet-18 (tf-KD w/ ResNet-18 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "tf-KD w/ ResNet-18 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "70.52%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.52, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=22018"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 36909, "rank": 755, "method": "PVTv2-B0", "mlmodel": {}, "method_short": "PVTv2-B0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-06-25", "metrics": {"Top 1 Accuracy": "70.5%", "Top 5 Accuracy": null, "Number of params": "3.4M", "GFLOPs": "0.6", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.5, "Top 5 Accuracy": null, "Number of params": 3400000.0, "GFLOPs": 0.6, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 825108, "title": "PVT v2: Improved Baselines with Pyramid Vision Transformer", "url": "/paper/pvtv2-improved-baselines-with-pyramid-vision", "published": "2021-06-25T00:00:00.000000", "code": true, "review_url": "/paper/pvtv2-improved-baselines-with-pyramid-vision/review/?hl=36909"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6141, "rank": 756, "method": "MSRA", "mlmodel": {}, "method_short": "MSRA", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2014-06-18", "metrics": {"Top 1 Accuracy": "70.32%", "Top 5 Accuracy": "89.05", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.32, "Top 5 Accuracy": 89.05, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 41081, "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "url": "/paper/spatial-pyramid-pooling-in-deep-convolutional", "published": "2014-06-18T00:00:00.000000", "code": true, "review_url": "/paper/spatial-pyramid-pooling-in-deep-convolutional/review/?hl=6141"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 22002, "rank": 757, "method": "ResNet-18 (SSKD w/ ResNet-34 teacher)", "mlmodel": {}, "method_short": "ResNet-18 ", "method_details": "SSKD w/ ResNet-34 teacher", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-11-25", "metrics": {"Top 1 Accuracy": "70.09%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 70.09, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 237343, "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation", "url": "/paper/torchdistill-a-modular-configuration-driven", "published": "2020-11-25T00:00:00.000000", "code": true, "review_url": "/paper/torchdistill-a-modular-configuration-driven/review/?hl=22002"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 19548, "rank": 758, "method": "DY-MobileNetV3-Small", "mlmodel": {}, "method_short": "DY-MobileNetV3-Small", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "69.7%", "Top 5 Accuracy": null, "Number of params": "4.8M", "GFLOPs": "0.137", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 69.7, "Top 5 Accuracy": null, "Number of params": 4800000.0, "GFLOPs": 0.137, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19548"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 28356, "rank": 759, "method": "HVT-Ti-1", "mlmodel": {}, "method_short": "HVT-Ti-1", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-19", "metrics": {"Top 1 Accuracy": "69.64%", "Top 5 Accuracy": "89.4", "Number of params": "5.74M", "GFLOPs": "0.64", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 69.64, "Top 5 Accuracy": 89.4, "Number of params": 5740000.0, "GFLOPs": 0.64, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 755458, "title": "Scalable Vision Transformers with Hierarchical Pooling", "url": "/paper/scalable-visual-transformers-with", "published": "2021-03-19T00:00:00.000000", "code": true, "review_url": "/paper/scalable-visual-transformers-with/review/?hl=28356"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 19546, "rank": 760, "method": "DY-MobileNetV2 \u00d70.5", "mlmodel": {}, "method_short": "DY-MobileNetV2 \u00d70.5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "69.4%", "Top 5 Accuracy": null, "Number of params": "4M", "GFLOPs": "0.203", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 69.4, "Top 5 Accuracy": null, "Number of params": 4000000.0, "GFLOPs": 0.203, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19546"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60690, "rank": 761, "method": "AsymmNet-Large \u00d70.5", "mlmodel": {}, "method_short": "AsymmNet-Large \u00d70.5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-15", "metrics": {"Top 1 Accuracy": "69.2%", "Top 5 Accuracy": null, "Number of params": "2.8M", "GFLOPs": "0.1344", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 69.2, "Top 5 Accuracy": null, "Number of params": 2800000.0, "GFLOPs": 0.1344, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 783606, "title": "AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks", "url": "/paper/asymmnet-towards-ultralight-convolution", "published": "2021-04-15T00:00:00.000000", "code": true, "review_url": "/paper/asymmnet-towards-ultralight-convolution/review/?hl=60690"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 33776, "rank": 762, "method": "Heteroscedastic (InceptionResNet-v2)", "mlmodel": {}, "method_short": "Heteroscedastic ", "method_details": "InceptionResNet-v2", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-05-19", "metrics": {"Top 1 Accuracy": "68.6%", "Top 5 Accuracy": "87.1%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 68.6, "Top 5 Accuracy": 87.1, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 803513, "title": "Correlated Input-Dependent Label Noise in Large-Scale Image Classification", "url": "/paper/correlated-input-dependent-label-noise-in", "published": "2021-05-19T00:00:00.000000", "code": false, "review_url": "/paper/correlated-input-dependent-label-noise-in/review/?hl=33776"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60689, "rank": 763, "method": "AsymmNet-Small \u00d71.0", "mlmodel": {}, "method_short": "AsymmNet-Small \u00d71.0", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-15", "metrics": {"Top 1 Accuracy": "68.4%", "Top 5 Accuracy": null, "Number of params": "3.1M", "GFLOPs": "0.1154", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 68.4, "Top 5 Accuracy": null, "Number of params": 3100000.0, "GFLOPs": 0.1154, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 783606, "title": "AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks", "url": "/paper/asymmnet-towards-ultralight-convolution", "published": "2021-04-15T00:00:00.000000", "code": true, "review_url": "/paper/asymmnet-towards-ultralight-convolution/review/?hl=60689"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 54265, "rank": 764, "method": "FireCaffe (GoogLeNet)", "mlmodel": {}, "method_short": "FireCaffe ", "method_details": "GoogLeNet", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-10-31", "metrics": {"Top 1 Accuracy": "68.3%", "Top 5 Accuracy": "88.7%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 68.3, "Top 5 Accuracy": 88.7, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 36686, "title": "FireCaffe: near-linear acceleration of deep neural network training on compute clusters", "url": "/paper/firecaffe-near-linear-acceleration-of-deep", "published": "2015-10-31T00:00:00.000000", "code": false, "review_url": "/paper/firecaffe-near-linear-acceleration-of-deep/review/?hl=54265"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11224, "rank": 765, "method": "Graph-RISE (40M)", "mlmodel": {}, "method_short": "Graph-RISE ", "method_details": "40M", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-02-14", "metrics": {"Top 1 Accuracy": "68.29%", "Top 5 Accuracy": "87.75%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 68.29, "Top 5 Accuracy": 87.75, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 107335, "title": "Graph-RISE: Graph-Regularized Image Semantic Embedding", "url": "/paper/graph-rise-graph-regularized-image-semantic", "published": "2019-02-14T00:00:00.000000", "code": true, "review_url": "/paper/graph-rise-graph-regularized-image-semantic/review/?hl=11224"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 30305, "rank": 766, "method": "ReActNet-A (BN-Free)", "mlmodel": {}, "method_short": "ReActNet-A ", "method_details": "BN-Free", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-16", "metrics": {"Top 1 Accuracy": "68.0%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 68.0, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 783568, "title": "\"BNN - BN = ?\": Training Binary Neural Networks without Batch Normalization", "url": "/paper/bnn-bn-training-binary-neural-networks", "published": "2021-04-16T00:00:00.000000", "code": true, "review_url": "/paper/bnn-bn-training-binary-neural-networks/review/?hl=30305"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 19550, "rank": 767, "method": "DY-ResNet-10", "mlmodel": {}, "method_short": "DY-ResNet-10", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "67.7%", "Top 5 Accuracy": null, "Number of params": "18.6M", "GFLOPs": "1.82", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 67.7, "Top 5 Accuracy": null, "Number of params": 18600000.0, "GFLOPs": 1.82, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19550"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 71858, "rank": 768, "method": "WaveMixLite-256/24", "mlmodel": {}, "method_short": "WaveMixLite-256/24", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-10-13", "metrics": {"Top 1 Accuracy": "67.7%", "Top 5 Accuracy": null, "Number of params": "32.4M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 67.7, "Top 5 Accuracy": null, "Number of params": 32400000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1093361, "title": "WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis", "url": "/paper/wavemix-lite-a-resource-efficient-neural-1", "published": "2022-10-13T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 16113, "rank": 769, "method": "MUXNet-xs", "mlmodel": {}, "method_short": "MUXNet-xs", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2020-03-31", "metrics": {"Top 1 Accuracy": "66.7%", "Top 5 Accuracy": "86.8", "Number of params": "1.8M", "GFLOPs": "0.132", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 66.7, "Top 5 Accuracy": 86.8, "Number of params": 1800000.0, "GFLOPs": 0.132, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 188988, "title": "MUXConv: Information Multiplexing in Convolutional Neural Networks", "url": "/paper/muxconv-information-multiplexing-in", "published": "2020-03-31T00:00:00.000000", "code": true, "review_url": "/paper/muxconv-information-multiplexing-in/review/?hl=16113"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 31362, "rank": 770, "method": "PAWS (ResNet-50, 1% labels)", "mlmodel": {}, "method_short": "PAWS ", "method_details": "ResNet-50, 1% labels", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-04-28", "metrics": {"Top 1 Accuracy": "66.5%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 66.5, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 788977, "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples", "url": "/paper/semi-supervised-learning-of-visual-features", "published": "2021-04-28T00:00:00.000000", "code": true, "review_url": "/paper/semi-supervised-learning-of-visual-features/review/?hl=31362"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 6143, "rank": 771, "method": "Five Base + Five HiRes", "mlmodel": {}, "method_short": "Five Base + Five HiRes", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2013-12-19", "metrics": {"Top 1 Accuracy": "66.3%", "Top 5 Accuracy": "86.3%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 66.3, "Top 5 Accuracy": 86.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 47267, "title": "Some Improvements on Deep Convolutional Neural Network Based Image Classification", "url": "/paper/some-improvements-on-deep-convolutional", "published": "2013-12-19T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 60594, "rank": 772, "method": "GhostNet \u00d70.5", "mlmodel": {}, "method_short": "GhostNet \u00d70.5", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-27", "metrics": {"Top 1 Accuracy": "66.2%", "Top 5 Accuracy": "86.6", "Number of params": "2.6M", "GFLOPs": "0.042", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 66.2, "Top 5 Accuracy": 86.6, "Number of params": 2600000.0, "GFLOPs": 0.042, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 174553, "title": "GhostNet: More Features from Cheap Operations", "url": "/paper/ghostnet-more-features-from-cheap-operations", "published": "2019-11-27T00:00:00.000000", "code": true, "review_url": "/paper/ghostnet-more-features-from-cheap-operations/review/?hl=60594"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 6144, "rank": 773, "method": "OverFeat - 7 accurate models", "mlmodel": {}, "method_short": "OverFeat - 7 accurate models", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2013-12-21", "metrics": {"Top 1 Accuracy": "66.04%", "Top 5 Accuracy": "86.76", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 66.04, "Top 5 Accuracy": 86.76, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 46358, "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "url": "/paper/overfeat-integrated-recognition-localization", "published": "2013-12-21T00:00:00.000000", "code": true, "review_url": "/paper/overfeat-integrated-recognition-localization/review/?hl=6144"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 19547, "rank": 774, "method": "DY-MobileNetV2 \u00d70.35", "mlmodel": {}, "method_short": "DY-MobileNetV2 \u00d70.35", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-12-07", "metrics": {"Top 1 Accuracy": "64.9%", "Top 5 Accuracy": null, "Number of params": "2.8M", "GFLOPs": "0.124", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 64.9, "Top 5 Accuracy": null, "Number of params": 2800000.0, "GFLOPs": 0.124, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 175971, "title": "Dynamic Convolution: Attention over Convolution Kernels", "url": "/paper/dynamic-convolution-attention-over", "published": "2019-12-07T00:00:00.000000", "code": true, "review_url": "/paper/dynamic-convolution-attention-over/review/?hl=19547"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6132, "rank": 775, "method": "ZFNet (ensemble, 6 convnets)", "mlmodel": {}, "method_short": "ZFNet ", "method_details": "ensemble, 6 convnets", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2013-11-12", "metrics": {"Top 1 Accuracy": "64%", "Top 5 Accuracy": "85.3", "Number of params": null, "GFLOPs": null, "Hardware Burden": "2G", "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 64.0, "Top 5 Accuracy": 85.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": 0, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 47460, "title": "Visualizing and Understanding Convolutional Networks", "url": "/paper/visualizing-and-understanding-convolutional", "published": "2013-11-12T00:00:00.000000", "code": true, "review_url": "/paper/visualizing-and-understanding-convolutional/review/?hl=6132"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 6133, "rank": 776, "method": "AlexNet", "mlmodel": {}, "method_short": "AlexNet", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2012-12-01", "metrics": {"Top 1 Accuracy": "63.3%", "Top 5 Accuracy": "84.6%", "Number of params": "60M", "GFLOPs": null, "Hardware Burden": "2G", "Operations per network pass": "0.07G", "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 63.3, "Top 5 Accuracy": 84.6, "Number of params": 60000000.0, "GFLOPs": null, "Hardware Burden": 0, "Operations per network pass": 0.07, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 79567, "title": "ImageNet Classification with Deep Convolutional Neural Networks", "url": "/paper/imagenet-classification-with-deep", "published": "2012-12-01T00:00:00.000000", "code": true, "review_url": null}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11225, "rank": 777, "method": "BBG (ResNet-34)", "mlmodel": {}, "method_short": "BBG ", "method_details": "ResNet-34", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-09-26", "metrics": {"Top 1 Accuracy": "62.6%", "Top 5 Accuracy": "84.1%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 62.6, "Top 5 Accuracy": 84.1, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 156101, "title": "Balanced Binary Neural Networks with Gated Residual", "url": "/paper/balanced-binary-neural-networks-with-gated", "published": "2019-09-26T00:00:00.000000", "code": true, "review_url": "/paper/balanced-binary-neural-networks-with-gated/review/?hl=11225"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 8806, "rank": 778, "method": "ZFNet (1 convnet, 512,1024,512 maps)", "mlmodel": {}, "method_short": "ZFNet ", "method_details": "1 convnet, 512,1024,512 maps", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2013-11-12", "metrics": {"Top 1 Accuracy": "62.5%", "Top 5 Accuracy": "84.0", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 62.5, "Top 5 Accuracy": 84.0, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 47460, "title": "Visualizing and Understanding Convolutional Networks", "url": "/paper/visualizing-and-understanding-convolutional", "published": "2013-11-12T00:00:00.000000", "code": true, "review_url": "/paper/visualizing-and-understanding-convolutional/review/?hl=8806"}, "external_source_url": null, "tags": [{"id": 17, "name": "CNN", "color": "#2771D3"}, {"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 54278, "rank": 779, "method": "SqueezeNet + Simple Bypass", "mlmodel": {}, "method_short": "SqueezeNet + Simple Bypass", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2016-02-24", "metrics": {"Top 1 Accuracy": "60.4%", "Top 5 Accuracy": "82.5%", "Number of params": "1.24M", "GFLOPs": "1.432", "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 60.4, "Top 5 Accuracy": 82.5, "Number of params": 1240000.0, "GFLOPs": 1.432, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 29015, "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003C0.5MB model size", "url": "/paper/squeezenet-alexnet-level-accuracy-with-50x", "published": "2016-02-24T00:00:00.000000", "code": true, "review_url": "/paper/squeezenet-alexnet-level-accuracy-with-50x/review/?hl=54278"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 11226, "rank": 780, "method": "BBG (ResNet-18)", "mlmodel": {}, "method_short": "BBG ", "method_details": "ResNet-18", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-09-26", "metrics": {"Top 1 Accuracy": "59.4%", "Top 5 Accuracy": "81.3%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 59.4, "Top 5 Accuracy": 81.3, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 156101, "title": "Balanced Binary Neural Networks with Gated Residual", "url": "/paper/balanced-binary-neural-networks-with-gated", "published": "2019-09-26T00:00:00.000000", "code": true, "review_url": "/paper/balanced-binary-neural-networks-with-gated/review/?hl=11226"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 54264, "rank": 781, "method": "FireCaffe (AlexNet)", "mlmodel": {}, "method_short": "FireCaffe ", "method_details": "AlexNet", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2015-10-31", "metrics": {"Top 1 Accuracy": "58.9%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 58.9, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 36686, "title": "FireCaffe: near-linear acceleration of deep neural network training on compute clusters", "url": "/paper/firecaffe-near-linear-acceleration-of-deep", "published": "2015-10-31T00:00:00.000000", "code": false, "review_url": "/paper/firecaffe-near-linear-acceleration-of-deep/review/?hl=54264"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}], "reports": []}, {"table_id": 116, "row_id": 79143, "rank": 782, "method": "ResNet-50 (PuzzleMix+DM)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "PuzzleMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "22.29%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 22.29, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79143"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 79144, "rank": 783, "method": "ResNet-50 (AutoMix+DM)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "AutoMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "21.85%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 21.85, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79144"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 79145, "rank": 784, "method": "ResNet-50 (SAMix+DM)", "mlmodel": {}, "method_short": "ResNet-50 ", "method_details": "SAMix+DM", "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-03-21", "metrics": {"Top 1 Accuracy": "21.64%", "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": 21.64, "Top 5 Accuracy": null, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 979854, "title": "Decoupled Mixup for Data-efficient Learning", "url": "/paper/decoupled-mixup-for-data-efficient-learning", "published": "2022-03-21T00:00:00.000000", "code": true, "review_url": "/paper/decoupled-mixup-for-data-efficient-learning/review/?hl=79145"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 64245, "rank": 785, "method": "IFQ-AlexNet", "mlmodel": {}, "method_short": "IFQ-AlexNet", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2019-11-19", "metrics": {"Top 1 Accuracy": null, "Top 5 Accuracy": "78.0%", "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": null, "Top 5 Accuracy": 78.0, "Number of params": null, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 173419, "title": "IFQ-Net: Integrated Fixed-point Quantization Networks for Embedded Vision", "url": "/paper/ifq-net-integrated-fixed-point-quantization", "published": "2019-11-19T00:00:00.000000", "code": false, "review_url": "/paper/ifq-net-integrated-fixed-point-quantization/review/?hl=64245"}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 29120, "rank": 786, "method": "ViL-Tiny", "mlmodel": {}, "method_short": "ViL-Tiny", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2021-03-29", "metrics": {"Top 1 Accuracy": null, "Top 5 Accuracy": null, "Number of params": "6.7M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": null, "Top 5 Accuracy": null, "Number of params": 6700000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 758437, "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding", "url": "/paper/2103-15358", "published": "2021-03-29T00:00:00.000000", "code": true, "review_url": "/paper/2103-15358/review/?hl=29120"}, "external_source_url": null, "tags": [{"id": 171, "name": "ImageNet-1k only", "color": "#ae27d3"}, {"id": 4, "name": "Transformer", "color": "#0037CC"}], "reports": []}, {"table_id": 116, "row_id": 70400, "rank": 787, "method": "PaLI", "mlmodel": {}, "method_short": "PaLI", "method_details": null, "mlmodel_short": null, "mlmodeldetails": null, "evaluation_date": "2022-09-14", "metrics": {"Top 1 Accuracy": null, "Top 5 Accuracy": null, "Number of params": "16900M", "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "raw_metrics": {"Top 1 Accuracy": null, "Top 5 Accuracy": null, "Number of params": 16900000000.0, "GFLOPs": null, "Hardware Burden": null, "Operations per network pass": null, "Number of parameters (M)": null}, "uses_additional_data": false, "paper": {"id": 1074922, "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "url": "/paper/pali-a-jointly-scaled-multilingual-language", "published": "2022-09-14T00:00:00.000000", "code": false, "review_url": "/paper/pali-a-jointly-scaled-multilingual-language/review/?hl=70400"}, "external_source_url": null, "tags": [], "reports": []}]</script>
    <script id="community-chart-data" type="application/json">{"all": {"yAxis": {"title": "Top 1 Accuracy", "includeZero": false, "gridColor": "#ddd", "valueFormatString": ""}, "data": {"trend": {"name": "State-of-the-art methods", "type": "line", "showInLegend": true, "markerSize": 10, "toolTipContent": "{name}: {y}", "color": "#21ccc7", "dataPoints": []}, "other": {"name": "Other methods", "type": "scatter", "showInLegend": true, "color": "#ddd", "markerSize": 10, "toolTipContent": "{name}: {y}", "dataPoints": []}}}, "uses_additional_data": {"yAxis": {"title": "Top 1 Accuracy", "includeZero": false, "gridColor": "#ddd", "valueFormatString": ""}, "data": {"trend": {"name": "State-of-the-art methods", "type": "line", "showInLegend": true, "markerSize": 10, "toolTipContent": "{name}: {y}", "color": "#21ccc7", "dataPoints": []}, "other": {"name": "Other methods", "type": "scatter", "showInLegend": true, "color": "#ddd", "markerSize": 10, "toolTipContent": "{name}: {y}", "dataPoints": []}}}, "no_additional_data": {"yAxis": {"title": "Top 1 Accuracy", "includeZero": false, "gridColor": "#ddd", "valueFormatString": ""}, "data": {"trend": {"name": "State-of-the-art methods", "type": "line", "showInLegend": true, "markerSize": 10, "toolTipContent": "{name}: {y}", "color": "#21ccc7", "dataPoints": []}, "other": {"name": "Other methods", "type": "scatter", "showInLegend": true, "color": "#ddd", "markerSize": 10, "toolTipContent": "{name}: {y}", "dataPoints": []}}}}</script>
    <script id="community-table-metrics" type="application/json">[{"id": 271, "name": "Top 1 Accuracy", "is_loss": false, "is_fixed": false}, {"id": 272, "name": "Top 5 Accuracy", "is_loss": false, "is_fixed": false}]</script>
    <script id="community-table-data" type="application/json">[{"table_id": 116, "row_id": 25911, "rank": 1, "method": "tf_efficientnet_l2_ns", "mlmodel": {"last_updated": "2021-02-14T14:19:43.873000+0000", "id": 89, "url": "/model/tf-efficientnet-l2-ns"}, "method_short": "tf_efficientnet_l2_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_l2_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "88.35%", "Top 5 Accuracy": "98.66%"}, "raw_metrics": {"Top 1 Accuracy": 88.35, "Top 5 Accuracy": 98.66}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25912, "rank": 2, "method": "tf_efficientnet_l2_ns_475", "mlmodel": {"last_updated": "2021-02-14T14:19:46.223000+0000", "id": 384, "url": "/model/tf-efficientnet-l2-ns-475"}, "method_short": "tf_efficientnet_l2_ns_475", "method_details": null, "mlmodel_short": "tf_efficientnet_l2_ns_475", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "88.24%", "Top 5 Accuracy": "98.55%"}, "raw_metrics": {"Top 1 Accuracy": 88.24, "Top 5 Accuracy": 98.55}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25902, "rank": 3, "method": "tf_efficientnet_b7_ns", "mlmodel": {"last_updated": "2021-02-14T14:19:26.209000+0000", "id": 367, "url": "/model/tf-efficientnet-b7-ns"}, "method_short": "tf_efficientnet_b7_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b7_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "86.83%", "Top 5 Accuracy": "98.08%"}, "raw_metrics": {"Top 1 Accuracy": 86.83, "Top 5 Accuracy": 98.08}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25899, "rank": 4, "method": "tf_efficientnet_b6_ns", "mlmodel": {"last_updated": "2021-02-14T14:19:18.036000+0000", "id": 120, "url": "/model/tf-efficientnet-b6-ns"}, "method_short": "tf_efficientnet_b6_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b6_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "86.45%", "Top 5 Accuracy": "97.88%"}, "raw_metrics": {"Top 1 Accuracy": 86.45, "Top 5 Accuracy": 97.88}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25896, "rank": 5, "method": "tf_efficientnet_b5_ns", "mlmodel": {"last_updated": "2021-02-14T14:19:10.819000+0000", "id": 115, "url": "/model/tf-efficientnet-b5-ns"}, "method_short": "tf_efficientnet_b5_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b5_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "86.08%", "Top 5 Accuracy": "97.75%"}, "raw_metrics": {"Top 1 Accuracy": 86.08, "Top 5 Accuracy": 97.75}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25763, "rank": 6, "method": "ig_resnext101_32x48d", "mlmodel": {"last_updated": "2021-02-14T14:13:35.032000+0000", "id": 244, "url": "/model/ig-resnext101-32x48d"}, "method_short": "ig_resnext101_32x48d", "method_details": null, "mlmodel_short": "ig_resnext101_32x48d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.42%", "Top 5 Accuracy": "97.58%"}, "raw_metrics": {"Top 1 Accuracy": 85.42, "Top 5 Accuracy": 97.58}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25904, "rank": 7, "method": "tf_efficientnet_b8_ap", "mlmodel": {"last_updated": "2021-02-14T14:19:30.937000+0000", "id": 364, "url": "/model/tf-efficientnet-b8-ap"}, "method_short": "tf_efficientnet_b8_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b8_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.37%", "Top 5 Accuracy": "97.3%"}, "raw_metrics": {"Top 1 Accuracy": 85.37, "Top 5 Accuracy": 97.3}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25903, "rank": 8, "method": "tf_efficientnet_b8", "mlmodel": {"last_updated": "2021-02-14T14:19:28.609000+0000", "id": 366, "url": "/model/tf-efficientnet-b8"}, "method_short": "tf_efficientnet_b8", "method_details": null, "mlmodel_short": "tf_efficientnet_b8", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.35%", "Top 5 Accuracy": "97.39%"}, "raw_metrics": {"Top 1 Accuracy": 85.35, "Top 5 Accuracy": 97.39}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25945, "rank": 9, "method": "vit_large_patch16_384", "mlmodel": {"last_updated": "2021-02-14T14:20:52.667000+0000", "id": 282, "url": "/model/vit-large-patch16-384"}, "method_short": "vit_large_patch16_384", "method_details": null, "mlmodel_short": "vit_large_patch16_384", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.17%", "Top 5 Accuracy": "97.36%"}, "raw_metrics": {"Top 1 Accuracy": 85.17, "Top 5 Accuracy": 97.36}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25893, "rank": 10, "method": "tf_efficientnet_b4_ns", "mlmodel": {"last_updated": "2021-02-14T14:19:03.813000+0000", "id": 299, "url": "/model/tf-efficientnet-b4-ns"}, "method_short": "tf_efficientnet_b4_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b4_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.15%", "Top 5 Accuracy": "97.47%"}, "raw_metrics": {"Top 1 Accuracy": 85.15, "Top 5 Accuracy": 97.47}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25901, "rank": 11, "method": "tf_efficientnet_b7_ap", "mlmodel": {"last_updated": "2021-02-14T14:19:23.290000+0000", "id": 360, "url": "/model/tf-efficientnet-b7-ap"}, "method_short": "tf_efficientnet_b7_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b7_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.12%", "Top 5 Accuracy": "97.25%"}, "raw_metrics": {"Top 1 Accuracy": 85.12, "Top 5 Accuracy": 97.25}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25762, "rank": 12, "method": "ig_resnext101_32x32d", "mlmodel": {"last_updated": "2021-02-14T14:13:32.128000+0000", "id": 234, "url": "/model/ig-resnext101-32x32d"}, "method_short": "ig_resnext101_32x32d", "method_details": null, "mlmodel_short": "ig_resnext101_32x32d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "85.09%", "Top 5 Accuracy": "97.44%"}, "raw_metrics": {"Top 1 Accuracy": 85.09, "Top 5 Accuracy": 97.44}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25943, "rank": 13, "method": "vit_base_resnet50_384", "mlmodel": {"last_updated": "2021-02-14T14:20:49.646000+0000", "id": 286, "url": "/model/vit-base-resnet50-384"}, "method_short": "vit_base_resnet50_384", "method_details": null, "mlmodel_short": "vit_base_resnet50_384", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.99%", "Top 5 Accuracy": "97.3%"}, "raw_metrics": {"Top 1 Accuracy": 84.99, "Top 5 Accuracy": 97.3}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25844, "rank": 14, "method": "resnetv2_152x4_bitm", "mlmodel": {"last_updated": "2021-02-14T14:17:02.644000+0000", "id": 137, "url": "/model/resnetv2-152x4-bitm"}, "method_short": "resnetv2_152x4_bitm", "method_details": null, "mlmodel_short": "resnetv2_152x4_bitm", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.95%", "Top 5 Accuracy": "97.45%"}, "raw_metrics": {"Top 1 Accuracy": 84.95, "Top 5 Accuracy": 97.45}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25900, "rank": 15, "method": "tf_efficientnet_b7", "mlmodel": {"last_updated": "2021-02-14T14:19:20.463000+0000", "id": 358, "url": "/model/tf-efficientnet-b7"}, "method_short": "tf_efficientnet_b7", "method_details": null, "mlmodel_short": "tf_efficientnet_b7", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.93%", "Top 5 Accuracy": "97.2%"}, "raw_metrics": {"Top 1 Accuracy": 84.93, "Top 5 Accuracy": 97.2}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25898, "rank": 16, "method": "tf_efficientnet_b6_ap", "mlmodel": {"last_updated": "2021-02-14T14:19:15.669000+0000", "id": 350, "url": "/model/tf-efficientnet-b6-ap"}, "method_short": "tf_efficientnet_b6_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b6_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.79%", "Top 5 Accuracy": "97.14%"}, "raw_metrics": {"Top 1 Accuracy": 84.79, "Top 5 Accuracy": 97.14}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25824, "rank": 17, "method": "resnest269e", "mlmodel": {"last_updated": "2021-02-14T14:16:10.243000+0000", "id": 347, "url": "/model/resnest269e"}, "method_short": "resnest269e", "method_details": null, "mlmodel_short": "resnest269e", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.53%", "Top 5 Accuracy": "96.99%"}, "raw_metrics": {"Top 1 Accuracy": 84.53, "Top 5 Accuracy": 96.99}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25843, "rank": 18, "method": "resnetv2_152x2_bitm", "mlmodel": {"last_updated": "2021-02-14T14:17:01.117000+0000", "id": 224, "url": "/model/resnetv2-152x2-bitm"}, "method_short": "resnetv2_152x2_bitm", "method_details": null, "mlmodel_short": "resnetv2_152x2_bitm", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.4%", "Top 5 Accuracy": "97.43%"}, "raw_metrics": {"Top 1 Accuracy": 84.4, "Top 5 Accuracy": 97.43}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25842, "rank": 19, "method": "resnetv2_101x3_bitm", "mlmodel": {"last_updated": "2021-02-14T14:16:59.730000+0000", "id": 179, "url": "/model/resnetv2-101x3-bitm"}, "method_short": "resnetv2_101x3_bitm", "method_details": null, "mlmodel_short": "resnetv2_101x3_bitm", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.38%", "Top 5 Accuracy": "97.37%"}, "raw_metrics": {"Top 1 Accuracy": 84.38, "Top 5 Accuracy": 97.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25877, "rank": 20, "method": "swsl_resnext101_32x8d", "mlmodel": {"last_updated": "2021-02-14T14:18:25.359000+0000", "id": 374, "url": "/model/swsl-resnext101-32x8d"}, "method_short": "swsl_resnext101_32x8d", "method_details": null, "mlmodel_short": "swsl_resnext101_32x8d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.27%", "Top 5 Accuracy": "97.17%"}, "raw_metrics": {"Top 1 Accuracy": 84.27, "Top 5 Accuracy": 97.17}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25895, "rank": 21, "method": "tf_efficientnet_b5_ap", "mlmodel": {"last_updated": "2021-02-14T14:19:08.657000+0000", "id": 310, "url": "/model/tf-efficientnet-b5-ap"}, "method_short": "tf_efficientnet_b5_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b5_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.25%", "Top 5 Accuracy": "96.97%"}, "raw_metrics": {"Top 1 Accuracy": 84.25, "Top 5 Accuracy": 96.97}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25941, "rank": 22, "method": "vit_base_patch16_384", "mlmodel": {"last_updated": "2021-02-14T14:20:46.673000+0000", "id": 275, "url": "/model/vit-base-patch16-384"}, "method_short": "vit_base_patch16_384", "method_details": null, "mlmodel_short": "vit_base_patch16_384", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.2%", "Top 5 Accuracy": "97.22%"}, "raw_metrics": {"Top 1 Accuracy": 84.2, "Top 5 Accuracy": 97.22}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25761, "rank": 23, "method": "ig_resnext101_32x16d", "mlmodel": {"last_updated": "2021-02-14T14:13:29.510000+0000", "id": 226, "url": "/model/ig-resnext101-32x16d"}, "method_short": "ig_resnext101_32x16d", "method_details": null, "mlmodel_short": "ig_resnext101_32x16d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.16%", "Top 5 Accuracy": "97.19%"}, "raw_metrics": {"Top 1 Accuracy": 84.16, "Top 5 Accuracy": 97.19}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25897, "rank": 24, "method": "tf_efficientnet_b6", "mlmodel": {"last_updated": "2021-02-14T14:19:13.158000+0000", "id": 338, "url": "/model/tf-efficientnet-b6"}, "method_short": "tf_efficientnet_b6", "method_details": null, "mlmodel_short": "tf_efficientnet_b6", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.11%", "Top 5 Accuracy": "96.89%"}, "raw_metrics": {"Top 1 Accuracy": 84.11, "Top 5 Accuracy": 96.89}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25890, "rank": 25, "method": "tf_efficientnet_b3_ns", "mlmodel": {"last_updated": "2021-02-14T14:18:56.928000+0000", "id": 110, "url": "/model/tf-efficientnet-b3-ns"}, "method_short": "tf_efficientnet_b3_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b3_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "84.04%", "Top 5 Accuracy": "96.91%"}, "raw_metrics": {"Top 1 Accuracy": 84.04, "Top 5 Accuracy": 96.91}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25823, "rank": 26, "method": "resnest200e", "mlmodel": {"last_updated": "2021-02-14T14:16:07.312000+0000", "id": 178, "url": "/model/resnest200e"}, "method_short": "resnest200e", "method_details": null, "mlmodel_short": "resnest200e", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.85%", "Top 5 Accuracy": "96.89%"}, "raw_metrics": {"Top 1 Accuracy": 83.85, "Top 5 Accuracy": 96.89}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25894, "rank": 27, "method": "tf_efficientnet_b5", "mlmodel": {"last_updated": "2021-02-14T14:19:06.100000+0000", "id": 304, "url": "/model/tf-efficientnet-b5"}, "method_short": "tf_efficientnet_b5", "method_details": null, "mlmodel_short": "tf_efficientnet_b5", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.81%", "Top 5 Accuracy": "96.75%"}, "raw_metrics": {"Top 1 Accuracy": 83.81, "Top 5 Accuracy": 96.75}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25846, "rank": 28, "method": "resnetv2_50x3_bitm", "mlmodel": {"last_updated": "2021-02-14T14:17:05.636000+0000", "id": 184, "url": "/model/resnetv2-50x3-bitm"}, "method_short": "resnetv2_50x3_bitm", "method_details": null, "mlmodel_short": "resnetv2_50x3_bitm", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.75%", "Top 5 Accuracy": "97.12%"}, "raw_metrics": {"Top 1 Accuracy": 83.75, "Top 5 Accuracy": 97.12}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25858, "rank": 29, "method": "seresnet152d", "mlmodel": {"last_updated": "2021-02-14T14:17:34.022000+0000", "id": 307, "url": "/model/seresnet152d"}, "method_short": "seresnet152d", "method_details": null, "mlmodel_short": "seresnet152d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.74%", "Top 5 Accuracy": "96.77%"}, "raw_metrics": {"Top 1 Accuracy": 83.74, "Top 5 Accuracy": 96.77}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25875, "rank": 30, "method": "swsl_resnext101_32x16d", "mlmodel": {"last_updated": "2021-02-14T14:18:19.587000+0000", "id": 371, "url": "/model/swsl-resnext101-32x16d"}, "method_short": "swsl_resnext101_32x16d", "method_details": null, "mlmodel_short": "swsl_resnext101_32x16d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.34%", "Top 5 Accuracy": "96.84%"}, "raw_metrics": {"Top 1 Accuracy": 83.34, "Top 5 Accuracy": 96.84}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25892, "rank": 31, "method": "tf_efficientnet_b4_ap", "mlmodel": {"last_updated": "2021-02-14T14:19:01.623000+0000", "id": 289, "url": "/model/tf-efficientnet-b4-ap"}, "method_short": "tf_efficientnet_b4_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b4_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.26%", "Top 5 Accuracy": "96.39%"}, "raw_metrics": {"Top 1 Accuracy": 83.26, "Top 5 Accuracy": 96.39}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25833, "rank": 32, "method": "resnet200d", "mlmodel": {"last_updated": "2021-02-14T14:16:38.311000+0000", "id": 181, "url": "/model/resnet200d"}, "method_short": "resnet200d", "method_details": null, "mlmodel_short": "resnet200d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.24%", "Top 5 Accuracy": "96.49%"}, "raw_metrics": {"Top 1 Accuracy": 83.24, "Top 5 Accuracy": 96.49}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25876, "rank": 33, "method": "swsl_resnext101_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:18:22.346000+0000", "id": 373, "url": "/model/swsl-resnext101-32x4d"}, "method_short": "swsl_resnext101_32x4d", "method_details": null, "mlmodel_short": "swsl_resnext101_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.22%", "Top 5 Accuracy": "96.77%"}, "raw_metrics": {"Top 1 Accuracy": 83.22, "Top 5 Accuracy": 96.77}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25830, "rank": 34, "method": "resnet152d", "mlmodel": {"last_updated": "2021-02-14T14:16:32.446000+0000", "id": 325, "url": "/model/resnet152d"}, "method_short": "resnet152d", "method_details": null, "mlmodel_short": "resnet152d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.13%", "Top 5 Accuracy": "96.35%"}, "raw_metrics": {"Top 1 Accuracy": 83.13, "Top 5 Accuracy": 96.35}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25944, "rank": 35, "method": "vit_large_patch16_224", "mlmodel": {"last_updated": "2021-02-14T14:20:51.131000+0000", "id": 276, "url": "/model/vit-large-patch16-224"}, "method_short": "vit_large_patch16_224", "method_details": null, "mlmodel_short": "vit_large_patch16_224", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.06%", "Top 5 Accuracy": "96.44%"}, "raw_metrics": {"Top 1 Accuracy": 83.06, "Top 5 Accuracy": 96.44}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25933, "rank": 36, "method": "tresnet_xl_448", "mlmodel": {"last_updated": "2021-02-14T14:20:32.821000+0000", "id": 327, "url": "/model/tresnet-xl-448"}, "method_short": "tresnet_xl_448", "method_details": null, "mlmodel_short": "tresnet_xl_448", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.06%", "Top 5 Accuracy": "96.19%"}, "raw_metrics": {"Top 1 Accuracy": 83.06, "Top 5 Accuracy": 96.19}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25891, "rank": 37, "method": "tf_efficientnet_b4", "mlmodel": {"last_updated": "2021-02-14T14:18:59.362000+0000", "id": 263, "url": "/model/tf-efficientnet-b4"}, "method_short": "tf_efficientnet_b4", "method_details": null, "mlmodel_short": "tf_efficientnet_b4", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "83.03%", "Top 5 Accuracy": "96.3%"}, "raw_metrics": {"Top 1 Accuracy": 83.03, "Top 5 Accuracy": 96.3}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25821, "rank": 38, "method": "resnest101e", "mlmodel": {"last_updated": "2021-02-14T14:15:59.360000+0000", "id": 343, "url": "/model/resnest101e"}, "method_short": "resnest101e", "method_details": null, "mlmodel_short": "resnest101e", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.88%", "Top 5 Accuracy": "96.31%"}, "raw_metrics": {"Top 1 Accuracy": 82.88, "Top 5 Accuracy": 96.31}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25764, "rank": 39, "method": "ig_resnext101_32x8d", "mlmodel": {"last_updated": "2021-02-14T14:13:38.676000+0000", "id": 249, "url": "/model/ig-resnext101-32x8d"}, "method_short": "ig_resnext101_32x8d", "method_details": null, "mlmodel_short": "ig_resnext101_32x8d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.7%", "Top 5 Accuracy": "96.64%"}, "raw_metrics": {"Top 1 Accuracy": 82.7, "Top 5 Accuracy": 96.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25788, "rank": 40, "method": "nasnetalarge", "mlmodel": {"last_updated": "2021-02-14T14:14:38.278000+0000", "id": 98, "url": "/model/nasnetalarge"}, "method_short": "nasnetalarge", "method_details": null, "mlmodel_short": "nasnetalarge", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.63%", "Top 5 Accuracy": "96.05%"}, "raw_metrics": {"Top 1 Accuracy": 82.63, "Top 5 Accuracy": 96.05}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25887, "rank": 41, "method": "tf_efficientnet_b2_ns", "mlmodel": {"last_updated": "2021-02-14T14:18:50.344000+0000", "id": 108, "url": "/model/tf-efficientnet-b2-ns"}, "method_short": "tf_efficientnet_b2_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b2_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.39%", "Top 5 Accuracy": "96.24%"}, "raw_metrics": {"Top 1 Accuracy": 82.39, "Top 5 Accuracy": 96.24}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25829, "rank": 42, "method": "resnet101d", "mlmodel": {"last_updated": "2021-02-14T14:16:29.838000+0000", "id": 323, "url": "/model/resnet101d"}, "method_short": "resnet101d", "method_details": null, "mlmodel_short": "resnet101d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.31%", "Top 5 Accuracy": "96.06%"}, "raw_metrics": {"Top 1 Accuracy": 82.31, "Top 5 Accuracy": 96.06}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25929, "rank": 43, "method": "tresnet_l_448", "mlmodel": {"last_updated": "2021-02-14T14:20:25.360000+0000", "id": 298, "url": "/model/tresnet-l-448"}, "method_short": "tresnet_l_448", "method_details": null, "mlmodel_short": "tresnet_l_448", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.26%", "Top 5 Accuracy": "95.98%"}, "raw_metrics": {"Top 1 Accuracy": 82.26, "Top 5 Accuracy": 95.98}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25721, "rank": 44, "method": "efficientnet_b3a", "mlmodel": {"last_updated": "2021-02-14T14:11:41.581000+0000", "id": 91, "url": "/model/efficientnet-b3a"}, "method_short": "efficientnet_b3a", "method_details": null, "mlmodel_short": "efficientnet_b3a", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.25%", "Top 5 Accuracy": "96.11%"}, "raw_metrics": {"Top 1 Accuracy": 82.25, "Top 5 Accuracy": 96.11}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25841, "rank": 45, "method": "resnetv2_101x1_bitm", "mlmodel": {"last_updated": "2021-02-14T14:16:58.235000+0000", "id": 356, "url": "/model/resnetv2-101x1-bitm"}, "method_short": "resnetv2_101x1_bitm", "method_details": null, "mlmodel_short": "resnetv2_101x1_bitm", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.21%", "Top 5 Accuracy": "96.47%"}, "raw_metrics": {"Top 1 Accuracy": 82.21, "Top 5 Accuracy": 96.47}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25708, "rank": 46, "method": "ecaresnet101d", "mlmodel": {"last_updated": "2021-02-14T14:11:09.199000+0000", "id": 202, "url": "/model/ecaresnet101d"}, "method_short": "ecaresnet101d", "method_details": null, "mlmodel_short": "ecaresnet101d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.18%", "Top 5 Accuracy": "96.06%"}, "raw_metrics": {"Top 1 Accuracy": 82.18, "Top 5 Accuracy": 96.06}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25878, "rank": 47, "method": "swsl_resnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:18:28.450000+0000", "id": 376, "url": "/model/swsl-resnext50-32x4d"}, "method_short": "swsl_resnext50_32x4d", "method_details": null, "mlmodel_short": "swsl_resnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.17%", "Top 5 Accuracy": "96.23%"}, "raw_metrics": {"Top 1 Accuracy": 82.17, "Top 5 Accuracy": 96.23}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25719, "rank": 48, "method": "efficientnet_b3", "mlmodel": {"last_updated": "2021-02-14T14:11:36.381000+0000", "id": 133, "url": "/model/efficientnet-b3"}, "method_short": "efficientnet_b3", "method_details": null, "mlmodel_short": "efficientnet_b3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.08%", "Top 5 Accuracy": "96.03%"}, "raw_metrics": {"Top 1 Accuracy": 82.08, "Top 5 Accuracy": 96.03}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25932, "rank": 49, "method": "tresnet_xl", "mlmodel": {"last_updated": "2021-02-14T14:20:31.309000+0000", "id": 319, "url": "/model/tresnet-xl"}, "method_short": "tresnet_xl", "method_details": null, "mlmodel_short": "tresnet_xl", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.05%", "Top 5 Accuracy": "95.93%"}, "raw_metrics": {"Top 1 Accuracy": 82.05, "Top 5 Accuracy": 95.93}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25807, "rank": 50, "method": "regnety_032", "mlmodel": {"last_updated": "2021-02-14T14:15:28.726000+0000", "id": 260, "url": "/model/regnety-032"}, "method_short": "regnety_032", "method_details": null, "mlmodel_short": "regnety_032", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "82.01%", "Top 5 Accuracy": "95.91%"}, "raw_metrics": {"Top 1 Accuracy": 82.01, "Top 5 Accuracy": 95.91}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25869, "rank": 51, "method": "ssl_resnext101_32x16d", "mlmodel": {"last_updated": "2021-02-14T14:18:02.475000+0000", "id": 359, "url": "/model/ssl-resnext101-32x16d"}, "method_short": "ssl_resnext101_32x16d", "method_details": null, "mlmodel_short": "ssl_resnext101_32x16d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.84%", "Top 5 Accuracy": "96.09%"}, "raw_metrics": {"Top 1 Accuracy": 81.84, "Top 5 Accuracy": 96.09}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25889, "rank": 52, "method": "tf_efficientnet_b3_ap", "mlmodel": {"last_updated": "2021-02-14T14:18:54.745000+0000", "id": 254, "url": "/model/tf-efficientnet-b3-ap"}, "method_short": "tf_efficientnet_b3_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b3_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.82%", "Top 5 Accuracy": "95.62%"}, "raw_metrics": {"Top 1 Accuracy": 81.82, "Top 5 Accuracy": 95.62}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25940, "rank": 53, "method": "vit_base_patch16_224", "mlmodel": {"last_updated": "2021-02-14T14:20:45.161000+0000", "id": 270, "url": "/model/vit-base-patch16-224"}, "method_short": "vit_base_patch16_224", "method_details": null, "mlmodel_short": "vit_base_patch16_224", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.78%", "Top 5 Accuracy": "96.13%"}, "raw_metrics": {"Top 1 Accuracy": 81.78, "Top 5 Accuracy": 96.13}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25931, "rank": 54, "method": "tresnet_m_448", "mlmodel": {"last_updated": "2021-02-14T14:20:29.756000+0000", "id": 312, "url": "/model/tresnet-m-448"}, "method_short": "tresnet_m_448", "method_details": null, "mlmodel_short": "tresnet_m_448", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.72%", "Top 5 Accuracy": "95.57%"}, "raw_metrics": {"Top 1 Accuracy": 81.72, "Top 5 Accuracy": 95.57}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25942, "rank": 55, "method": "vit_base_patch32_384", "mlmodel": {"last_updated": "2021-02-14T14:20:48.147000+0000", "id": 280, "url": "/model/vit-base-patch32-384"}, "method_short": "vit_base_patch32_384", "method_details": null, "mlmodel_short": "vit_base_patch32_384", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.66%", "Top 5 Accuracy": "96.13%"}, "raw_metrics": {"Top 1 Accuracy": 81.66, "Top 5 Accuracy": 96.13}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25888, "rank": 56, "method": "tf_efficientnet_b3", "mlmodel": {"last_updated": "2021-02-14T14:18:52.589000+0000", "id": 250, "url": "/model/tf-efficientnet-b3"}, "method_short": "tf_efficientnet_b3", "method_details": null, "mlmodel_short": "tf_efficientnet_b3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.65%", "Top 5 Accuracy": "95.72%"}, "raw_metrics": {"Top 1 Accuracy": 81.65, "Top 5 Accuracy": 95.72}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25853, "rank": 57, "method": "rexnet_200", "mlmodel": {"last_updated": "2021-02-14T14:17:20.271000+0000", "id": 288, "url": "/model/rexnet-200"}, "method_short": "rexnet_200", "method_details": null, "mlmodel_short": "rexnet_200", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.63%", "Top 5 Accuracy": "95.67%"}, "raw_metrics": {"Top 1 Accuracy": 81.63, "Top 5 Accuracy": 95.67}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25871, "rank": 58, "method": "ssl_resnext101_32x8d", "mlmodel": {"last_updated": "2021-02-14T14:18:08.563000+0000", "id": 365, "url": "/model/ssl-resnext101-32x8d"}, "method_short": "ssl_resnext101_32x8d", "method_details": null, "mlmodel_short": "ssl_resnext101_32x8d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.61%", "Top 5 Accuracy": "96.04%"}, "raw_metrics": {"Top 1 Accuracy": 81.61, "Top 5 Accuracy": 96.04}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25917, "rank": 59, "method": "tf_efficientnet_lite4", "mlmodel": {"last_updated": "2021-02-14T14:19:58.491000+0000", "id": 306, "url": "/model/tf-efficientnet-lite4"}, "method_short": "tf_efficientnet_lite4", "method_details": null, "mlmodel_short": "tf_efficientnet_lite4", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.54%", "Top 5 Accuracy": "95.66%"}, "raw_metrics": {"Top 1 Accuracy": 81.54, "Top 5 Accuracy": 95.66}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25928, "rank": 60, "method": "tresnet_l", "mlmodel": {"last_updated": "2021-02-14T14:20:23.555000+0000", "id": 174, "url": "/model/tresnet-l"}, "method_short": "tresnet_l", "method_details": null, "mlmodel_short": "tresnet_l", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.49%", "Top 5 Accuracy": "95.62%"}, "raw_metrics": {"Top 1 Accuracy": 81.49, "Top 5 Accuracy": 95.62}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25948, "rank": 61, "method": "wide_resnet50_2", "mlmodel": {"last_updated": "2021-02-14T14:20:59.871000+0000", "id": 329, "url": "/model/wide-resnet50-2"}, "method_short": "wide_resnet50_2", "method_details": null, "mlmodel_short": "wide_resnet50_2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.45%", "Top 5 Accuracy": "95.52%"}, "raw_metrics": {"Top 1 Accuracy": 81.45, "Top 5 Accuracy": 95.52}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25884, "rank": 62, "method": "tf_efficientnet_b1_ns", "mlmodel": {"last_updated": "2021-02-14T14:18:43.344000+0000", "id": 101, "url": "/model/tf-efficientnet-b1-ns"}, "method_short": "tf_efficientnet_b1_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b1_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.39%", "Top 5 Accuracy": "95.74%"}, "raw_metrics": {"Top 1 Accuracy": 81.39, "Top 5 Accuracy": 95.74}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25768, "rank": 63, "method": "legacy_senet154", "mlmodel": {"last_updated": "2021-02-14T14:13:48.859000+0000", "id": 264, "url": "/model/legacy-senet154"}, "method_short": "legacy_senet154", "method_details": null, "mlmodel_short": "legacy_senet154", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.33%", "Top 5 Accuracy": "95.51%"}, "raw_metrics": {"Top 1 Accuracy": 81.33, "Top 5 Accuracy": 95.51}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25862, "rank": 64, "method": "seresnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:17:44.395000+0000", "id": 332, "url": "/model/seresnext50-32x4d"}, "method_short": "seresnext50_32x4d", "method_details": null, "mlmodel_short": "seresnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.27%", "Top 5 Accuracy": "95.62%"}, "raw_metrics": {"Top 1 Accuracy": 81.27, "Top 5 Accuracy": 95.62}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25747, "rank": 65, "method": "gluon_senet154", "mlmodel": {"last_updated": "2021-02-14T14:12:41.101000+0000", "id": 96, "url": "/model/gluon-senet154"}, "method_short": "gluon_senet154", "method_details": null, "mlmodel_short": "gluon_senet154", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.23%", "Top 5 Accuracy": "95.35%"}, "raw_metrics": {"Top 1 Accuracy": 81.23, "Top 5 Accuracy": 95.35}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25874, "rank": 66, "method": "swsl_resnet50", "mlmodel": {"last_updated": "2021-02-14T14:18:16.926000+0000", "id": 369, "url": "/model/swsl-resnet50"}, "method_short": "swsl_resnet50", "method_details": null, "mlmodel_short": "swsl_resnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.14%", "Top 5 Accuracy": "95.97%"}, "raw_metrics": {"Top 1 Accuracy": 81.14, "Top 5 Accuracy": 95.97}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25828, "rank": 67, "method": "resnest50d_4s2x40d", "mlmodel": {"last_updated": "2021-02-14T14:16:25.609000+0000", "id": 90, "url": "/model/resnest50d-4s2x40d"}, "method_short": "resnest50d_4s2x40d", "method_details": null, "mlmodel_short": "resnest50d_4s2x40d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.11%", "Top 5 Accuracy": "95.55%"}, "raw_metrics": {"Top 1 Accuracy": 81.11, "Top 5 Accuracy": 95.55}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27689, "rank": 68, "method": "ResNet-50 Semi-weakly supervised", "mlmodel": {"last_updated": "2021-02-27T09:28:18.132000+0000", "id": 874, "url": "/model/resnet-50-semi-weakly-supervised"}, "method_short": "ResNet-50 Semi-weakly supervised", "method_details": null, "mlmodel_short": "ResNet-50 Semi-weakly supervised", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.06%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 81.06, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25737, "rank": 69, "method": "gluon_resnet152_v1s", "mlmodel": {"last_updated": "2021-02-14T14:12:18.742000+0000", "id": 223, "url": "/model/gluon-resnet152-v1s"}, "method_short": "gluon_resnet152_v1s", "method_details": null, "mlmodel_short": "gluon_resnet152_v1s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.02%", "Top 5 Accuracy": "95.42%"}, "raw_metrics": {"Top 1 Accuracy": 81.02, "Top 5 Accuracy": 95.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25827, "rank": 70, "method": "resnest50d_1s4x24d", "mlmodel": {"last_updated": "2021-02-14T14:16:22.129000+0000", "id": 354, "url": "/model/resnest50d-1s4x24d"}, "method_short": "resnest50d_1s4x24d", "method_details": null, "mlmodel_short": "resnest50d_1s4x24d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "81.0%", "Top 5 Accuracy": "95.33%"}, "raw_metrics": {"Top 1 Accuracy": 81.0, "Top 5 Accuracy": 95.33}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25826, "rank": 71, "method": "resnest50d", "mlmodel": {"last_updated": "2021-02-14T14:16:18.002000+0000", "id": 353, "url": "/model/resnest50d"}, "method_short": "resnest50d", "method_details": null, "mlmodel_short": "resnest50d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.96%", "Top 5 Accuracy": "95.38%"}, "raw_metrics": {"Top 1 Accuracy": 80.96, "Top 5 Accuracy": 95.38}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25870, "rank": 72, "method": "ssl_resnext101_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:18:05.626000+0000", "id": 361, "url": "/model/ssl-resnext101-32x4d"}, "method_short": "ssl_resnext101_32x4d", "method_details": null, "mlmodel_short": "ssl_resnext101_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.91%", "Top 5 Accuracy": "95.73%"}, "raw_metrics": {"Top 1 Accuracy": 80.91, "Top 5 Accuracy": 95.73}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25749, "rank": 73, "method": "gluon_seresnext101_64x4d", "mlmodel": {"last_updated": "2021-02-14T14:12:45.866000+0000", "id": 107, "url": "/model/gluon-seresnext101-64x4d"}, "method_short": "gluon_seresnext101_64x4d", "method_details": null, "mlmodel_short": "gluon_seresnext101_64x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.88%", "Top 5 Accuracy": "95.31%"}, "raw_metrics": {"Top 1 Accuracy": 80.88, "Top 5 Accuracy": 95.31}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25748, "rank": 74, "method": "gluon_seresnext101_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:12:43.375000+0000", "id": 100, "url": "/model/gluon-seresnext101-32x4d"}, "method_short": "gluon_seresnext101_32x4d", "method_details": null, "mlmodel_short": "gluon_seresnext101_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.87%", "Top 5 Accuracy": "95.29%"}, "raw_metrics": {"Top 1 Accuracy": 80.87, "Top 5 Accuracy": 95.29}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25720, "rank": 75, "method": "efficientnet_b3_pruned", "mlmodel": {"last_updated": "2021-02-14T14:11:39.632000+0000", "id": 176, "url": "/model/efficientnet-b3-pruned"}, "method_short": "efficientnet_b3_pruned", "method_details": null, "mlmodel_short": "efficientnet_b3_pruned", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.86%", "Top 5 Accuracy": "95.24%"}, "raw_metrics": {"Top 1 Accuracy": 80.86, "Top 5 Accuracy": 95.24}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25709, "rank": 76, "method": "ecaresnet101d_pruned", "mlmodel": {"last_updated": "2021-02-14T14:11:11.789000+0000", "id": 209, "url": "/model/ecaresnet101d-pruned"}, "method_short": "ecaresnet101d_pruned", "method_details": null, "mlmodel_short": "ecaresnet101d_pruned", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.82%", "Top 5 Accuracy": "95.64%"}, "raw_metrics": {"Top 1 Accuracy": 80.82, "Top 5 Accuracy": 95.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25813, "rank": 77, "method": "regnety_320", "mlmodel": {"last_updated": "2021-02-14T14:15:44.381000+0000", "id": 291, "url": "/model/regnety-320"}, "method_short": "regnety_320", "method_details": null, "mlmodel_short": "regnety_320", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": "95.25%"}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": 95.25}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25930, "rank": 78, "method": "tresnet_m", "mlmodel": {"last_updated": "2021-02-14T14:20:28.046000+0000", "id": 303, "url": "/model/tresnet-m"}, "method_short": "tresnet_m", "method_details": null, "mlmodel_short": "tresnet_m", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.8%", "Top 5 Accuracy": "94.86%"}, "raw_metrics": {"Top 1 Accuracy": 80.8, "Top 5 Accuracy": 94.86}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25745, "rank": 79, "method": "gluon_resnext101_64x4d", "mlmodel": {"last_updated": "2021-02-14T14:12:36.569000+0000", "id": 180, "url": "/model/gluon-resnext101-64x4d"}, "method_short": "gluon_resnext101_64x4d", "method_details": null, "mlmodel_short": "gluon_resnext101_64x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.63%", "Top 5 Accuracy": "95.0%"}, "raw_metrics": {"Top 1 Accuracy": 80.63, "Top 5 Accuracy": 95.0}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25718, "rank": 80, "method": "efficientnet_b2a", "mlmodel": {"last_updated": "2021-02-14T14:11:34.639000+0000", "id": 186, "url": "/model/efficientnet-b2a"}, "method_short": "efficientnet_b2a", "method_details": null, "mlmodel_short": "efficientnet_b2a", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.61%", "Top 5 Accuracy": "95.32%"}, "raw_metrics": {"Top 1 Accuracy": 80.61, "Top 5 Accuracy": 95.32}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25710, "rank": 81, "method": "ecaresnet50d", "mlmodel": {"last_updated": "2021-02-14T14:11:15.344000+0000", "id": 134, "url": "/model/ecaresnet50d"}, "method_short": "ecaresnet50d", "method_details": null, "mlmodel_short": "ecaresnet50d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.61%", "Top 5 Accuracy": "95.31%"}, "raw_metrics": {"Top 1 Accuracy": 80.61, "Top 5 Accuracy": 95.31}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25839, "rank": 82, "method": "resnet50d", "mlmodel": {"last_updated": "2021-02-14T14:16:53.357000+0000", "id": 135, "url": "/model/resnet50d"}, "method_short": "resnet50d", "method_details": null, "mlmodel_short": "resnet50d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.55%", "Top 5 Accuracy": "95.16%"}, "raw_metrics": {"Top 1 Accuracy": 80.55, "Top 5 Accuracy": 95.16}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25736, "rank": 83, "method": "gluon_resnet152_v1d", "mlmodel": {"last_updated": "2021-02-14T14:12:16.435000+0000", "id": 219, "url": "/model/gluon-resnet152-v1d"}, "method_short": "gluon_resnet152_v1d", "method_details": null, "mlmodel_short": "gluon_resnet152_v1d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.48%", "Top 5 Accuracy": "95.2%"}, "raw_metrics": {"Top 1 Accuracy": 80.48, "Top 5 Accuracy": 95.2}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25780, "rank": 84, "method": "mixnet_xl", "mlmodel": {"last_updated": "2021-02-14T14:14:19.273000+0000", "id": 318, "url": "/model/mixnet-xl"}, "method_short": "mixnet_xl", "method_details": null, "mlmodel_short": "mixnet_xl", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.47%", "Top 5 Accuracy": "94.93%"}, "raw_metrics": {"Top 1 Accuracy": 80.47, "Top 5 Accuracy": 94.93}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25712, "rank": 85, "method": "ecaresnetlight", "mlmodel": {"last_updated": "2021-02-14T14:11:20.422000+0000", "id": 222, "url": "/model/ecaresnetlight"}, "method_short": "ecaresnetlight", "method_details": null, "mlmodel_short": "ecaresnetlight", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.46%", "Top 5 Accuracy": "95.25%"}, "raw_metrics": {"Top 1 Accuracy": 80.46, "Top 5 Accuracy": 95.25}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25908, "rank": 86, "method": "tf_efficientnet_el", "mlmodel": {"last_updated": "2021-02-14T14:19:37.705000+0000", "id": 375, "url": "/model/tf-efficientnet-el"}, "method_short": "tf_efficientnet_el", "method_details": null, "mlmodel_short": "tf_efficientnet_el", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.45%", "Top 5 Accuracy": "95.17%"}, "raw_metrics": {"Top 1 Accuracy": 80.45, "Top 5 Accuracy": 95.17}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25732, "rank": 87, "method": "gluon_resnet101_v1d", "mlmodel": {"last_updated": "2021-02-14T14:12:07.096000+0000", "id": 118, "url": "/model/gluon-resnet101-v1d"}, "method_short": "gluon_resnet101_v1d", "method_details": null, "mlmodel_short": "gluon_resnet101_v1d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.4%", "Top 5 Accuracy": "95.02%"}, "raw_metrics": {"Top 1 Accuracy": 80.4, "Top 5 Accuracy": 95.02}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25811, "rank": 88, "method": "regnety_120", "mlmodel": {"last_updated": "2021-02-14T14:15:39.463000+0000", "id": 279, "url": "/model/regnety-120"}, "method_short": "regnety_120", "method_details": null, "mlmodel_short": "regnety_120", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.38%", "Top 5 Accuracy": "95.12%"}, "raw_metrics": {"Top 1 Accuracy": 80.38, "Top 5 Accuracy": 95.12}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25716, "rank": 89, "method": "efficientnet_b2", "mlmodel": {"last_updated": "2021-02-14T14:11:29.145000+0000", "id": 239, "url": "/model/efficientnet-b2"}, "method_short": "efficientnet_b2", "method_details": null, "mlmodel_short": "efficientnet_b2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.38%", "Top 5 Accuracy": "95.08%"}, "raw_metrics": {"Top 1 Accuracy": 80.38, "Top 5 Accuracy": 95.08}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25744, "rank": 90, "method": "gluon_resnext101_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:12:34.297000+0000", "id": 125, "url": "/model/gluon-resnext101-32x4d"}, "method_short": "gluon_resnext101_32x4d", "method_details": null, "mlmodel_short": "gluon_resnext101_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.33%", "Top 5 Accuracy": "94.91%"}, "raw_metrics": {"Top 1 Accuracy": 80.33, "Top 5 Accuracy": 94.91}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25852, "rank": 91, "method": "rexnet_150", "mlmodel": {"last_updated": "2021-02-14T14:17:17.979000+0000", "id": 287, "url": "/model/rexnet-150"}, "method_short": "rexnet_150", "method_details": null, "mlmodel_short": "rexnet_150", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.31%", "Top 5 Accuracy": "95.16%"}, "raw_metrics": {"Top 1 Accuracy": 80.31, "Top 5 Accuracy": 95.16}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25872, "rank": 92, "method": "ssl_resnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:18:11.559000+0000", "id": 362, "url": "/model/ssl-resnext50-32x4d"}, "method_short": "ssl_resnext50_32x4d", "method_details": null, "mlmodel_short": "ssl_resnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.3%", "Top 5 Accuracy": "95.41%"}, "raw_metrics": {"Top 1 Accuracy": 80.3, "Top 5 Accuracy": 95.41}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25886, "rank": 93, "method": "tf_efficientnet_b2_ap", "mlmodel": {"last_updated": "2021-02-14T14:18:47.935000+0000", "id": 236, "url": "/model/tf-efficientnet-b2-ap"}, "method_short": "tf_efficientnet_b2_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b2_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.3%", "Top 5 Accuracy": "95.03%"}, "raw_metrics": {"Top 1 Accuracy": 80.3, "Top 5 Accuracy": 95.03}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25733, "rank": 94, "method": "gluon_resnet101_v1s", "mlmodel": {"last_updated": "2021-02-14T14:12:09.469000+0000", "id": 204, "url": "/model/gluon-resnet101-v1s"}, "method_short": "gluon_resnet101_v1s", "method_details": null, "mlmodel_short": "gluon_resnet101_v1s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.29%", "Top 5 Accuracy": "95.16%"}, "raw_metrics": {"Top 1 Accuracy": 80.29, "Top 5 Accuracy": 95.16}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25812, "rank": 95, "method": "regnety_160", "mlmodel": {"last_updated": "2021-02-14T14:15:41.834000+0000", "id": 285, "url": "/model/regnety-160"}, "method_short": "regnety_160", "method_details": null, "mlmodel_short": "regnety_160", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.28%", "Top 5 Accuracy": "94.97%"}, "raw_metrics": {"Top 1 Accuracy": 80.28, "Top 5 Accuracy": 94.97}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25859, "rank": 96, "method": "seresnet50", "mlmodel": {"last_updated": "2021-02-14T14:17:36.708000+0000", "id": 316, "url": "/model/seresnet50"}, "method_short": "seresnet50", "method_details": null, "mlmodel_short": "seresnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.26%", "Top 5 Accuracy": "95.07%"}, "raw_metrics": {"Top 1 Accuracy": 80.26, "Top 5 Accuracy": 95.07}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25801, "rank": 97, "method": "regnetx_320", "mlmodel": {"last_updated": "2021-02-14T14:15:11.899000+0000", "id": 235, "url": "/model/regnetx-320"}, "method_short": "regnetx_320", "method_details": null, "mlmodel_short": "regnetx_320", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.25%", "Top 5 Accuracy": "95.03%"}, "raw_metrics": {"Top 1 Accuracy": 80.25, "Top 5 Accuracy": 95.03}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25774, "rank": 98, "method": "legacy_seresnext101_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:14:04.298000+0000", "id": 294, "url": "/model/legacy-seresnext101-32x4d"}, "method_short": "legacy_seresnext101_32x4d", "method_details": null, "mlmodel_short": "legacy_seresnext101_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.23%", "Top 5 Accuracy": "95.02%"}, "raw_metrics": {"Top 1 Accuracy": 80.23, "Top 5 Accuracy": 95.02}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25845, "rank": 99, "method": "resnetv2_50x1_bitm", "mlmodel": {"last_updated": "2021-02-14T14:17:04.239000+0000", "id": 177, "url": "/model/resnetv2-50x1-bitm"}, "method_short": "resnetv2_50x1_bitm", "method_details": null, "mlmodel_short": "resnetv2_50x1_bitm", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.19%", "Top 5 Accuracy": "95.63%"}, "raw_metrics": {"Top 1 Accuracy": 80.19, "Top 5 Accuracy": 95.63}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25702, "rank": 100, "method": "dpn107", "mlmodel": {"last_updated": "2021-02-14T14:10:52.821000+0000", "id": 140, "url": "/model/dpn107"}, "method_short": "dpn107", "method_details": null, "mlmodel_short": "dpn107", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.16%", "Top 5 Accuracy": "94.91%"}, "raw_metrics": {"Top 1 Accuracy": 80.16, "Top 5 Accuracy": 94.91}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25865, "rank": 101, "method": "skresnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:17:50.489000+0000", "id": 351, "url": "/model/skresnext50-32x4d"}, "method_short": "skresnext50_32x4d", "method_details": null, "mlmodel_short": "skresnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.15%", "Top 5 Accuracy": "94.64%"}, "raw_metrics": {"Top 1 Accuracy": 80.15, "Top 5 Accuracy": 94.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25885, "rank": 102, "method": "tf_efficientnet_b2", "mlmodel": {"last_updated": "2021-02-14T14:18:45.651000+0000", "id": 230, "url": "/model/tf-efficientnet-b2"}, "method_short": "tf_efficientnet_b2", "method_details": null, "mlmodel_short": "tf_efficientnet_b2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.07%", "Top 5 Accuracy": "94.9%"}, "raw_metrics": {"Top 1 Accuracy": 80.07, "Top 5 Accuracy": 94.9}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25682, "rank": 103, "method": "cspdarknet53", "mlmodel": {"last_updated": "2021-02-14T14:10:20.648000+0000", "id": 382, "url": "/model/cspdarknet53"}, "method_short": "cspdarknet53", "method_details": null, "mlmodel_short": "cspdarknet53", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.05%", "Top 5 Accuracy": "95.09%"}, "raw_metrics": {"Top 1 Accuracy": 80.05, "Top 5 Accuracy": 95.09}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25684, "rank": 104, "method": "cspresnext50", "mlmodel": {"last_updated": "2021-02-14T14:10:23.642000+0000", "id": 87, "url": "/model/cspresnext50"}, "method_short": "cspresnext50", "method_details": null, "mlmodel_short": "cspresnext50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "80.05%", "Top 5 Accuracy": "94.94%"}, "raw_metrics": {"Top 1 Accuracy": 80.05, "Top 5 Accuracy": 94.94}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25706, "rank": 105, "method": "dpn92", "mlmodel": {"last_updated": "2021-02-14T14:11:02.983000+0000", "id": 199, "url": "/model/dpn92"}, "method_short": "dpn92", "method_details": null, "mlmodel_short": "dpn92", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.99%", "Top 5 Accuracy": "94.84%"}, "raw_metrics": {"Top 1 Accuracy": 79.99, "Top 5 Accuracy": 94.84}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25750, "rank": 106, "method": "gluon_seresnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:12:48.374000+0000", "id": 109, "url": "/model/gluon-seresnext50-32x4d"}, "method_short": "gluon_seresnext50_32x4d", "method_details": null, "mlmodel_short": "gluon_seresnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.92%", "Top 5 Accuracy": "94.82%"}, "raw_metrics": {"Top 1 Accuracy": 79.92, "Top 5 Accuracy": 94.82}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25717, "rank": 107, "method": "efficientnet_b2_pruned", "mlmodel": {"last_updated": "2021-02-14T14:11:32.488000+0000", "id": 243, "url": "/model/efficientnet-b2-pruned"}, "method_short": "efficientnet_b2_pruned", "method_details": null, "mlmodel_short": "efficientnet_b2_pruned", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.91%", "Top 5 Accuracy": "94.86%"}, "raw_metrics": {"Top 1 Accuracy": 79.91, "Top 5 Accuracy": 94.86}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25735, "rank": 108, "method": "gluon_resnet152_v1c", "mlmodel": {"last_updated": "2021-02-14T14:12:14.207000+0000", "id": 206, "url": "/model/gluon-resnet152-v1c"}, "method_short": "gluon_resnet152_v1c", "method_details": null, "mlmodel_short": "gluon_resnet152_v1c", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.91%", "Top 5 Accuracy": "94.85%"}, "raw_metrics": {"Top 1 Accuracy": 79.91, "Top 5 Accuracy": 94.85}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25952, "rank": 109, "method": "xception71", "mlmodel": {"last_updated": "2021-02-14T14:21:10.199000+0000", "id": 352, "url": "/model/xception71"}, "method_short": "xception71", "method_details": null, "mlmodel_short": "xception71", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.88%", "Top 5 Accuracy": "94.93%"}, "raw_metrics": {"Top 1 Accuracy": 79.88, "Top 5 Accuracy": 94.93}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25810, "rank": 110, "method": "regnety_080", "mlmodel": {"last_updated": "2021-02-14T14:15:37.054000+0000", "id": 262, "url": "/model/regnety-080"}, "method_short": "regnety_080", "method_details": null, "mlmodel_short": "regnety_080", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.87%", "Top 5 Accuracy": "94.83%"}, "raw_metrics": {"Top 1 Accuracy": 79.87, "Top 5 Accuracy": 94.83}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25800, "rank": 111, "method": "regnetx_160", "mlmodel": {"last_updated": "2021-02-14T14:15:08.656000+0000", "id": 233, "url": "/model/regnetx-160"}, "method_short": "regnetx_160", "method_details": null, "mlmodel_short": "regnetx_160", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.84%", "Top 5 Accuracy": "94.82%"}, "raw_metrics": {"Top 1 Accuracy": 79.84, "Top 5 Accuracy": 94.82}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25916, "rank": 112, "method": "tf_efficientnet_lite3", "mlmodel": {"last_updated": "2021-02-14T14:19:56.138000+0000", "id": 119, "url": "/model/tf-efficientnet-lite3"}, "method_short": "tf_efficientnet_lite3", "method_details": null, "mlmodel_short": "tf_efficientnet_lite3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.83%", "Top 5 Accuracy": "94.91%"}, "raw_metrics": {"Top 1 Accuracy": 79.83, "Top 5 Accuracy": 94.91}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25703, "rank": 113, "method": "dpn131", "mlmodel": {"last_updated": "2021-02-14T14:10:55.335000+0000", "id": 97, "url": "/model/dpn131"}, "method_short": "dpn131", "method_details": null, "mlmodel_short": "dpn131", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.83%", "Top 5 Accuracy": "94.71%"}, "raw_metrics": {"Top 1 Accuracy": 79.83, "Top 5 Accuracy": 94.71}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25848, "rank": 114, "method": "resnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:17:09.545000+0000", "id": 266, "url": "/model/resnext50-32x4d"}, "method_short": "resnext50_32x4d", "method_details": null, "mlmodel_short": "resnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.79%", "Top 5 Accuracy": "94.61%"}, "raw_metrics": {"Top 1 Accuracy": 79.79, "Top 5 Accuracy": 94.61}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 25809, "rank": 115, "method": "regnety_064", "mlmodel": {"last_updated": "2021-02-14T14:15:34.058000+0000", "id": 272, "url": "/model/regnety-064"}, "method_short": "regnety_064", "method_details": null, "mlmodel_short": "regnety_064", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.73%", "Top 5 Accuracy": "94.76%"}, "raw_metrics": {"Top 1 Accuracy": 79.73, "Top 5 Accuracy": 94.76}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25711, "rank": 116, "method": "ecaresnet50d_pruned", "mlmodel": {"last_updated": "2021-02-14T14:11:17.885000+0000", "id": 217, "url": "/model/ecaresnet50d-pruned"}, "method_short": "ecaresnet50d_pruned", "method_details": null, "mlmodel_short": "ecaresnet50d_pruned", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.71%", "Top 5 Accuracy": "94.88%"}, "raw_metrics": {"Top 1 Accuracy": 79.71, "Top 5 Accuracy": 94.88}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25751, "rank": 117, "method": "gluon_xception65", "mlmodel": {"last_updated": "2021-02-14T14:12:50.962000+0000", "id": 148, "url": "/model/gluon-xception65"}, "method_short": "gluon_xception65", "method_details": null, "mlmodel_short": "gluon_xception65", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.7%", "Top 5 Accuracy": "94.87%"}, "raw_metrics": {"Top 1 Accuracy": 79.7, "Top 5 Accuracy": 94.87}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25734, "rank": 118, "method": "gluon_resnet152_v1b", "mlmodel": {"last_updated": "2021-02-14T14:12:11.854000+0000", "id": 211, "url": "/model/gluon-resnet152-v1b"}, "method_short": "gluon_resnet152_v1b", "method_details": null, "mlmodel_short": "gluon_resnet152_v1b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.69%", "Top 5 Accuracy": "94.73%"}, "raw_metrics": {"Top 1 Accuracy": 79.69, "Top 5 Accuracy": 94.73}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25849, "rank": 119, "method": "resnext50d_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:17:11.631000+0000", "id": 267, "url": "/model/resnext50d-32x4d"}, "method_short": "resnext50d_32x4d", "method_details": null, "mlmodel_short": "resnext50d_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.67%", "Top 5 Accuracy": "94.87%"}, "raw_metrics": {"Top 1 Accuracy": 79.67, "Top 5 Accuracy": 94.87}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 25707, "rank": 120, "method": "dpn98", "mlmodel": {"last_updated": "2021-02-14T14:11:05.883000+0000", "id": 182, "url": "/model/dpn98"}, "method_short": "dpn98", "method_details": null, "mlmodel_short": "dpn98", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.65%", "Top 5 Accuracy": "94.61%"}, "raw_metrics": {"Top 1 Accuracy": 79.65, "Top 5 Accuracy": 94.61}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25799, "rank": 121, "method": "regnetx_120", "mlmodel": {"last_updated": "2021-02-14T14:15:06.494000+0000", "id": 227, "url": "/model/regnetx-120"}, "method_short": "regnetx_120", "method_details": null, "mlmodel_short": "regnetx_120", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.61%", "Top 5 Accuracy": "94.73%"}, "raw_metrics": {"Top 1 Accuracy": 79.61, "Top 5 Accuracy": 94.73}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25683, "rank": 122, "method": "cspresnet50", "mlmodel": {"last_updated": "2021-02-14T14:10:22.185000+0000", "id": 383, "url": "/model/cspresnet50"}, "method_short": "cspresnet50", "method_details": null, "mlmodel_short": "cspresnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.57%", "Top 5 Accuracy": "94.71%"}, "raw_metrics": {"Top 1 Accuracy": 79.57, "Top 5 Accuracy": 94.71}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25951, "rank": 123, "method": "xception65", "mlmodel": {"last_updated": "2021-02-14T14:21:07.419000+0000", "id": 344, "url": "/model/xception65"}, "method_short": "xception65", "method_details": null, "mlmodel_short": "xception65", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.55%", "Top 5 Accuracy": "94.66%"}, "raw_metrics": {"Top 1 Accuracy": 79.55, "Top 5 Accuracy": 94.66}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25731, "rank": 124, "method": "gluon_resnet101_v1c", "mlmodel": {"last_updated": "2021-02-14T14:12:04.641000+0000", "id": 102, "url": "/model/gluon-resnet101-v1c"}, "method_short": "gluon_resnet101_v1c", "method_details": null, "mlmodel_short": "gluon_resnet101_v1c", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.53%", "Top 5 Accuracy": "94.59%"}, "raw_metrics": {"Top 1 Accuracy": 79.53, "Top 5 Accuracy": 94.59}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25851, "rank": 125, "method": "rexnet_130", "mlmodel": {"last_updated": "2021-02-14T14:17:15.869000+0000", "id": 281, "url": "/model/rexnet-130"}, "method_short": "rexnet_130", "method_details": null, "mlmodel_short": "rexnet_130", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.49%", "Top 5 Accuracy": "94.67%"}, "raw_metrics": {"Top 1 Accuracy": 79.49, "Top 5 Accuracy": 94.67}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25760, "rank": 126, "method": "hrnet_w64", "mlmodel": {"last_updated": "2021-02-14T14:13:25.172000+0000", "id": 216, "url": "/model/hrnet-w64"}, "method_short": "hrnet_w64", "method_details": null, "mlmodel_short": "hrnet_w64", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.46%", "Top 5 Accuracy": "94.65%"}, "raw_metrics": {"Top 1 Accuracy": 79.46, "Top 5 Accuracy": 94.65}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25692, "rank": 127, "method": "dla102x2", "mlmodel": {"last_updated": "2021-02-14T14:10:36.392000+0000", "id": 160, "url": "/model/dla102x2"}, "method_short": "dla102x2", "method_details": null, "mlmodel_short": "dla102x2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.44%", "Top 5 Accuracy": "94.65%"}, "raw_metrics": {"Top 1 Accuracy": 79.44, "Top 5 Accuracy": 94.65}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25746, "rank": 128, "method": "gluon_resnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:12:38.917000+0000", "id": 92, "url": "/model/gluon-resnext50-32x4d"}, "method_short": "gluon_resnext50_32x4d", "method_details": null, "mlmodel_short": "gluon_resnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.35%", "Top 5 Accuracy": "94.42%"}, "raw_metrics": {"Top 1 Accuracy": 79.35, "Top 5 Accuracy": 94.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25907, "rank": 129, "method": "tf_efficientnet_cc_b1_8e", "mlmodel": {"last_updated": "2021-02-14T14:19:36.187000+0000", "id": 372, "url": "/model/tf-efficientnet-cc-b1-8e"}, "method_short": "tf_efficientnet_cc_b1_8e", "method_details": null, "mlmodel_short": "tf_efficientnet_cc_b1_8e", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.33%", "Top 5 Accuracy": "94.37%"}, "raw_metrics": {"Top 1 Accuracy": 79.33, "Top 5 Accuracy": 94.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25759, "rank": 130, "method": "hrnet_w48", "mlmodel": {"last_updated": "2021-02-14T14:13:21.261000+0000", "id": 213, "url": "/model/hrnet-w48"}, "method_short": "hrnet_w48", "method_details": null, "mlmodel_short": "hrnet_w48", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.32%", "Top 5 Accuracy": "94.51%"}, "raw_metrics": {"Top 1 Accuracy": 79.32, "Top 5 Accuracy": 94.51}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25727, "rank": 131, "method": "ese_vovnet39b", "mlmodel": {"last_updated": "2021-02-14T14:11:54.928000+0000", "id": 117, "url": "/model/ese-vovnet39b"}, "method_short": "ese_vovnet39b", "method_details": null, "mlmodel_short": "ese_vovnet39b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.31%", "Top 5 Accuracy": "94.72%"}, "raw_metrics": {"Top 1 Accuracy": 79.31, "Top 5 Accuracy": 94.72}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25656, "rank": 132, "method": "ResNeXt-101-32x8d", "mlmodel": {"last_updated": "2021-02-12T11:31:34.287000+0000", "id": 139, "url": "/model/resnext-101-32x8d"}, "method_short": "ResNeXt-101-32x8d", "method_details": null, "mlmodel_short": "ResNeXt-101-32x8d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.31%", "Top 5 Accuracy": "94.53%"}, "raw_metrics": {"Top 1 Accuracy": 79.31, "Top 5 Accuracy": 94.53}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25730, "rank": 133, "method": "gluon_resnet101_v1b", "mlmodel": {"last_updated": "2021-02-14T14:12:01.998000+0000", "id": 191, "url": "/model/gluon-resnet101-v1b"}, "method_short": "gluon_resnet101_v1b", "method_details": null, "mlmodel_short": "gluon_resnet101_v1b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.3%", "Top 5 Accuracy": "94.53%"}, "raw_metrics": {"Top 1 Accuracy": 79.3, "Top 5 Accuracy": 94.53}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25847, "rank": 134, "method": "resnext101_32x8d", "mlmodel": {"last_updated": "2021-02-14T14:17:07.092000+0000", "id": 258, "url": "/model/resnext101-32x8d"}, "method_short": "resnext101_32x8d", "method_details": null, "mlmodel_short": "resnext101_32x8d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.3%", "Top 5 Accuracy": "94.53%"}, "raw_metrics": {"Top 1 Accuracy": 79.3, "Top 5 Accuracy": 94.53}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 6, "name": "ResNeXt", "color": "#86960b"}], "reports": []}, {"table_id": 116, "row_id": 25840, "rank": 135, "method": "resnetblur50", "mlmodel": {"last_updated": "2021-02-14T14:16:55.657000+0000", "id": 218, "url": "/model/resnetblur50"}, "method_short": "resnetblur50", "method_details": null, "mlmodel_short": "resnetblur50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.29%", "Top 5 Accuracy": "94.64%"}, "raw_metrics": {"Top 1 Accuracy": 79.29, "Top 5 Accuracy": 94.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25883, "rank": 136, "method": "tf_efficientnet_b1_ap", "mlmodel": {"last_updated": "2021-02-14T14:18:39.974000+0000", "id": 207, "url": "/model/tf-efficientnet-b1-ap"}, "method_short": "tf_efficientnet_b1_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b1_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.28%", "Top 5 Accuracy": "94.3%"}, "raw_metrics": {"Top 1 Accuracy": 79.28, "Top 5 Accuracy": 94.3}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25722, "rank": 137, "method": "efficientnet_em", "mlmodel": {"last_updated": "2021-02-14T14:11:43.667000+0000", "id": 95, "url": "/model/efficientnet-em"}, "method_short": "efficientnet_em", "method_details": null, "mlmodel_short": "efficientnet_em", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.26%", "Top 5 Accuracy": "94.79%"}, "raw_metrics": {"Top 1 Accuracy": 79.26, "Top 5 Accuracy": 94.79}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25868, "rank": 138, "method": "ssl_resnet50", "mlmodel": {"last_updated": "2021-02-14T14:17:59.705000+0000", "id": 324, "url": "/model/ssl-resnet50"}, "method_short": "ssl_resnet50", "method_details": null, "mlmodel_short": "ssl_resnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.24%", "Top 5 Accuracy": "94.83%"}, "raw_metrics": {"Top 1 Accuracy": 79.24, "Top 5 Accuracy": 94.83}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25808, "rank": 139, "method": "regnety_040", "mlmodel": {"last_updated": "2021-02-14T14:15:31.510000+0000", "id": 265, "url": "/model/regnety-040"}, "method_short": "regnety_040", "method_details": null, "mlmodel_short": "regnety_040", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.23%", "Top 5 Accuracy": "94.64%"}, "raw_metrics": {"Top 1 Accuracy": 79.23, "Top 5 Accuracy": 94.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25798, "rank": 140, "method": "regnetx_080", "mlmodel": {"last_updated": "2021-02-14T14:15:04.115000+0000", "id": 221, "url": "/model/regnetx-080"}, "method_short": "regnetx_080", "method_details": null, "mlmodel_short": "regnetx_080", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.21%", "Top 5 Accuracy": "94.55%"}, "raw_metrics": {"Top 1 Accuracy": 79.21, "Top 5 Accuracy": 94.55}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25705, "rank": 141, "method": "dpn68b", "mlmodel": {"last_updated": "2021-02-14T14:11:00.450000+0000", "id": 196, "url": "/model/dpn68b"}, "method_short": "dpn68b", "method_details": null, "mlmodel_short": "dpn68b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.21%", "Top 5 Accuracy": "94.42%"}, "raw_metrics": {"Top 1 Accuracy": 79.21, "Top 5 Accuracy": 94.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27662, "rank": 142, "method": "ResNet-50 Semi-supervised", "mlmodel": {"last_updated": "2021-02-27T09:28:15.533000+0000", "id": 865, "url": "/model/resnet-50-semi-supervised"}, "method_short": "ResNet-50 Semi-supervised", "method_details": null, "mlmodel_short": "ResNet-50 Semi-supervised", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.2%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 79.2, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25814, "rank": 143, "method": "res2net101_26w_4s", "mlmodel": {"last_updated": "2021-02-14T14:15:47.013000+0000", "id": 293, "url": "/model/res2net101-26w-4s"}, "method_short": "res2net101_26w_4s", "method_details": null, "mlmodel_short": "res2net101_26w_4s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.19%", "Top 5 Accuracy": "94.43%"}, "raw_metrics": {"Top 1 Accuracy": 79.19, "Top 5 Accuracy": 94.43}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25818, "rank": 144, "method": "res2net50_26w_8s", "mlmodel": {"last_updated": "2021-02-14T14:15:53.830000+0000", "id": 308, "url": "/model/res2net50-26w-8s"}, "method_short": "res2net50_26w_8s", "method_details": null, "mlmodel_short": "res2net50_26w_8s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.19%", "Top 5 Accuracy": "94.37%"}, "raw_metrics": {"Top 1 Accuracy": 79.19, "Top 5 Accuracy": 94.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25776, "rank": 145, "method": "legacy_seresnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:14:09.101000+0000", "id": 300, "url": "/model/legacy-seresnext50-32x4d"}, "method_short": "legacy_seresnext50_32x4d", "method_details": null, "mlmodel_short": "legacy_seresnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.08%", "Top 5 Accuracy": "94.43%"}, "raw_metrics": {"Top 1 Accuracy": 79.08, "Top 5 Accuracy": 94.43}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25797, "rank": 146, "method": "regnetx_064", "mlmodel": {"last_updated": "2021-02-14T14:15:01.711000+0000", "id": 215, "url": "/model/regnetx-064"}, "method_short": "regnetx_064", "method_details": null, "mlmodel_short": "regnetx_064", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.06%", "Top 5 Accuracy": "94.47%"}, "raw_metrics": {"Top 1 Accuracy": 79.06, "Top 5 Accuracy": 94.47}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25742, "rank": 147, "method": "gluon_resnet50_v1d", "mlmodel": {"last_updated": "2021-02-14T14:12:30.069000+0000", "id": 242, "url": "/model/gluon-resnet50-v1d"}, "method_short": "gluon_resnet50_v1d", "method_details": null, "mlmodel_short": "gluon_resnet50_v1d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.06%", "Top 5 Accuracy": "94.46%"}, "raw_metrics": {"Top 1 Accuracy": 79.06, "Top 5 Accuracy": 94.46}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25949, "rank": 148, "method": "xception", "mlmodel": {"last_updated": "2021-02-14T14:21:02.270000+0000", "id": 333, "url": "/model/xception-1"}, "method_short": "xception", "method_details": null, "mlmodel_short": "xception", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.05%", "Top 5 Accuracy": "94.4%"}, "raw_metrics": {"Top 1 Accuracy": 79.05, "Top 5 Accuracy": 94.4}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25838, "rank": 149, "method": "resnet50", "mlmodel": {"last_updated": "2021-02-14T14:16:51.031000+0000", "id": 210, "url": "/model/resnet50"}, "method_short": "resnet50", "method_details": null, "mlmodel_short": "resnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "79.04%", "Top 5 Accuracy": "94.39%"}, "raw_metrics": {"Top 1 Accuracy": 79.04, "Top 5 Accuracy": 94.39}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25777, "rank": 150, "method": "mixnet_l", "mlmodel": {"last_updated": "2021-02-14T14:14:11.754000+0000", "id": 305, "url": "/model/mixnet-l"}, "method_short": "mixnet_l", "method_details": null, "mlmodel_short": "mixnet_l", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.98%", "Top 5 Accuracy": "94.18%"}, "raw_metrics": {"Top 1 Accuracy": 78.98, "Top 5 Accuracy": 94.18}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25757, "rank": 151, "method": "hrnet_w40", "mlmodel": {"last_updated": "2021-02-14T14:13:13.559000+0000", "id": 205, "url": "/model/hrnet-w40"}, "method_short": "hrnet_w40", "method_details": null, "mlmodel_short": "hrnet_w40", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.93%", "Top 5 Accuracy": "94.48%"}, "raw_metrics": {"Top 1 Accuracy": 78.93, "Top 5 Accuracy": 94.48}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25758, "rank": 152, "method": "hrnet_w44", "mlmodel": {"last_updated": "2021-02-14T14:13:17.350000+0000", "id": 212, "url": "/model/hrnet-w44"}, "method_short": "hrnet_w44", "method_details": null, "mlmodel_short": "hrnet_w44", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.89%", "Top 5 Accuracy": "94.37%"}, "raw_metrics": {"Top 1 Accuracy": 78.89, "Top 5 Accuracy": 94.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25947, "rank": 153, "method": "wide_resnet101_2", "mlmodel": {"last_updated": "2021-02-14T14:20:57.086000+0000", "id": 321, "url": "/model/wide-resnet101-2"}, "method_short": "wide_resnet101_2", "method_details": null, "mlmodel_short": "wide_resnet101_2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.85%", "Top 5 Accuracy": "94.28%"}, "raw_metrics": {"Top 1 Accuracy": 78.85, "Top 5 Accuracy": 94.28}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25679, "rank": 154, "method": "Wide ResNet-101-2", "mlmodel": {"last_updated": "2021-02-12T11:32:01.582000+0000", "id": 173, "url": "/model/wide-resnet-101-2"}, "method_short": "Wide ResNet-101-2", "method_details": null, "mlmodel_short": "Wide ResNet-101-2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.84%", "Top 5 Accuracy": "94.28%"}, "raw_metrics": {"Top 1 Accuracy": 78.84, "Top 5 Accuracy": 94.28}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25882, "rank": 155, "method": "tf_efficientnet_b1", "mlmodel": {"last_updated": "2021-02-14T14:18:37.583000+0000", "id": 188, "url": "/model/tf-efficientnet-b1"}, "method_short": "tf_efficientnet_b1", "method_details": null, "mlmodel_short": "tf_efficientnet_b1", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.84%", "Top 5 Accuracy": "94.2%"}, "raw_metrics": {"Top 1 Accuracy": 78.84, "Top 5 Accuracy": 94.2}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25729, "rank": 156, "method": "gluon_inception_v3", "mlmodel": {"last_updated": "2021-02-14T14:11:59.612000+0000", "id": 189, "url": "/model/gluon-inception-v3"}, "method_short": "gluon_inception_v3", "method_details": null, "mlmodel_short": "gluon_inception_v3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.8%", "Top 5 Accuracy": "94.38%"}, "raw_metrics": {"Top 1 Accuracy": 78.8, "Top 5 Accuracy": 94.38}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25919, "rank": 157, "method": "tf_mixnet_l", "mlmodel": {"last_updated": "2021-02-14T14:20:03.631000+0000", "id": 337, "url": "/model/tf-mixnet-l"}, "method_short": "tf_mixnet_l", "method_details": null, "mlmodel_short": "tf_mixnet_l", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.78%", "Top 5 Accuracy": "94.0%"}, "raw_metrics": {"Top 1 Accuracy": 78.78, "Top 5 Accuracy": 94.0}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25909, "rank": 158, "method": "tf_efficientnet_em", "mlmodel": {"last_updated": "2021-02-14T14:19:39.872000+0000", "id": 377, "url": "/model/tf-efficientnet-em"}, "method_short": "tf_efficientnet_em", "method_details": null, "mlmodel_short": "tf_efficientnet_em", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.71%", "Top 5 Accuracy": "94.33%"}, "raw_metrics": {"Top 1 Accuracy": 78.71, "Top 5 Accuracy": 94.33}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25714, "rank": 159, "method": "efficientnet_b1", "mlmodel": {"last_updated": "2021-02-14T14:11:23.834000+0000", "id": 231, "url": "/model/efficientnet-b1"}, "method_short": "efficientnet_b1", "method_details": null, "mlmodel_short": "efficientnet_b1", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.71%", "Top 5 Accuracy": "94.15%"}, "raw_metrics": {"Top 1 Accuracy": 78.71, "Top 5 Accuracy": 94.15}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25743, "rank": 160, "method": "gluon_resnet50_v1s", "mlmodel": {"last_updated": "2021-02-14T14:12:32.177000+0000", "id": 248, "url": "/model/gluon-resnet50-v1s"}, "method_short": "gluon_resnet50_v1s", "method_details": null, "mlmodel_short": "gluon_resnet50_v1s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.7%", "Top 5 Accuracy": "94.25%"}, "raw_metrics": {"Top 1 Accuracy": 78.7, "Top 5 Accuracy": 94.25}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25693, "rank": 161, "method": "dla169", "mlmodel": {"last_updated": "2021-02-14T14:10:37.883000+0000", "id": 391, "url": "/model/dla169"}, "method_short": "dla169", "method_details": null, "mlmodel_short": "dla169", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.69%", "Top 5 Accuracy": "94.33%"}, "raw_metrics": {"Top 1 Accuracy": 78.69, "Top 5 Accuracy": 94.33}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25770, "rank": 162, "method": "legacy_seresnet152", "mlmodel": {"last_updated": "2021-02-14T14:13:54.132000+0000", "id": 273, "url": "/model/legacy-seresnet152"}, "method_short": "legacy_seresnet152", "method_details": null, "mlmodel_short": "legacy_seresnet152", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.67%", "Top 5 Accuracy": "94.38%"}, "raw_metrics": {"Top 1 Accuracy": 78.67, "Top 5 Accuracy": 94.38}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25881, "rank": 163, "method": "tf_efficientnet_b0_ns", "mlmodel": {"last_updated": "2021-02-14T14:18:35.305000+0000", "id": 93, "url": "/model/tf-efficientnet-b0-ns"}, "method_short": "tf_efficientnet_b0_ns", "method_details": null, "mlmodel_short": "tf_efficientnet_b0_ns", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.66%", "Top 5 Accuracy": "94.37%"}, "raw_metrics": {"Top 1 Accuracy": 78.66, "Top 5 Accuracy": 94.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25817, "rank": 164, "method": "res2net50_26w_6s", "mlmodel": {"last_updated": "2021-02-14T14:15:51.800000+0000", "id": 269, "url": "/model/res2net50-26w-6s"}, "method_short": "res2net50_26w_6s", "method_details": null, "mlmodel_short": "res2net50_26w_6s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.57%", "Top 5 Accuracy": "94.12%"}, "raw_metrics": {"Top 1 Accuracy": 78.57, "Top 5 Accuracy": 94.12}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25950, "rank": 165, "method": "xception41", "mlmodel": {"last_updated": "2021-02-14T14:21:04.606000+0000", "id": 339, "url": "/model/xception41"}, "method_short": "xception41", "method_details": null, "mlmodel_short": "xception41", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.54%", "Top 5 Accuracy": "94.28%"}, "raw_metrics": {"Top 1 Accuracy": 78.54, "Top 5 Accuracy": 94.28}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25691, "rank": 166, "method": "dla102x", "mlmodel": {"last_updated": "2021-02-14T14:10:35.058000+0000", "id": 390, "url": "/model/dla102x"}, "method_short": "dla102x", "method_details": null, "mlmodel_short": "dla102x", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.51%", "Top 5 Accuracy": "94.23%"}, "raw_metrics": {"Top 1 Accuracy": 78.51, "Top 5 Accuracy": 94.23}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25680, "rank": 167, "method": "Wide ResNet-50-2", "mlmodel": {"last_updated": "2021-02-12T11:32:03.046000+0000", "id": 162, "url": "/model/wide-resnet-50-2"}, "method_short": "Wide ResNet-50-2", "method_details": null, "mlmodel_short": "Wide ResNet-50-2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.51%", "Top 5 Accuracy": "94.09%"}, "raw_metrics": {"Top 1 Accuracy": 78.51, "Top 5 Accuracy": 94.09}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25825, "rank": 168, "method": "resnest26d", "mlmodel": {"last_updated": "2021-02-14T14:16:12.854000+0000", "id": 349, "url": "/model/resnest26d"}, "method_short": "resnest26d", "method_details": null, "mlmodel_short": "resnest26d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.48%", "Top 5 Accuracy": "94.3%"}, "raw_metrics": {"Top 1 Accuracy": 78.48, "Top 5 Accuracy": 94.3}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25796, "rank": 169, "method": "regnetx_040", "mlmodel": {"last_updated": "2021-02-14T14:14:59.424000+0000", "id": 136, "url": "/model/regnetx-040"}, "method_short": "regnetx_040", "method_details": null, "mlmodel_short": "regnetx_040", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.48%", "Top 5 Accuracy": "94.25%"}, "raw_metrics": {"Top 1 Accuracy": 78.48, "Top 5 Accuracy": 94.25}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25698, "rank": 170, "method": "dla60_res2net", "mlmodel": {"last_updated": "2021-02-14T14:10:46.482000+0000", "id": 394, "url": "/model/dla60-res2net"}, "method_short": "dla60_res2net", "method_details": null, "mlmodel_short": "dla60_res2net", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.46%", "Top 5 Accuracy": "94.21%"}, "raw_metrics": {"Top 1 Accuracy": 78.46, "Top 5 Accuracy": 94.21}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25756, "rank": 171, "method": "hrnet_w32", "mlmodel": {"last_updated": "2021-02-14T14:13:09.852000+0000", "id": 104, "url": "/model/hrnet-w32"}, "method_short": "hrnet_w32", "method_details": null, "mlmodel_short": "hrnet_w32", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.45%", "Top 5 Accuracy": "94.19%"}, "raw_metrics": {"Top 1 Accuracy": 78.45, "Top 5 Accuracy": 94.19}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25699, "rank": 172, "method": "dla60_res2next", "mlmodel": {"last_updated": "2021-02-14T14:10:48.133000+0000", "id": 395, "url": "/model/dla60-res2next"}, "method_short": "dla60_res2next", "method_details": null, "mlmodel_short": "dla60_res2next", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.44%", "Top 5 Accuracy": "94.16%"}, "raw_metrics": {"Top 1 Accuracy": 78.44, "Top 5 Accuracy": 94.16}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25856, "rank": 173, "method": "selecsls60b", "mlmodel": {"last_updated": "2021-02-14T14:17:28.281000+0000", "id": 301, "url": "/model/selecsls60b"}, "method_short": "selecsls60b", "method_details": null, "mlmodel_short": "selecsls60b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.41%", "Top 5 Accuracy": "94.18%"}, "raw_metrics": {"Top 1 Accuracy": 78.41, "Top 5 Accuracy": 94.18}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25769, "rank": 174, "method": "legacy_seresnet101", "mlmodel": {"last_updated": "2021-02-14T14:13:51.515000+0000", "id": 271, "url": "/model/legacy-seresnet101"}, "method_short": "legacy_seresnet101", "method_details": null, "mlmodel_short": "legacy_seresnet101", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.38%", "Top 5 Accuracy": "94.26%"}, "raw_metrics": {"Top 1 Accuracy": 78.38, "Top 5 Accuracy": 94.26}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25936, "rank": 175, "method": "tv_resnet152", "mlmodel": {"last_updated": "2021-02-14T14:20:37.506000+0000", "id": 200, "url": "/model/tv-resnet152"}, "method_short": "tv_resnet152", "method_details": null, "mlmodel_short": "tv_resnet152", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.32%", "Top 5 Accuracy": "94.05%"}, "raw_metrics": {"Top 1 Accuracy": 78.32, "Top 5 Accuracy": 94.05}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25663, "rank": 176, "method": "ResNet-152", "mlmodel": {"last_updated": "2021-02-12T11:31:42.008000+0000", "id": 126, "url": "/model/resnet-152"}, "method_short": "ResNet-152", "method_details": null, "mlmodel_short": "ResNet-152", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.31%", "Top 5 Accuracy": "94.06%"}, "raw_metrics": {"Top 1 Accuracy": 78.31, "Top 5 Accuracy": 94.06}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25700, "rank": 177, "method": "dla60x", "mlmodel": {"last_updated": "2021-02-14T14:10:49.611000+0000", "id": 397, "url": "/model/dla60x"}, "method_short": "dla60x", "method_details": null, "mlmodel_short": "dla60x", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.25%", "Top 5 Accuracy": "94.02%"}, "raw_metrics": {"Top 1 Accuracy": 78.25, "Top 5 Accuracy": 94.02}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25715, "rank": 178, "method": "efficientnet_b1_pruned", "mlmodel": {"last_updated": "2021-02-14T14:11:26.903000+0000", "id": 237, "url": "/model/efficientnet-b1-pruned"}, "method_short": "efficientnet_b1_pruned", "method_details": null, "mlmodel_short": "efficientnet_b1_pruned", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.25%", "Top 5 Accuracy": "93.84%"}, "raw_metrics": {"Top 1 Accuracy": 78.25, "Top 5 Accuracy": 93.84}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25820, "rank": 179, "method": "res2next50", "mlmodel": {"last_updated": "2021-02-14T14:15:57.242000+0000", "id": 317, "url": "/model/res2next50"}, "method_short": "res2next50", "method_details": null, "mlmodel_short": "res2next50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.24%", "Top 5 Accuracy": "93.91%"}, "raw_metrics": {"Top 1 Accuracy": 78.24, "Top 5 Accuracy": 93.91}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25755, "rank": 180, "method": "hrnet_w30", "mlmodel": {"last_updated": "2021-02-14T14:13:05.759000+0000", "id": 124, "url": "/model/hrnet-w30"}, "method_short": "hrnet_w30", "method_details": null, "mlmodel_short": "hrnet_w30", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.21%", "Top 5 Accuracy": "94.22%"}, "raw_metrics": {"Top 1 Accuracy": 78.21, "Top 5 Accuracy": 94.22}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25795, "rank": 181, "method": "regnetx_032", "mlmodel": {"last_updated": "2021-02-14T14:14:56.019000+0000", "id": 208, "url": "/model/regnetx-032"}, "method_short": "regnetx_032", "method_details": null, "mlmodel_short": "regnetx_032", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.15%", "Top 5 Accuracy": "94.09%"}, "raw_metrics": {"Top 1 Accuracy": 78.15, "Top 5 Accuracy": 94.09}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25815, "rank": 182, "method": "res2net50_14w_8s", "mlmodel": {"last_updated": "2021-02-14T14:15:48.646000+0000", "id": 296, "url": "/model/res2net50-14w-8s"}, "method_short": "res2net50_14w_8s", "method_details": null, "mlmodel_short": "res2net50_14w_8s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.14%", "Top 5 Accuracy": "93.86%"}, "raw_metrics": {"Top 1 Accuracy": 78.14, "Top 5 Accuracy": 93.86}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25723, "rank": 183, "method": "efficientnet_es", "mlmodel": {"last_updated": "2021-02-14T14:11:45.845000+0000", "id": 99, "url": "/model/efficientnet-es"}, "method_short": "efficientnet_es", "method_details": null, "mlmodel_short": "efficientnet_es", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.09%", "Top 5 Accuracy": "93.93%"}, "raw_metrics": {"Top 1 Accuracy": 78.09, "Top 5 Accuracy": 93.93}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25690, "rank": 184, "method": "dla102", "mlmodel": {"last_updated": "2021-02-14T14:10:33.788000+0000", "id": 183, "url": "/model/dla102"}, "method_short": "dla102", "method_details": null, "mlmodel_short": "dla102", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.03%", "Top 5 Accuracy": "93.95%"}, "raw_metrics": {"Top 1 Accuracy": 78.03, "Top 5 Accuracy": 93.95}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25741, "rank": 185, "method": "gluon_resnet50_v1c", "mlmodel": {"last_updated": "2021-02-14T14:12:27.728000+0000", "id": 240, "url": "/model/gluon-resnet50-v1c"}, "method_short": "gluon_resnet50_v1c", "method_details": null, "mlmodel_short": "gluon_resnet50_v1c", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "78.01%", "Top 5 Accuracy": "93.99%"}, "raw_metrics": {"Top 1 Accuracy": 78.01, "Top 5 Accuracy": 93.99}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25816, "rank": 186, "method": "res2net50_26w_4s", "mlmodel": {"last_updated": "2021-02-14T14:15:50.184000+0000", "id": 302, "url": "/model/res2net50-26w-4s"}, "method_short": "res2net50_26w_4s", "method_details": null, "mlmodel_short": "res2net50_26w_4s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.99%", "Top 5 Accuracy": "93.85%"}, "raw_metrics": {"Top 1 Accuracy": 77.99, "Top 5 Accuracy": 93.85}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25855, "rank": 187, "method": "selecsls60", "mlmodel": {"last_updated": "2021-02-14T14:17:25.595000+0000", "id": 295, "url": "/model/selecsls60"}, "method_short": "selecsls60", "method_details": null, "mlmodel_short": "selecsls60", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.99%", "Top 5 Accuracy": "93.83%"}, "raw_metrics": {"Top 1 Accuracy": 77.99, "Top 5 Accuracy": 93.83}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25861, "rank": 188, "method": "seresnext26t_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:17:41.935000+0000", "id": 328, "url": "/model/seresnext26t-32x4d"}, "method_short": "seresnext26t_32x4d", "method_details": null, "mlmodel_short": "seresnext26t_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.99%", "Top 5 Accuracy": "93.73%"}, "raw_metrics": {"Top 1 Accuracy": 77.99, "Top 5 Accuracy": 93.73}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25906, "rank": 189, "method": "tf_efficientnet_cc_b0_8e", "mlmodel": {"last_updated": "2021-02-14T14:19:34.685000+0000", "id": 370, "url": "/model/tf-efficientnet-cc-b0-8e"}, "method_short": "tf_efficientnet_cc_b0_8e", "method_details": null, "mlmodel_short": "tf_efficientnet_cc_b0_8e", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.91%", "Top 5 Accuracy": "93.65%"}, "raw_metrics": {"Top 1 Accuracy": 77.91, "Top 5 Accuracy": 93.65}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25806, "rank": 190, "method": "regnety_016", "mlmodel": {"last_updated": "2021-02-14T14:15:26.226000+0000", "id": 256, "url": "/model/regnety-016"}, "method_short": "regnety_016", "method_details": null, "mlmodel_short": "regnety_016", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.87%", "Top 5 Accuracy": "93.73%"}, "raw_metrics": {"Top 1 Accuracy": 77.87, "Top 5 Accuracy": 93.73}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25918, "rank": 191, "method": "tf_inception_v3", "mlmodel": {"last_updated": "2021-02-14T14:20:00.832000+0000", "id": 336, "url": "/model/tf-inception-v3-1"}, "method_short": "tf_inception_v3", "method_details": null, "mlmodel_short": "tf_inception_v3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.87%", "Top 5 Accuracy": "93.65%"}, "raw_metrics": {"Top 1 Accuracy": 77.87, "Top 5 Accuracy": 93.65}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25850, "rank": 192, "method": "rexnet_100", "mlmodel": {"last_updated": "2021-02-14T14:17:13.759000+0000", "id": 274, "url": "/model/rexnet-100"}, "method_short": "rexnet_100", "method_details": null, "mlmodel_short": "rexnet_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.86%", "Top 5 Accuracy": "93.88%"}, "raw_metrics": {"Top 1 Accuracy": 77.86, "Top 5 Accuracy": 93.88}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25946, "rank": 193, "method": "vit_small_patch16_224", "mlmodel": {"last_updated": "2021-02-14T14:20:55.119000+0000", "id": 393, "url": "/model/vit-small-patch16-224"}, "method_short": "vit_small_patch16_224", "method_details": null, "mlmodel_short": "vit_small_patch16_224", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.85%", "Top 5 Accuracy": "93.42%"}, "raw_metrics": {"Top 1 Accuracy": 77.85, "Top 5 Accuracy": 93.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25713, "rank": 194, "method": "efficientnet_b0", "mlmodel": {"last_updated": "2021-02-14T14:11:21.909000+0000", "id": 228, "url": "/model/efficientnet-b0"}, "method_short": "efficientnet_b0", "method_details": null, "mlmodel_short": "efficientnet_b0", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.71%", "Top 5 Accuracy": "93.52%"}, "raw_metrics": {"Top 1 Accuracy": 77.71, "Top 5 Accuracy": 93.52}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25634, "rank": 195, "method": "Densenet-161", "mlmodel": {"last_updated": "2021-02-12T11:31:11.971000+0000", "id": 145, "url": "/model/densenet-161"}, "method_short": "Densenet-161", "method_details": null, "mlmodel_short": "Densenet-161", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.65%", "Top 5 Accuracy": "93.8%"}, "raw_metrics": {"Top 1 Accuracy": 77.65, "Top 5 Accuracy": 93.8}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25773, "rank": 196, "method": "legacy_seresnet50", "mlmodel": {"last_updated": "2021-02-14T14:14:01.833000+0000", "id": 290, "url": "/model/legacy-seresnet50"}, "method_short": "legacy_seresnet50", "method_details": null, "mlmodel_short": "legacy_seresnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.64%", "Top 5 Accuracy": "93.74%"}, "raw_metrics": {"Top 1 Accuracy": 77.64, "Top 5 Accuracy": 93.74}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25657, "rank": 197, "method": "ResNeXt-50-32x4d", "mlmodel": {"last_updated": "2021-02-12T11:31:35.582000+0000", "id": 127, "url": "/model/resnext-50-32x4d"}, "method_short": "ResNeXt-50-32x4d", "method_details": null, "mlmodel_short": "ResNeXt-50-32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.62%", "Top 5 Accuracy": "93.7%"}, "raw_metrics": {"Top 1 Accuracy": 77.62, "Top 5 Accuracy": 93.7}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25939, "rank": 198, "method": "tv_resnext50_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:20:43.063000+0000", "id": 257, "url": "/model/tv-resnext50-32x4d"}, "method_short": "tv_resnext50_32x4d", "method_details": null, "mlmodel_short": "tv_resnext50_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.61%", "Top 5 Accuracy": "93.68%"}, "raw_metrics": {"Top 1 Accuracy": 77.61, "Top 5 Accuracy": 93.68}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25860, "rank": 199, "method": "seresnext26d_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:17:39.535000+0000", "id": 320, "url": "/model/seresnext26d-32x4d"}, "method_short": "seresnext26d_32x4d", "method_details": null, "mlmodel_short": "seresnext26d_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.59%", "Top 5 Accuracy": "93.61%"}, "raw_metrics": {"Top 1 Accuracy": 77.59, "Top 5 Accuracy": 93.61}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25681, "rank": 200, "method": "adv_inception_v3", "mlmodel": {"last_updated": "2021-02-14T14:10:18.512000+0000", "id": 378, "url": "/model/adv-inception-v3"}, "method_short": "adv_inception_v3", "method_details": null, "mlmodel_short": "adv_inception_v3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.58%", "Top 5 Accuracy": "93.74%"}, "raw_metrics": {"Top 1 Accuracy": 77.58, "Top 5 Accuracy": 93.74}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25740, "rank": 201, "method": "gluon_resnet50_v1b", "mlmodel": {"last_updated": "2021-02-14T14:12:25.308000+0000", "id": 225, "url": "/model/gluon-resnet50-v1b"}, "method_short": "gluon_resnet50_v1b", "method_details": null, "mlmodel_short": "gluon_resnet50_v1b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.58%", "Top 5 Accuracy": "93.72%"}, "raw_metrics": {"Top 1 Accuracy": 77.58, "Top 5 Accuracy": 93.72}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25819, "rank": 202, "method": "res2net50_48w_2s", "mlmodel": {"last_updated": "2021-02-14T14:15:55.676000+0000", "id": 314, "url": "/model/res2net50-48w-2s"}, "method_short": "res2net50_48w_2s", "method_details": null, "mlmodel_short": "res2net50_48w_2s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.53%", "Top 5 Accuracy": "93.56%"}, "raw_metrics": {"Top 1 Accuracy": 77.53, "Top 5 Accuracy": 93.56}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25915, "rank": 203, "method": "tf_efficientnet_lite2", "mlmodel": {"last_updated": "2021-02-14T14:19:53.782000+0000", "id": 388, "url": "/model/tf-efficientnet-lite2"}, "method_short": "tf_efficientnet_lite2", "method_details": null, "mlmodel_short": "tf_efficientnet_lite2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.48%", "Top 5 Accuracy": "93.75%"}, "raw_metrics": {"Top 1 Accuracy": 77.48, "Top 5 Accuracy": 93.75}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25766, "rank": 204, "method": "inception_v3", "mlmodel": {"last_updated": "2021-02-14T14:13:44.234000+0000", "id": 255, "url": "/model/inception-v3-1"}, "method_short": "inception_v3", "method_details": null, "mlmodel_short": "inception_v3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.46%", "Top 5 Accuracy": "93.48%"}, "raw_metrics": {"Top 1 Accuracy": 77.46, "Top 5 Accuracy": 93.48}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25643, "rank": 205, "method": "Inception v3", "mlmodel": {"last_updated": "2021-02-12T11:31:24.264000+0000", "id": 43, "url": "/model/inception-v3"}, "method_short": "Inception v3", "method_details": null, "mlmodel_short": "Inception v3", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.45%", "Top 5 Accuracy": "93.56%"}, "raw_metrics": {"Top 1 Accuracy": 77.45, "Top 5 Accuracy": 93.56}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25661, "rank": 206, "method": "ResNet-101", "mlmodel": {"last_updated": "2021-02-12T11:31:40.168000+0000", "id": 122, "url": "/model/resnet-101"}, "method_short": "ResNet-101", "method_details": null, "mlmodel_short": "ResNet-101", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.37%", "Top 5 Accuracy": "93.56%"}, "raw_metrics": {"Top 1 Accuracy": 77.37, "Top 5 Accuracy": 93.56}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25935, "rank": 207, "method": "tv_resnet101", "mlmodel": {"last_updated": "2021-02-14T14:20:35.614000+0000", "id": 144, "url": "/model/tv-resnet101"}, "method_short": "tv_resnet101", "method_details": null, "mlmodel_short": "tv_resnet101", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.37%", "Top 5 Accuracy": "93.56%"}, "raw_metrics": {"Top 1 Accuracy": 77.37, "Top 5 Accuracy": 93.56}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25686, "rank": 208, "method": "densenet161", "mlmodel": {"last_updated": "2021-02-14T14:10:27.443000+0000", "id": 386, "url": "/model/densenet161"}, "method_short": "densenet161", "method_details": null, "mlmodel_short": "densenet161", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.36%", "Top 5 Accuracy": "93.63%"}, "raw_metrics": {"Top 1 Accuracy": 77.36, "Top 5 Accuracy": 93.63}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25905, "rank": 209, "method": "tf_efficientnet_cc_b0_4e", "mlmodel": {"last_updated": "2021-02-14T14:19:33.042000+0000", "id": 368, "url": "/model/tf-efficientnet-cc-b0-4e"}, "method_short": "tf_efficientnet_cc_b0_4e", "method_details": null, "mlmodel_short": "tf_efficientnet_cc_b0_4e", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.32%", "Top 5 Accuracy": "93.32%"}, "raw_metrics": {"Top 1 Accuracy": 77.32, "Top 5 Accuracy": 93.32}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25688, "rank": 210, "method": "densenet201", "mlmodel": {"last_updated": "2021-02-14T14:10:31.050000+0000", "id": 326, "url": "/model/densenet201"}, "method_short": "densenet201", "method_details": null, "mlmodel_short": "densenet201", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.29%", "Top 5 Accuracy": "93.48%"}, "raw_metrics": {"Top 1 Accuracy": 77.29, "Top 5 Accuracy": 93.48}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25910, "rank": 211, "method": "tf_efficientnet_es", "mlmodel": {"last_updated": "2021-02-14T14:19:41.832000+0000", "id": 380, "url": "/model/tf-efficientnet-es"}, "method_short": "tf_efficientnet_es", "method_details": null, "mlmodel_short": "tf_efficientnet_es", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.28%", "Top 5 Accuracy": "93.6%"}, "raw_metrics": {"Top 1 Accuracy": 77.28, "Top 5 Accuracy": 93.6}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25784, "rank": 212, "method": "mobilenetv2_120d", "mlmodel": {"last_updated": "2021-02-14T14:14:28.033000+0000", "id": 340, "url": "/model/mobilenetv2-120d"}, "method_short": "mobilenetv2_120d", "method_details": null, "mlmodel_short": "mobilenetv2_120d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.28%", "Top 5 Accuracy": "93.51%"}, "raw_metrics": {"Top 1 Accuracy": 77.28, "Top 5 Accuracy": 93.51}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25778, "rank": 213, "method": "mixnet_m", "mlmodel": {"last_updated": "2021-02-14T14:14:14.093000+0000", "id": 309, "url": "/model/mixnet-m"}, "method_short": "mixnet_m", "method_details": null, "mlmodel_short": "mixnet_m", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.27%", "Top 5 Accuracy": "93.42%"}, "raw_metrics": {"Top 1 Accuracy": 77.27, "Top 5 Accuracy": 93.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27659, "rank": 214, "method": "ResNet-101 (supervised, torchvision)", "mlmodel": {"last_updated": null, "id": 877, "url": "/model/resnet-101-supervised-torchvision"}, "method_short": "ResNet-101 ", "method_details": "supervised, torchvision", "mlmodel_short": "ResNet-101 ", "mlmodeldetails": "supervised, torchvision", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.21%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 77.21, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25636, "rank": 215, "method": "Densenet-201", "mlmodel": {"last_updated": "2021-02-12T11:31:14.158000+0000", "id": 138, "url": "/model/densenet-201"}, "method_short": "Densenet-201", "method_details": null, "mlmodel_short": "Densenet-201", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.2%", "Top 5 Accuracy": "93.57%"}, "raw_metrics": {"Top 1 Accuracy": 77.2, "Top 5 Accuracy": 93.57}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25854, "rank": 216, "method": "selecsls42b", "mlmodel": {"last_updated": "2021-02-14T14:17:23.485000+0000", "id": 292, "url": "/model/selecsls42b"}, "method_short": "selecsls42b", "method_details": null, "mlmodel_short": "selecsls42b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.18%", "Top 5 Accuracy": "93.39%"}, "raw_metrics": {"Top 1 Accuracy": 77.18, "Top 5 Accuracy": 93.39}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25837, "rank": 217, "method": "resnet34d", "mlmodel": {"last_updated": "2021-02-14T14:16:48.510000+0000", "id": 203, "url": "/model/resnet34d"}, "method_short": "resnet34d", "method_details": null, "mlmodel_short": "resnet34d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.11%", "Top 5 Accuracy": "93.38%"}, "raw_metrics": {"Top 1 Accuracy": 77.11, "Top 5 Accuracy": 93.38}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25775, "rank": 218, "method": "legacy_seresnext26_32x4d", "mlmodel": {"last_updated": "2021-02-14T14:14:06.974000+0000", "id": 297, "url": "/model/legacy-seresnext26-32x4d"}, "method_short": "legacy_seresnext26_32x4d", "method_details": null, "mlmodel_short": "legacy_seresnext26_32x4d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.11%", "Top 5 Accuracy": "93.31%"}, "raw_metrics": {"Top 1 Accuracy": 77.11, "Top 5 Accuracy": 93.31}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25880, "rank": 219, "method": "tf_efficientnet_b0_ap", "mlmodel": {"last_updated": "2021-02-14T14:18:32.934000+0000", "id": 381, "url": "/model/tf-efficientnet-b0-ap"}, "method_short": "tf_efficientnet_b0_ap", "method_details": null, "mlmodel_short": "tf_efficientnet_b0_ap", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.1%", "Top 5 Accuracy": "93.26%"}, "raw_metrics": {"Top 1 Accuracy": 77.1, "Top 5 Accuracy": 93.26}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25697, "rank": 220, "method": "dla60", "mlmodel": {"last_updated": "2021-02-14T14:10:43.833000+0000", "id": 112, "url": "/model/dla60"}, "method_short": "dla60", "method_details": null, "mlmodel_short": "dla60", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.04%", "Top 5 Accuracy": "93.32%"}, "raw_metrics": {"Top 1 Accuracy": 77.04, "Top 5 Accuracy": 93.32}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27695, "rank": 221, "method": "SwAV ResNet-50-w4 (400 epochs, 2x224+6x96, 2560 bs)", "mlmodel": {"last_updated": "2021-02-27T09:29:14.214000+0000", "id": 906, "url": "/model/swav-resnet-50-w4-400-epochs-2x224-6x96-2560"}, "method_short": "SwAV ResNet-50-w4 ", "method_details": "400 epochs, 2x224+6x96, 2560 bs", "mlmodel_short": "SwAV ResNet-50-w4 ", "mlmodeldetails": "400 epochs, 2x224+6x96, 2560 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.03%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 77.03, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27674, "rank": 222, "method": "SwAV ResNet-50-w2 (400 epochs, 2x224+6x96, 4096 bs)", "mlmodel": {"last_updated": "2021-02-27T09:29:12.226000+0000", "id": 902, "url": "/model/swav-resnet-50-w2-400-epochs-2x224-6x96-4096"}, "method_short": "SwAV ResNet-50-w2 ", "method_details": "400 epochs, 2x224+6x96, 4096 bs", "mlmodel_short": "SwAV ResNet-50-w2 ", "mlmodeldetails": "400 epochs, 2x224+6x96, 4096 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "77.01%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 77.01, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25920, "rank": 223, "method": "tf_mixnet_m", "mlmodel": {"last_updated": "2021-02-14T14:20:06.094000+0000", "id": 341, "url": "/model/tf-mixnet-m"}, "method_short": "tf_mixnet_m", "method_details": null, "mlmodel_short": "tf_mixnet_m", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.96%", "Top 5 Accuracy": "93.16%"}, "raw_metrics": {"Top 1 Accuracy": 76.96, "Top 5 Accuracy": 93.16}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25794, "rank": 224, "method": "regnetx_016", "mlmodel": {"last_updated": "2021-02-14T14:14:53.883000+0000", "id": 201, "url": "/model/regnetx-016"}, "method_short": "regnetx_016", "method_details": null, "mlmodel_short": "regnetx_016", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.95%", "Top 5 Accuracy": "93.43%"}, "raw_metrics": {"Top 1 Accuracy": 76.95, "Top 5 Accuracy": 93.43}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25864, "rank": 225, "method": "skresnet34", "mlmodel": {"last_updated": "2021-02-14T14:17:48.672000+0000", "id": 346, "url": "/model/skresnet34"}, "method_short": "skresnet34", "method_details": null, "mlmodel_short": "skresnet34", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.93%", "Top 5 Accuracy": "93.32%"}, "raw_metrics": {"Top 1 Accuracy": 76.93, "Top 5 Accuracy": 93.32}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25879, "rank": 226, "method": "tf_efficientnet_b0", "mlmodel": {"last_updated": "2021-02-14T14:18:30.819000+0000", "id": 379, "url": "/model/tf-efficientnet-b0"}, "method_short": "tf_efficientnet_b0", "method_details": null, "mlmodel_short": "tf_efficientnet_b0", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.85%", "Top 5 Accuracy": "93.23%"}, "raw_metrics": {"Top 1 Accuracy": 76.85, "Top 5 Accuracy": 93.23}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25726, "rank": 227, "method": "ese_vovnet19b_dw", "mlmodel": {"last_updated": "2021-02-14T14:11:52.906000+0000", "id": 172, "url": "/model/ese-vovnet19b-dw"}, "method_short": "ese_vovnet19b_dw", "method_details": null, "mlmodel_short": "ese_vovnet19b_dw", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.82%", "Top 5 Accuracy": "93.28%"}, "raw_metrics": {"Top 1 Accuracy": 76.82, "Top 5 Accuracy": 93.28}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25752, "rank": 228, "method": "hrnet_w18", "mlmodel": {"last_updated": "2021-02-14T14:12:53.926000+0000", "id": 116, "url": "/model/hrnet-w18"}, "method_short": "hrnet_w18", "method_details": null, "mlmodel_short": "hrnet_w18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.76%", "Top 5 Accuracy": "93.44%"}, "raw_metrics": {"Top 1 Accuracy": 76.76, "Top 5 Accuracy": 93.44}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25835, "rank": 229, "method": "resnet26d", "mlmodel": {"last_updated": "2021-02-14T14:16:43.547000+0000", "id": 197, "url": "/model/resnet26d"}, "method_short": "resnet26d", "method_details": null, "mlmodel_short": "resnet26d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.69%", "Top 5 Accuracy": "93.15%"}, "raw_metrics": {"Top 1 Accuracy": 76.69, "Top 5 Accuracy": 93.15}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25914, "rank": 230, "method": "tf_efficientnet_lite1", "mlmodel": {"last_updated": "2021-02-14T14:19:51.544000+0000", "id": 387, "url": "/model/tf-efficientnet-lite1"}, "method_short": "tf_efficientnet_lite1", "method_details": null, "mlmodel_short": "tf_efficientnet_lite1", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.67%", "Top 5 Accuracy": "93.24%"}, "raw_metrics": {"Top 1 Accuracy": 76.67, "Top 5 Accuracy": 93.24}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25689, "rank": 231, "method": "densenetblur121d", "mlmodel": {"last_updated": "2021-02-14T14:10:32.443000+0000", "id": 335, "url": "/model/densenetblur121d"}, "method_short": "densenetblur121d", "method_details": null, "mlmodel_short": "densenetblur121d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.59%", "Top 5 Accuracy": "93.2%"}, "raw_metrics": {"Top 1 Accuracy": 76.59, "Top 5 Accuracy": 93.2}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25785, "rank": 232, "method": "mobilenetv2_140", "mlmodel": {"last_updated": "2021-02-14T14:14:30.363000+0000", "id": 345, "url": "/model/mobilenetv2-140"}, "method_short": "mobilenetv2_140", "method_details": null, "mlmodel_short": "mobilenetv2_140", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.51%", "Top 5 Accuracy": "93.0%"}, "raw_metrics": {"Top 1 Accuracy": 76.51, "Top 5 Accuracy": 93.0}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25805, "rank": 233, "method": "regnety_008", "mlmodel": {"last_updated": "2021-02-14T14:15:23.837000+0000", "id": 253, "url": "/model/regnety-008"}, "method_short": "regnety_008", "method_details": null, "mlmodel_short": "regnety_008", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.32%", "Top 5 Accuracy": "93.07%"}, "raw_metrics": {"Top 1 Accuracy": 76.32, "Top 5 Accuracy": 93.07}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25704, "rank": 234, "method": "dpn68", "mlmodel": {"last_updated": "2021-02-14T14:10:57.907000+0000", "id": 190, "url": "/model/dpn68"}, "method_short": "dpn68", "method_details": null, "mlmodel_short": "dpn68", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.31%", "Top 5 Accuracy": "92.97%"}, "raw_metrics": {"Top 1 Accuracy": 76.31, "Top 5 Accuracy": 92.97}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25938, "rank": 235, "method": "tv_resnet50", "mlmodel": {"last_updated": "2021-02-14T14:20:41.088000+0000", "id": 238, "url": "/model/tv-resnet50"}, "method_short": "tv_resnet50", "method_details": null, "mlmodel_short": "tv_resnet50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.16%", "Top 5 Accuracy": "92.88%"}, "raw_metrics": {"Top 1 Accuracy": 76.16, "Top 5 Accuracy": 92.88}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25666, "rank": 236, "method": "ResNet-50", "mlmodel": {"last_updated": "2021-02-12T11:31:46.105000+0000", "id": 153, "url": "/model/resnet-50"}, "method_short": "ResNet-50", "method_details": null, "mlmodel_short": "ResNet-50", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.15%", "Top 5 Accuracy": "92.87%"}, "raw_metrics": {"Top 1 Accuracy": 76.15, "Top 5 Accuracy": 92.87}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27699, "rank": 237, "method": "ResNet-50 (supervised, torchvision)", "mlmodel": {"last_updated": null, "id": 880, "url": "/model/resnet-50-supervised-torchvision"}, "method_short": "ResNet-50 ", "method_details": "supervised, torchvision", "mlmodel_short": "ResNet-50 ", "mlmodeldetails": "supervised, torchvision", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.1%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 76.1, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25635, "rank": 238, "method": "Densenet-169", "mlmodel": {"last_updated": "2021-02-12T11:31:12.891000+0000", "id": 143, "url": "/model/densenet-169"}, "method_short": "Densenet-169", "method_details": null, "mlmodel_short": "Densenet-169", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "76.0%", "Top 5 Accuracy": "93.0%"}, "raw_metrics": {"Top 1 Accuracy": 76.0, "Top 5 Accuracy": 93.0}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25779, "rank": 239, "method": "mixnet_s", "mlmodel": {"last_updated": "2021-02-14T14:14:16.587000+0000", "id": 315, "url": "/model/mixnet-s"}, "method_short": "mixnet_s", "method_details": null, "mlmodel_short": "mixnet_s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.99%", "Top 5 Accuracy": "92.79%"}, "raw_metrics": {"Top 1 Accuracy": 75.99, "Top 5 Accuracy": 92.79}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25687, "rank": 240, "method": "densenet169", "mlmodel": {"last_updated": "2021-02-14T14:10:29.264000+0000", "id": 389, "url": "/model/densenet169"}, "method_short": "densenet169", "method_details": null, "mlmodel_short": "densenet169", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.9%", "Top 5 Accuracy": "93.02%"}, "raw_metrics": {"Top 1 Accuracy": 75.9, "Top 5 Accuracy": 93.02}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27660, "rank": 241, "method": "ResNet-50 (supervised, caffe2)", "mlmodel": {"last_updated": null, "id": 878, "url": "/model/resnet-50-supervised-caffe2"}, "method_short": "ResNet-50 ", "method_details": "supervised, caffe2", "mlmodel_short": "ResNet-50 ", "mlmodeldetails": "supervised, caffe2", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.88%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 75.88, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25786, "rank": 242, "method": "mobilenetv3_large_100", "mlmodel": {"last_updated": "2021-02-14T14:14:32.593000+0000", "id": 348, "url": "/model/mobilenetv3-large-100"}, "method_short": "mobilenetv3_large_100", "method_details": null, "mlmodel_short": "mobilenetv3_large_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.77%", "Top 5 Accuracy": "92.54%"}, "raw_metrics": {"Top 1 Accuracy": 75.77, "Top 5 Accuracy": 92.54}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25921, "rank": 243, "method": "tf_mixnet_s", "mlmodel": {"last_updated": "2021-02-14T14:20:08.439000+0000", "id": 342, "url": "/model/tf-mixnet-s"}, "method_short": "tf_mixnet_s", "method_details": null, "mlmodel_short": "tf_mixnet_s", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.68%", "Top 5 Accuracy": "92.64%"}, "raw_metrics": {"Top 1 Accuracy": 75.68, "Top 5 Accuracy": 92.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25787, "rank": 244, "method": "mobilenetv3_rw", "mlmodel": {"last_updated": "2021-02-14T14:14:35.683000+0000", "id": 175, "url": "/model/mobilenetv3-rw"}, "method_short": "mobilenetv3_rw", "method_details": null, "mlmodel_short": "mobilenetv3_rw", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.62%", "Top 5 Accuracy": "92.71%"}, "raw_metrics": {"Top 1 Accuracy": 75.62, "Top 5 Accuracy": 92.71}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25685, "rank": 245, "method": "densenet121", "mlmodel": {"last_updated": "2021-02-14T14:10:25.754000+0000", "id": 313, "url": "/model/densenet121"}, "method_short": "densenet121", "method_details": null, "mlmodel_short": "densenet121", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.56%", "Top 5 Accuracy": "92.65%"}, "raw_metrics": {"Top 1 Accuracy": 75.56, "Top 5 Accuracy": 92.65}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25923, "rank": 246, "method": "tf_mobilenetv3_large_100", "mlmodel": {"last_updated": "2021-02-14T14:20:13.183000+0000", "id": 245, "url": "/model/tf-mobilenetv3-large-100"}, "method_short": "tf_mobilenetv3_large_100", "method_details": null, "mlmodel_short": "tf_mobilenetv3_large_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.51%", "Top 5 Accuracy": "92.61%"}, "raw_metrics": {"Top 1 Accuracy": 75.51, "Top 5 Accuracy": 92.61}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25822, "rank": 247, "method": "resnest14d", "mlmodel": {"last_updated": "2021-02-14T14:16:02.995000+0000", "id": 331, "url": "/model/resnest14d"}, "method_short": "resnest14d", "method_details": null, "mlmodel_short": "resnest14d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.51%", "Top 5 Accuracy": "92.52%"}, "raw_metrics": {"Top 1 Accuracy": 75.51, "Top 5 Accuracy": 92.52}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25724, "rank": 248, "method": "efficientnet_lite0", "mlmodel": {"last_updated": "2021-02-14T14:11:47.844000+0000", "id": 106, "url": "/model/efficientnet-lite0"}, "method_short": "efficientnet_lite0", "method_details": null, "mlmodel_short": "efficientnet_lite0", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.5%", "Top 5 Accuracy": "92.51%"}, "raw_metrics": {"Top 1 Accuracy": 75.5, "Top 5 Accuracy": 92.51}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 5, "name": "EfficientNet", "color": "#05A300"}], "reports": []}, {"table_id": 116, "row_id": 25857, "rank": 249, "method": "semnasnet_100", "mlmodel": {"last_updated": "2021-02-14T14:17:31.340000+0000", "id": 268, "url": "/model/semnasnet-100"}, "method_short": "semnasnet_100", "method_details": null, "mlmodel_short": "semnasnet_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.45%", "Top 5 Accuracy": "92.61%"}, "raw_metrics": {"Top 1 Accuracy": 75.45, "Top 5 Accuracy": 92.61}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27665, "rank": 250, "method": "ResNet-50 (supervised, vissl)", "mlmodel": {"last_updated": null, "id": 885, "url": "/model/resnet-50-supervised-vissl"}, "method_short": "ResNet-50 ", "method_details": "supervised, vissl", "mlmodel_short": "ResNet-50 ", "mlmodeldetails": "supervised, vissl", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.45%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 75.45, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25834, "rank": 251, "method": "resnet26", "mlmodel": {"last_updated": "2021-02-14T14:16:40.684000+0000", "id": 195, "url": "/model/resnet26"}, "method_short": "resnet26", "method_details": null, "mlmodel_short": "resnet26", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.29%", "Top 5 Accuracy": "92.57%"}, "raw_metrics": {"Top 1 Accuracy": 75.29, "Top 5 Accuracy": 92.57}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25804, "rank": 252, "method": "regnety_006", "mlmodel": {"last_updated": "2021-02-14T14:15:20.441000+0000", "id": 247, "url": "/model/regnety-006"}, "method_short": "regnety_006", "method_details": null, "mlmodel_short": "regnety_006", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.27%", "Top 5 Accuracy": "92.53%"}, "raw_metrics": {"Top 1 Accuracy": 75.27, "Top 5 Accuracy": 92.53}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27669, "rank": 253, "method": "DeepClusterV2 ResNet-50 (800 epochs, 2x224+6x96)", "mlmodel": {"last_updated": "2021-02-27T09:27:12.857000+0000", "id": 896, "url": "/model/deepclusterv2-resnet-50-800-epochs-2x224-6x96"}, "method_short": "DeepClusterV2 ResNet-50 ", "method_details": "800 epochs, 2x224+6x96", "mlmodel_short": "DeepClusterV2 ResNet-50 ", "mlmodeldetails": "800 epochs, 2x224+6x96", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.18%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 75.18, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25728, "rank": 254, "method": "fbnetc_100", "mlmodel": {"last_updated": "2021-02-14T14:11:57.397000+0000", "id": 187, "url": "/model/fbnetc-100"}, "method_short": "fbnetc_100", "method_details": null, "mlmodel_short": "fbnetc_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.12%", "Top 5 Accuracy": "92.37%"}, "raw_metrics": {"Top 1 Accuracy": 75.12, "Top 5 Accuracy": 92.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25754, "rank": 255, "method": "hrnet_w18_small_v2", "mlmodel": {"last_updated": "2021-02-14T14:13:01.594000+0000", "id": 193, "url": "/model/hrnet-w18-small-v2"}, "method_short": "hrnet_w18_small_v2", "method_details": null, "mlmodel_short": "hrnet_w18_small_v2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.11%", "Top 5 Accuracy": "92.41%"}, "raw_metrics": {"Top 1 Accuracy": 75.11, "Top 5 Accuracy": 92.41}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25836, "rank": 256, "method": "resnet34", "mlmodel": {"last_updated": "2021-02-14T14:16:45.891000+0000", "id": 88, "url": "/model/resnet34"}, "method_short": "resnet34", "method_details": null, "mlmodel_short": "resnet34", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.11%", "Top 5 Accuracy": "92.28%"}, "raw_metrics": {"Top 1 Accuracy": 75.11, "Top 5 Accuracy": 92.28}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25793, "rank": 257, "method": "regnetx_008", "mlmodel": {"last_updated": "2021-02-14T14:14:51.818000+0000", "id": 103, "url": "/model/regnetx-008"}, "method_short": "regnetx_008", "method_details": null, "mlmodel_short": "regnetx_008", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.05%", "Top 5 Accuracy": "92.34%"}, "raw_metrics": {"Top 1 Accuracy": 75.05, "Top 5 Accuracy": 92.34}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25783, "rank": 258, "method": "mobilenetv2_110d", "mlmodel": {"last_updated": "2021-02-14T14:14:25.685000+0000", "id": 334, "url": "/model/mobilenetv2-110d"}, "method_short": "mobilenetv2_110d", "method_details": null, "mlmodel_short": "mobilenetv2_110d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "75.05%", "Top 5 Accuracy": "92.19%"}, "raw_metrics": {"Top 1 Accuracy": 75.05, "Top 5 Accuracy": 92.19}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27650, "rank": 259, "method": "SwAV ResNet-50 (800 epochs, 2x224+6x96, 4096 bs)", "mlmodel": {"last_updated": "2021-02-27T09:29:09.709000+0000", "id": 899, "url": "/model/swav-resnet-50-800-epochs-2x224-6x96-4096-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "800 epochs, 2x224+6x96, 4096 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "800 epochs, 2x224+6x96, 4096 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.92%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 74.92, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25913, "rank": 260, "method": "tf_efficientnet_lite0", "mlmodel": {"last_updated": "2021-02-14T14:19:49.300000+0000", "id": 385, "url": "/model/tf-efficientnet-lite0"}, "method_short": "tf_efficientnet_lite0", "method_details": null, "mlmodel_short": "tf_efficientnet_lite0", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.83%", "Top 5 Accuracy": "92.17%"}, "raw_metrics": {"Top 1 Accuracy": 74.83, "Top 5 Accuracy": 92.17}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27684, "rank": 261, "method": "SwAV ResNet-50 (400 epochs, 2x224+6x96, 4096 bs)", "mlmodel": {"last_updated": "2021-02-27T09:29:04.230000+0000", "id": 868, "url": "/model/swav-resnet-50-400-epochs-2x224-6x96-4096-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "400 epochs, 2x224+6x96, 4096 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "400 epochs, 2x224+6x96, 4096 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.81%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 74.81, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25772, "rank": 262, "method": "legacy_seresnet34", "mlmodel": {"last_updated": "2021-02-14T14:13:59.122000+0000", "id": 283, "url": "/model/legacy-seresnet34"}, "method_short": "legacy_seresnet34", "method_details": null, "mlmodel_short": "legacy_seresnet34", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.79%", "Top 5 Accuracy": "92.13%"}, "raw_metrics": {"Top 1 Accuracy": 74.79, "Top 5 Accuracy": 92.13}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25934, "rank": 263, "method": "tv_densenet121", "mlmodel": {"last_updated": "2021-02-14T14:20:34.265000+0000", "id": 185, "url": "/model/tv-densenet121"}, "method_short": "tv_densenet121", "method_details": null, "mlmodel_short": "tv_densenet121", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.74%", "Top 5 Accuracy": "92.15%"}, "raw_metrics": {"Top 1 Accuracy": 74.74, "Top 5 Accuracy": 92.15}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25781, "rank": 264, "method": "mnasnet_100", "mlmodel": {"last_updated": "2021-02-14T14:14:21.655000+0000", "id": 322, "url": "/model/mnasnet-100"}, "method_short": "mnasnet_100", "method_details": null, "mlmodel_short": "mnasnet_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.67%", "Top 5 Accuracy": "92.1%"}, "raw_metrics": {"Top 1 Accuracy": 74.67, "Top 5 Accuracy": 92.1}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25654, "rank": 265, "method": "Densenet-121", "mlmodel": {"last_updated": "2021-02-12T11:31:11.074000+0000", "id": 142, "url": "/model/densenet-121"}, "method_short": "Densenet-121", "method_details": null, "mlmodel_short": "Densenet-121", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.65%", "Top 5 Accuracy": "92.17%"}, "raw_metrics": {"Top 1 Accuracy": 74.65, "Top 5 Accuracy": 92.17}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25694, "rank": 266, "method": "dla34", "mlmodel": {"last_updated": "2021-02-14T14:10:39.394000+0000", "id": 396, "url": "/model/dla34"}, "method_short": "dla34", "method_details": null, "mlmodel_short": "dla34", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.62%", "Top 5 Accuracy": "92.06%"}, "raw_metrics": {"Top 1 Accuracy": 74.62, "Top 5 Accuracy": 92.06}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25739, "rank": 267, "method": "gluon_resnet34_v1b", "mlmodel": {"last_updated": "2021-02-14T14:12:23.104000+0000", "id": 232, "url": "/model/gluon-resnet34-v1b"}, "method_short": "gluon_resnet34_v1b", "method_details": null, "mlmodel_short": "gluon_resnet34_v1b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.59%", "Top 5 Accuracy": "92.0%"}, "raw_metrics": {"Top 1 Accuracy": 74.59, "Top 5 Accuracy": 92.0}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27679, "rank": 268, "method": "DeepClusterV2 ResNet-50 (400 epochs, 2x160+4x96)", "mlmodel": {"last_updated": "2021-02-27T09:27:07.466000+0000", "id": 873, "url": "/model/deepclusterv2-resnet-50-400-epochs-2x160-4x96"}, "method_short": "DeepClusterV2 ResNet-50 ", "method_details": "400 epochs, 2x160+4x96", "mlmodel_short": "DeepClusterV2 ResNet-50 ", "mlmodeldetails": "400 epochs, 2x160+4x96", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.32%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 74.32, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27649, "rank": 269, "method": "SwAV ResNet-50 (400 epochs, 2x224+6x96, 256 bs)", "mlmodel": {"last_updated": "2021-02-27T09:29:01.491000+0000", "id": 914, "url": "/model/swav-resnet-50-400-epochs-2x224-6x96-256-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "400 epochs, 2x224+6x96, 256 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "400 epochs, 2x224+6x96, 256 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.3%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 74.3, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25678, "rank": 270, "method": "VGG-19 with batch normalization", "mlmodel": {"last_updated": "2021-02-12T11:32:00.054000+0000", "id": 171, "url": "/model/vgg-19-with-batch-normalization"}, "method_short": "VGG-19 with batch normalization", "method_details": null, "mlmodel_short": "VGG-19 with batch normalization", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.24%", "Top 5 Accuracy": "91.85%"}, "raw_metrics": {"Top 1 Accuracy": 74.24, "Top 5 Accuracy": 91.85}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25866, "rank": 271, "method": "spnasnet_100", "mlmodel": {"last_updated": "2021-02-14T14:17:54.196000+0000", "id": 355, "url": "/model/spnasnet-100"}, "method_short": "spnasnet_100", "method_details": null, "mlmodel_short": "spnasnet_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.08%", "Top 5 Accuracy": "91.82%"}, "raw_metrics": {"Top 1 Accuracy": 74.08, "Top 5 Accuracy": 91.82}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25653, "rank": 272, "method": "MobileNet V3 Large", "mlmodel": {"last_updated": "2021-02-12T11:31:31.752000+0000", "id": 150, "url": "/model/mobilenet-v3-large"}, "method_short": "MobileNet V3 Large", "method_details": null, "mlmodel_short": "MobileNet V3 Large", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.042%", "Top 5 Accuracy": "91.340%"}, "raw_metrics": {"Top 1 Accuracy": 74.042, "Top 5 Accuracy": 91.34}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25803, "rank": 273, "method": "regnety_004", "mlmodel": {"last_updated": "2021-02-14T14:15:17.832000+0000", "id": 246, "url": "/model/regnety-004"}, "method_short": "regnety_004", "method_details": null, "mlmodel_short": "regnety_004", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "74.02%", "Top 5 Accuracy": "91.76%"}, "raw_metrics": {"Top 1 Accuracy": 74.02, "Top 5 Accuracy": 91.76}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27645, "rank": 274, "method": "SwAV ResNet-50 (200 epochs, 2x224+6x96, 4096 bs)", "mlmodel": {"last_updated": "2021-02-27T09:28:58.962000+0000", "id": 911, "url": "/model/swav-resnet-50-200-epochs-2x224-6x96-4096-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "200 epochs, 2x224+6x96, 4096 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "200 epochs, 2x224+6x96, 4096 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.85%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 73.85, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25792, "rank": 275, "method": "regnetx_006", "mlmodel": {"last_updated": "2021-02-14T14:14:49.772000+0000", "id": 198, "url": "/model/regnetx-006"}, "method_short": "regnetx_006", "method_details": null, "mlmodel_short": "regnetx_006", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.84%", "Top 5 Accuracy": "91.68%"}, "raw_metrics": {"Top 1 Accuracy": 73.84, "Top 5 Accuracy": 91.68}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27667, "rank": 276, "method": "SimCLR ResNet-50-w2 (1000 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:49.087000+0000", "id": 920, "url": "/model/simclr-resnet-50-w2-1000-epochs"}, "method_short": "SimCLR ResNet-50-w2 ", "method_details": "1000 epochs", "mlmodel_short": "SimCLR ResNet-50-w2 ", "mlmodeldetails": "1000 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.84%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 73.84, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25646, "rank": 277, "method": "MNASNet 1.0", "mlmodel": {"last_updated": "2021-02-12T11:31:27.702000+0000", "id": 33, "url": "/model/mnasnet-1-0"}, "method_short": "MNASNet 1.0", "method_details": null, "mlmodel_short": "MNASNet 1.0", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.51%", "Top 5 Accuracy": "91.54%"}, "raw_metrics": {"Top 1 Accuracy": 73.51, "Top 5 Accuracy": 91.54}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25922, "rank": 278, "method": "tf_mobilenetv3_large_075", "mlmodel": {"last_updated": "2021-02-14T14:20:11.048000+0000", "id": 214, "url": "/model/tf-mobilenetv3-large-075"}, "method_short": "tf_mobilenetv3_large_075", "method_details": null, "mlmodel_short": "tf_mobilenetv3_large_075", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.45%", "Top 5 Accuracy": "91.34%"}, "raw_metrics": {"Top 1 Accuracy": 73.45, "Top 5 Accuracy": 91.34}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25676, "rank": 279, "method": "VGG-16 with batch normalization", "mlmodel": {"last_updated": "2021-02-12T11:31:57.599000+0000", "id": 170, "url": "/model/vgg-16-with-batch-normalization"}, "method_short": "VGG-16 with batch normalization", "method_details": null, "mlmodel_short": "VGG-16 with batch normalization", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.37%", "Top 5 Accuracy": "91.5%"}, "raw_metrics": {"Top 1 Accuracy": 73.37, "Top 5 Accuracy": 91.5}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25665, "rank": 280, "method": "ResNet-34", "mlmodel": {"last_updated": "2021-02-12T11:31:44.767000+0000", "id": 149, "url": "/model/resnet-34"}, "method_short": "ResNet-34", "method_details": null, "mlmodel_short": "ResNet-34", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.3%", "Top 5 Accuracy": "91.42%"}, "raw_metrics": {"Top 1 Accuracy": 73.3, "Top 5 Accuracy": 91.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25937, "rank": 281, "method": "tv_resnet34", "mlmodel": {"last_updated": "2021-02-14T14:20:39.342000+0000", "id": 220, "url": "/model/tv-resnet34"}, "method_short": "tv_resnet34", "method_details": null, "mlmodel_short": "tv_resnet34", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.3%", "Top 5 Accuracy": "91.42%"}, "raw_metrics": {"Top 1 Accuracy": 73.3, "Top 5 Accuracy": 91.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25873, "rank": 282, "method": "swsl_resnet18", "mlmodel": {"last_updated": "2021-02-14T14:18:14.261000+0000", "id": 363, "url": "/model/swsl-resnet18"}, "method_short": "swsl_resnet18", "method_details": null, "mlmodel_short": "swsl_resnet18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.28%", "Top 5 Accuracy": "91.76%"}, "raw_metrics": {"Top 1 Accuracy": 73.28, "Top 5 Accuracy": 91.76}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27681, "rank": 283, "method": "SwAV ResNet-50 (200 epochs, 2x224+6x96, 256 bs)", "mlmodel": {"last_updated": "2021-02-27T09:28:56.521000+0000", "id": 915, "url": "/model/swav-resnet-50-200-epochs-2x224-6x96-256-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "200 epochs, 2x224+6x96, 256 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "200 epochs, 2x224+6x96, 256 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.07%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 73.07, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25863, "rank": 284, "method": "skresnet18", "mlmodel": {"last_updated": "2021-02-14T14:17:46.917000+0000", "id": 311, "url": "/model/skresnet18"}, "method_short": "skresnet18", "method_details": null, "mlmodel_short": "skresnet18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "73.03%", "Top 5 Accuracy": "91.17%"}, "raw_metrics": {"Top 1 Accuracy": 73.03, "Top 5 Accuracy": 91.17}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25782, "rank": 285, "method": "mobilenetv2_100", "mlmodel": {"last_updated": "2021-02-14T14:14:23.672000+0000", "id": 330, "url": "/model/mobilenetv2-100"}, "method_short": "mobilenetv2_100", "method_details": null, "mlmodel_short": "mobilenetv2_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.95%", "Top 5 Accuracy": "91.0%"}, "raw_metrics": {"Top 1 Accuracy": 72.95, "Top 5 Accuracy": 91.0}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25867, "rank": 286, "method": "ssl_resnet18", "mlmodel": {"last_updated": "2021-02-14T14:17:56.767000+0000", "id": 357, "url": "/model/ssl-resnet18"}, "method_short": "ssl_resnet18", "method_details": null, "mlmodel_short": "ssl_resnet18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.62%", "Top 5 Accuracy": "91.42%"}, "raw_metrics": {"Top 1 Accuracy": 72.62, "Top 5 Accuracy": 91.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25791, "rank": 287, "method": "regnetx_004", "mlmodel": {"last_updated": "2021-02-14T14:14:47.669000+0000", "id": 194, "url": "/model/regnetx-004"}, "method_short": "regnetx_004", "method_details": null, "mlmodel_short": "regnetx_004", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.39%", "Top 5 Accuracy": "90.82%"}, "raw_metrics": {"Top 1 Accuracy": 72.39, "Top 5 Accuracy": 90.82}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25677, "rank": 288, "method": "VGG-19", "mlmodel": {"last_updated": "2021-02-12T11:31:58.807000+0000", "id": 161, "url": "/model/vgg-19"}, "method_short": "VGG-19", "method_details": null, "mlmodel_short": "VGG-19", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.38%", "Top 5 Accuracy": "90.88%"}, "raw_metrics": {"Top 1 Accuracy": 72.38, "Top 5 Accuracy": 90.88}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25753, "rank": 289, "method": "hrnet_w18_small", "mlmodel": {"last_updated": "2021-02-14T14:12:58.347000+0000", "id": 192, "url": "/model/hrnet-w18-small"}, "method_short": "hrnet_w18_small", "method_details": null, "mlmodel_short": "hrnet_w18_small", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.34%", "Top 5 Accuracy": "90.68%"}, "raw_metrics": {"Top 1 Accuracy": 72.34, "Top 5 Accuracy": 90.68}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25832, "rank": 290, "method": "resnet18d", "mlmodel": {"last_updated": "2021-02-14T14:16:36.227000+0000", "id": 105, "url": "/model/resnet18d"}, "method_short": "resnet18d", "method_details": null, "mlmodel_short": "resnet18d", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.27%", "Top 5 Accuracy": "90.69%"}, "raw_metrics": {"Top 1 Accuracy": 72.27, "Top 5 Accuracy": 90.69}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25924, "rank": 291, "method": "tf_mobilenetv3_large_minimal_100", "mlmodel": {"last_updated": "2021-02-14T14:20:15.243000+0000", "id": 251, "url": "/model/tf-mobilenetv3-large-minimal-100"}, "method_short": "tf_mobilenetv3_large_minimal_100", "method_details": null, "mlmodel_short": "tf_mobilenetv3_large_minimal_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "72.24%", "Top 5 Accuracy": "90.64%"}, "raw_metrics": {"Top 1 Accuracy": 72.24, "Top 5 Accuracy": 90.64}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27647, "rank": 292, "method": "SwAV ResNet-50 (100 epochs, 2x224+6x96, 4096 bs)", "mlmodel": {"last_updated": "2021-02-27T09:28:53.740000+0000", "id": 895, "url": "/model/swav-resnet-50-100-epochs-2x224-6x96-4096-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "100 epochs, 2x224+6x96, 4096 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "100 epochs, 2x224+6x96, 4096 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.99%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 71.99, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25652, "rank": 293, "method": "MobileNet V2", "mlmodel": {"last_updated": "2021-02-12T11:31:30.326000+0000", "id": 34, "url": "/model/mobilenet-v2"}, "method_short": "MobileNet V2", "method_details": null, "mlmodel_short": "MobileNet V2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.88%", "Top 5 Accuracy": "90.29%"}, "raw_metrics": {"Top 1 Accuracy": 71.88, "Top 5 Accuracy": 90.29}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25771, "rank": 294, "method": "legacy_seresnet18", "mlmodel": {"last_updated": "2021-02-14T14:13:56.542000+0000", "id": 278, "url": "/model/legacy-seresnet18"}, "method_short": "legacy_seresnet18", "method_details": null, "mlmodel_short": "legacy_seresnet18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.74%", "Top 5 Accuracy": "90.34%"}, "raw_metrics": {"Top 1 Accuracy": 71.74, "Top 5 Accuracy": 90.34}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27696, "rank": 295, "method": "SimCLR ResNet-50-w4 (1000 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:51.182000+0000", "id": 913, "url": "/model/simclr-resnet-50-w4-1000-epochs"}, "method_short": "SimCLR ResNet-50-w4 ", "method_details": "1000 epochs", "mlmodel_short": "SimCLR ResNet-50-w4 ", "mlmodeldetails": "1000 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.61%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 71.61, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25675, "rank": 296, "method": "VGG-16", "mlmodel": {"last_updated": "2021-02-12T11:31:56.364000+0000", "id": 168, "url": "/model/vgg-16"}, "method_short": "VGG-16", "method_details": null, "mlmodel_short": "VGG-16", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.59%", "Top 5 Accuracy": "90.38%"}, "raw_metrics": {"Top 1 Accuracy": 71.59, "Top 5 Accuracy": 90.38}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27680, "rank": 297, "method": "SimCLR ResNet-101 (1000 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:31.618000+0000", "id": 872, "url": "/model/simclr-resnet-101-1000-epochs"}, "method_short": "SimCLR ResNet-101 ", "method_details": "1000 epochs", "mlmodel_short": "SimCLR ResNet-101 ", "mlmodeldetails": "1000 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.56%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 71.56, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25674, "rank": 298, "method": "VGG-13 with batch normalization", "mlmodel": {"last_updated": "2021-02-12T11:31:55.232000+0000", "id": 167, "url": "/model/vgg-13-with-batch-normalization"}, "method_short": "VGG-13 with batch normalization", "method_details": null, "mlmodel_short": "VGG-13 with batch normalization", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "71.55%", "Top 5 Accuracy": "90.37%"}, "raw_metrics": {"Top 1 Accuracy": 71.55, "Top 5 Accuracy": 90.37}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27687, "rank": 299, "method": "PIRL ResNet-50-w2 (400 epochs, MLP head)", "mlmodel": {"last_updated": "2021-02-27T09:28:09.879000+0000", "id": 864, "url": "/model/pirl-resnet-50-w2-400-epochs-mlp-head"}, "method_short": "PIRL ResNet-50-w2 ", "method_details": "400 epochs, MLP head", "mlmodel_short": "PIRL ResNet-50-w2 ", "mlmodeldetails": "400 epochs, MLP head", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "70.9%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 70.9, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25738, "rank": 300, "method": "gluon_resnet18_v1b", "mlmodel": {"last_updated": "2021-02-14T14:12:21.076000+0000", "id": 229, "url": "/model/gluon-resnet18-v1b"}, "method_short": "gluon_resnet18_v1b", "method_details": null, "mlmodel_short": "gluon_resnet18_v1b", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "70.84%", "Top 5 Accuracy": "89.76%"}, "raw_metrics": {"Top 1 Accuracy": 70.84, "Top 5 Accuracy": 89.76}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25672, "rank": 301, "method": "VGG-11 with batch normalization", "mlmodel": {"last_updated": "2021-02-12T11:31:53.231000+0000", "id": 158, "url": "/model/vgg-11-with-batch-normalization"}, "method_short": "VGG-11 with batch normalization", "method_details": null, "mlmodel_short": "VGG-11 with batch normalization", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "70.38%", "Top 5 Accuracy": "89.81%"}, "raw_metrics": {"Top 1 Accuracy": 70.38, "Top 5 Accuracy": 89.81}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25802, "rank": 302, "method": "regnety_002", "mlmodel": {"last_updated": "2021-02-14T14:15:15.397000+0000", "id": 241, "url": "/model/regnety-002"}, "method_short": "regnety_002", "method_details": null, "mlmodel_short": "regnety_002", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "70.28%", "Top 5 Accuracy": "89.55%"}, "raw_metrics": {"Top 1 Accuracy": 70.28, "Top 5 Accuracy": 89.55}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27693, "rank": 303, "method": "DeepClusterV2 ResNet-50 (400 epochs, 2x224)", "mlmodel": {"last_updated": "2021-02-27T09:27:10.135000+0000", "id": 891, "url": "/model/deepclusterv2-resnet-50-400-epochs-2x224"}, "method_short": "DeepClusterV2 ResNet-50 ", "method_details": "400 epochs, 2x224", "mlmodel_short": "DeepClusterV2 ResNet-50 ", "mlmodeldetails": "400 epochs, 2x224", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "70.01%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 70.01, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25673, "rank": 304, "method": "VGG-13", "mlmodel": {"last_updated": "2021-02-12T11:31:54.226000+0000", "id": 163, "url": "/model/vgg-13"}, "method_short": "VGG-13", "method_details": null, "mlmodel_short": "VGG-13", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.93%", "Top 5 Accuracy": "89.25%"}, "raw_metrics": {"Top 1 Accuracy": 69.93, "Top 5 Accuracy": 89.25}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27700, "rank": 305, "method": "PIRL ResNet-50 (800 epochs, MLP head)", "mlmodel": {"last_updated": "2021-02-27T09:28:04.802000+0000", "id": 908, "url": "/model/pirl-resnet-50-800-epochs-mlp-head"}, "method_short": "PIRL ResNet-50 ", "method_details": "800 epochs, MLP head", "mlmodel_short": "PIRL ResNet-50 ", "mlmodeldetails": "800 epochs, MLP head", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.9%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 69.9, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27652, "rank": 306, "method": "SimCLR ResNet-50-w2 (100 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:47.097000+0000", "id": 887, "url": "/model/simclr-resnet-50-w2-100-epochs"}, "method_short": "SimCLR ResNet-50-w2 ", "method_details": "100 epochs", "mlmodel_short": "SimCLR ResNet-50-w2 ", "mlmodeldetails": "100 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.82%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 69.82, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25642, "rank": 307, "method": "GoogleNet", "mlmodel": {"last_updated": "2021-02-12T11:31:23.037000+0000", "id": 39, "url": "/model/googlenet"}, "method_short": "GoogleNet", "method_details": null, "mlmodel_short": "GoogleNet", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.78%", "Top 5 Accuracy": "89.53%"}, "raw_metrics": {"Top 1 Accuracy": 69.78, "Top 5 Accuracy": 89.53}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25664, "rank": 308, "method": "ResNet-18", "mlmodel": {"last_updated": "2021-02-12T11:31:43.481000+0000", "id": 130, "url": "/model/resnet-18"}, "method_short": "ResNet-18", "method_details": null, "mlmodel_short": "ResNet-18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.76%", "Top 5 Accuracy": "89.08%"}, "raw_metrics": {"Top 1 Accuracy": 69.76, "Top 5 Accuracy": 89.08}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25831, "rank": 309, "method": "resnet18", "mlmodel": {"last_updated": "2021-02-14T14:16:34.204000+0000", "id": 132, "url": "/model/resnet18"}, "method_short": "resnet18", "method_details": null, "mlmodel_short": "resnet18", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.74%", "Top 5 Accuracy": "89.09%"}, "raw_metrics": {"Top 1 Accuracy": 69.74, "Top 5 Accuracy": 89.09}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27673, "rank": 310, "method": "SimCLR ResNet-50 (800 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:44.658000+0000", "id": 883, "url": "/model/simclr-resnet-50-800-epochs"}, "method_short": "SimCLR ResNet-50 ", "method_details": "800 epochs", "mlmodel_short": "SimCLR ResNet-50 ", "mlmodeldetails": "800 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.68%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 69.68, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27701, "rank": 311, "method": "SwAV ResNet-50 (400 epochs, 2x224, 4096 bs)", "mlmodel": {"last_updated": "2021-02-27T09:29:07.000000+0000", "id": 912, "url": "/model/swav-resnet-50-400-epochs-2x224-4096-bs"}, "method_short": "SwAV ResNet-50 ", "method_details": "400 epochs, 2x224, 4096 bs", "mlmodel_short": "SwAV ResNet-50 ", "mlmodeldetails": "400 epochs, 2x224, 4096 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.53%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 69.53, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25668, "rank": 312, "method": "ShuffleNet V2", "mlmodel": {"last_updated": "2021-02-12T11:31:48.881000+0000", "id": 36, "url": "/model/shufflenet-v2"}, "method_short": "ShuffleNet V2", "method_details": null, "mlmodel_short": "ShuffleNet V2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.36%", "Top 5 Accuracy": "88.32%"}, "raw_metrics": {"Top 1 Accuracy": 69.36, "Top 5 Accuracy": 88.32}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27668, "rank": 313, "method": "PIRL ResNet-50-w2 (400 epochs, linear head)", "mlmodel": {"last_updated": "2021-02-27T09:28:12.170000+0000", "id": 863, "url": "/model/pirl-resnet-50-w2-400-epochs-linear-head"}, "method_short": "PIRL ResNet-50-w2 ", "method_details": "400 epochs, linear head", "mlmodel_short": "PIRL ResNet-50-w2 ", "mlmodeldetails": "400 epochs, linear head", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.3%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 69.3, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25671, "rank": 314, "method": "VGG-11", "mlmodel": {"last_updated": "2021-02-12T11:31:52.304000+0000", "id": 129, "url": "/model/vgg-11"}, "method_short": "VGG-11", "method_details": null, "mlmodel_short": "VGG-11", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "69.02%", "Top 5 Accuracy": "88.63%"}, "raw_metrics": {"Top 1 Accuracy": 69.02, "Top 5 Accuracy": 88.63}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27656, "rank": 315, "method": "SimCLR ResNet-50 (1000 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:36.616000+0000", "id": 919, "url": "/model/simclr-resnet-50-1000-epochs"}, "method_short": "SimCLR ResNet-50 ", "method_details": "1000 epochs", "mlmodel_short": "SimCLR ResNet-50 ", "mlmodeldetails": "1000 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "68.8%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 68.8, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25790, "rank": 316, "method": "regnetx_002", "mlmodel": {"last_updated": "2021-02-14T14:14:45.617000+0000", "id": 114, "url": "/model/regnetx-002"}, "method_short": "regnetx_002", "method_details": null, "mlmodel_short": "regnetx_002", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "68.75%", "Top 5 Accuracy": "88.56%"}, "raw_metrics": {"Top 1 Accuracy": 68.75, "Top 5 Accuracy": 88.56}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25926, "rank": 317, "method": "tf_mobilenetv3_small_100", "mlmodel": {"last_updated": "2021-02-14T14:20:19.285000+0000", "id": 277, "url": "/model/tf-mobilenetv3-small-100"}, "method_short": "tf_mobilenetv3_small_100", "method_details": null, "mlmodel_short": "tf_mobilenetv3_small_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "67.92%", "Top 5 Accuracy": "87.68%"}, "raw_metrics": {"Top 1 Accuracy": 67.92, "Top 5 Accuracy": 87.68}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25701, "rank": 318, "method": "dla60x_c", "mlmodel": {"last_updated": "2021-02-14T14:10:51.072000+0000", "id": 398, "url": "/model/dla60x-c"}, "method_short": "dla60x_c", "method_details": null, "mlmodel_short": "dla60x_c", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "67.91%", "Top 5 Accuracy": "88.42%"}, "raw_metrics": {"Top 1 Accuracy": 67.91, "Top 5 Accuracy": 88.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27670, "rank": 319, "method": "SimCLR ResNet-50 (400 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:42.136000+0000", "id": 882, "url": "/model/simclr-resnet-50-400-epochs"}, "method_short": "SimCLR ResNet-50 ", "method_details": "400 epochs", "mlmodel_short": "SimCLR ResNet-50 ", "mlmodeldetails": "400 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "67.71%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 67.71, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25655, "rank": 320, "method": "MobileNet V3 Small", "mlmodel": {"last_updated": "2021-02-12T11:31:32.900000+0000", "id": 152, "url": "/model/mobilenet-v3-small"}, "method_short": "MobileNet V3 Small", "method_details": null, "mlmodel_short": "MobileNet V3 Small", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "67.668%", "Top 5 Accuracy": "87.402%"}, "raw_metrics": {"Top 1 Accuracy": 67.668, "Top 5 Accuracy": 87.402}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27661, "rank": 321, "method": "SimCLR ResNet-50 (200 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:39.419000+0000", "id": 879, "url": "/model/simclr-resnet-50-200-epochs"}, "method_short": "SimCLR ResNet-50 ", "method_details": "200 epochs", "mlmodel_short": "SimCLR ResNet-50 ", "mlmodeldetails": "200 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "66.61%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 66.61, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27686, "rank": 322, "method": "MoCo-v2 ResNet-50 (200 epochs, 256 bs)", "mlmodel": {"last_updated": "2021-02-27T09:27:45.855000+0000", "id": 881, "url": "/model/moco-v2-resnet-50-200-epochs-256-bs"}, "method_short": "MoCo-v2 ResNet-50 ", "method_details": "200 epochs, 256 bs", "mlmodel_short": "MoCo-v2 ResNet-50 ", "mlmodeldetails": "200 epochs, 256 bs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "66.4%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 66.4, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25696, "rank": 323, "method": "dla46x_c", "mlmodel": {"last_updated": "2021-02-14T14:10:42.262000+0000", "id": 392, "url": "/model/dla46x-c"}, "method_short": "dla46x_c", "method_details": null, "mlmodel_short": "dla46x_c", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "65.98%", "Top 5 Accuracy": "86.99%"}, "raw_metrics": {"Top 1 Accuracy": 65.98, "Top 5 Accuracy": 86.99}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27664, "rank": 324, "method": "PIRL ResNet-50 (200 epochs, MLP head)", "mlmodel": {"last_updated": "2021-02-27T09:27:59.561000+0000", "id": 907, "url": "/model/pirl-resnet-50-200-epochs-mlp-head"}, "method_short": "PIRL ResNet-50 ", "method_details": "200 epochs, MLP head", "mlmodel_short": "PIRL ResNet-50 ", "mlmodeldetails": "200 epochs, MLP head", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "65.8%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 65.8, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25925, "rank": 325, "method": "tf_mobilenetv3_small_075", "mlmodel": {"last_updated": "2021-02-14T14:20:17.267000+0000", "id": 261, "url": "/model/tf-mobilenetv3-small-075"}, "method_short": "tf_mobilenetv3_small_075", "method_details": null, "mlmodel_short": "tf_mobilenetv3_small_075", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "65.72%", "Top 5 Accuracy": "86.13%"}, "raw_metrics": {"Top 1 Accuracy": 65.72, "Top 5 Accuracy": 86.13}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25695, "rank": 326, "method": "dla46_c", "mlmodel": {"last_updated": "2021-02-14T14:10:40.888000+0000", "id": 94, "url": "/model/dla46-c"}, "method_short": "dla46_c", "method_details": null, "mlmodel_short": "dla46_c", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "64.87%", "Top 5 Accuracy": "86.29%"}, "raw_metrics": {"Top 1 Accuracy": 64.87, "Top 5 Accuracy": 86.29}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27678, "rank": 327, "method": "SimCLR ResNet-50 (100 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:33.929000+0000", "id": 875, "url": "/model/simclr-resnet-50-100-epochs"}, "method_short": "SimCLR ResNet-50 ", "method_details": "100 epochs", "mlmodel_short": "SimCLR ResNet-50 ", "mlmodeldetails": "100 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "64.4%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 64.4, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27697, "rank": 328, "method": "PIRL ResNet-50 (800 epochs, linear head)", "mlmodel": {"last_updated": "2021-02-27T09:28:07.470000+0000", "id": 909, "url": "/model/pirl-resnet-50-800-epochs-linear-head"}, "method_short": "PIRL ResNet-50 ", "method_details": "800 epochs, linear head", "mlmodel_short": "PIRL ResNet-50 ", "mlmodeldetails": "800 epochs, linear head", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "64.29%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 64.29, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25927, "rank": 329, "method": "tf_mobilenetv3_small_minimal_100", "mlmodel": {"last_updated": "2021-02-14T14:20:21.352000+0000", "id": 284, "url": "/model/tf-mobilenetv3-small-minimal-100"}, "method_short": "tf_mobilenetv3_small_minimal_100", "method_details": null, "mlmodel_short": "tf_mobilenetv3_small_minimal_100", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "62.91%", "Top 5 Accuracy": "84.24%"}, "raw_metrics": {"Top 1 Accuracy": 62.91, "Top 5 Accuracy": 84.24}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27657, "rank": 330, "method": "PIRL ResNet-50 (200 epochs, linear head)", "mlmodel": {"last_updated": "2021-02-27T09:28:02.207000+0000", "id": 904, "url": "/model/pirl-resnet-50-200-epochs-linear-head"}, "method_short": "PIRL ResNet-50 ", "method_details": "200 epochs, linear head", "mlmodel_short": "PIRL ResNet-50 ", "mlmodeldetails": "200 epochs, linear head", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "62.9%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 62.9, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27688, "rank": 331, "method": "SimCLR ResNet-101 (100 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:28:28.707000+0000", "id": 917, "url": "/model/simclr-resnet-101-100-epochs"}, "method_short": "SimCLR ResNet-101 ", "method_details": "100 epochs", "mlmodel_short": "SimCLR ResNet-101 ", "mlmodeldetails": "100 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "62.76%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 62.76, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27675, "rank": 332, "method": "NPID++ ResNet-50-w2 (32k negatives, 800 epochs, cosine LR)", "mlmodel": {"last_updated": "2021-02-27T09:27:57.134000+0000", "id": 905, "url": "/model/npid-resnet-50-w2-32k-negatives-800-epochs"}, "method_short": "NPID++ ResNet-50-w2 ", "method_details": "32k negatives, 800 epochs, cosine LR", "mlmodel_short": "NPID++ ResNet-50-w2 ", "mlmodeldetails": "32k negatives, 800 epochs, cosine LR", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "62.73%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 62.73, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27698, "rank": 333, "method": "ResNet-50 (supervised, caffe2, places205)", "mlmodel": {"last_updated": null, "id": 884, "url": "/model/resnet-50-supervised-caffe2-places205"}, "method_short": "ResNet-50 ", "method_details": "supervised, caffe2, places205", "mlmodel_short": "ResNet-50 ", "mlmodeldetails": "supervised, caffe2, places205", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "58.49%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 58.49, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25670, "rank": 334, "method": "SqueezeNet 1.1", "mlmodel": {"last_updated": "2021-02-12T11:31:51.116000+0000", "id": 121, "url": "/model/squeezenet-1-1"}, "method_short": "SqueezeNet 1.1", "method_details": null, "mlmodel_short": "SqueezeNet 1.1", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "58.19%", "Top 5 Accuracy": "80.62%"}, "raw_metrics": {"Top 1 Accuracy": 58.19, "Top 5 Accuracy": 80.62}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25669, "rank": 335, "method": "SqueezeNet 1.0", "mlmodel": {"last_updated": "2021-02-12T11:31:50.174000+0000", "id": 131, "url": "/model/squeezenet-1-0"}, "method_short": "SqueezeNet 1.0", "method_details": null, "mlmodel_short": "SqueezeNet 1.0", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "58.1%", "Top 5 Accuracy": "80.42%"}, "raw_metrics": {"Top 1 Accuracy": 58.1, "Top 5 Accuracy": 80.42}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27692, "rank": 336, "method": "NPID++ ResNet-50 (32k negatives, 800 epochs, cosine LR)", "mlmodel": {"last_updated": "2021-02-27T09:27:54.698000+0000", "id": 898, "url": "/model/npid-resnet-50-32k-negatives-800-epochs"}, "method_short": "NPID++ ResNet-50 ", "method_details": "32k negatives, 800 epochs, cosine LR", "mlmodel_short": "NPID++ ResNet-50 ", "mlmodeldetails": "32k negatives, 800 epochs, cosine LR", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "56.68%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 56.68, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 25662, "rank": 337, "method": "AlexNet", "mlmodel": {"last_updated": "2021-02-12T11:31:03.794000+0000", "id": 35, "url": "/model/alexnet"}, "method_short": "AlexNet", "method_details": null, "mlmodel_short": "AlexNet", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "56.55%", "Top 5 Accuracy": "79.09%"}, "raw_metrics": {"Top 1 Accuracy": 56.55, "Top 5 Accuracy": 79.09}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27658, "rank": 338, "method": "NPID ResNet-50 (ImageNet-1K, official)", "mlmodel": {"last_updated": "2021-02-27T09:27:51.866000+0000", "id": 892, "url": "/model/npid-resnet-50-imagenet-1k-official"}, "method_short": "NPID ResNet-50 ", "method_details": "ImageNet-1K, official", "mlmodel_short": "NPID ResNet-50 ", "mlmodeldetails": "ImageNet-1K, official", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "54.99%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 54.99, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27648, "rank": 339, "method": "RotNet ResNet-50 (Goyal19, ImageNet-22K)", "mlmodel": {"last_updated": "2021-02-27T09:28:25.644000+0000", "id": 888, "url": "/model/rotnet-resnet-50-goyal19-imagenet-22k"}, "method_short": "RotNet ResNet-50 ", "method_details": "Goyal19, ImageNet-22K", "mlmodel_short": "RotNet ResNet-50 ", "mlmodeldetails": "Goyal19, ImageNet-22K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "54.89%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 54.89, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27646, "rank": 340, "method": "ClusterFit ResNet-50 (ImageNet-1K, 16K RotNet clusters)", "mlmodel": {"last_updated": "2021-02-27T09:26:52.536000+0000", "id": 871, "url": "/model/clusterfit-resnet-50-imagenet-1k-16k-rotnet"}, "method_short": "ClusterFit ResNet-50 ", "method_details": "ImageNet-1K, 16K RotNet clusters", "mlmodel_short": "ClusterFit ResNet-50 ", "mlmodeldetails": "ImageNet-1K, 16K RotNet clusters", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "53.63%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 53.63, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27691, "rank": 341, "method": "Jigsaw ResNet-50 (Goyal19, ImageNet-22K)", "mlmodel": {"last_updated": "2021-02-27T09:27:26.882000+0000", "id": 890, "url": "/model/jigsaw-resnet-50-goyal19-imagenet-22k"}, "method_short": "Jigsaw ResNet-50 ", "method_details": "Goyal19, ImageNet-22K", "mlmodel_short": "Jigsaw ResNet-50 ", "mlmodeldetails": "Goyal19, ImageNet-22K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "53.09%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 53.09, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27677, "rank": 342, "method": "NPID ResNet-50 (4k negatives, 200 epochs)", "mlmodel": {"last_updated": "2021-02-27T09:27:49.274000+0000", "id": 886, "url": "/model/npid-resnet-50-4k-negatives-200-epochs"}, "method_short": "NPID ResNet-50 ", "method_details": "4k negatives, 200 epochs", "mlmodel_short": "NPID ResNet-50 ", "mlmodeldetails": "4k negatives, 200 epochs", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "52.73%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 52.73, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27653, "rank": 343, "method": "Jigsaw ResNet-50 (Goyal19, YFCC100M)", "mlmodel": {"last_updated": "2021-02-27T09:27:30.223000+0000", "id": 867, "url": "/model/jigsaw-resnet-50-goyal19-yfcc100m"}, "method_short": "Jigsaw ResNet-50 ", "method_details": "Goyal19, YFCC100M", "mlmodel_short": "Jigsaw ResNet-50 ", "mlmodeldetails": "Goyal19, YFCC100M", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "51.37%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 51.37, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27672, "rank": 344, "method": "Colorization ResNet-50 (Goyal19, ImageNet-22K)", "mlmodel": {"last_updated": "2021-02-27T09:26:58.444000+0000", "id": 916, "url": "/model/colorization-resnet-50-goyal19-imagenet-22k"}, "method_short": "Colorization ResNet-50 ", "method_details": "Goyal19, ImageNet-22K", "mlmodel_short": "Colorization ResNet-50 ", "mlmodeldetails": "Goyal19, ImageNet-22K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "49.24%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 49.24, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27683, "rank": 345, "method": "Jigsaw ResNet-50 - 100 permutations", "mlmodel": {"last_updated": "2021-02-27T09:27:39.083000+0000", "id": 901, "url": "/model/jigsaw-resnet-50-100-permutations"}, "method_short": "Jigsaw ResNet-50 - 100 permutations", "method_details": null, "mlmodel_short": "Jigsaw ResNet-50 - 100 permutations", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "48.57%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 48.57, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27655, "rank": 346, "method": "RotNet ResNet-50 (Goyal19, ImageNet-1K)", "mlmodel": {"last_updated": "2021-02-27T09:28:23.007000+0000", "id": 870, "url": "/model/rotnet-resnet-50-goyal19-imagenet-1k"}, "method_short": "RotNet ResNet-50 ", "method_details": "Goyal19, ImageNet-1K", "mlmodel_short": "RotNet ResNet-50 ", "mlmodeldetails": "Goyal19, ImageNet-1K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "48.2%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 48.2, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27644, "rank": 347, "method": "Jigsaw ResNet-50 - 10K permutations", "mlmodel": {"last_updated": "2021-02-27T09:27:41.919000+0000", "id": 903, "url": "/model/jigsaw-resnet-50-10k-permutations"}, "method_short": "Jigsaw ResNet-50 - 10K permutations", "method_details": null, "mlmodel_short": "Jigsaw ResNet-50 - 10K permutations", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "48.11%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 48.11, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27666, "rank": 348, "method": "Colorization ResNet-50 (Goyal19, YFCC100M)", "mlmodel": {"last_updated": "2021-02-27T09:27:01.298000+0000", "id": 918, "url": "/model/colorization-resnet-50-goyal19-yfcc100m"}, "method_short": "Colorization ResNet-50 ", "method_details": "Goyal19, YFCC100M", "mlmodel_short": "Colorization ResNet-50 ", "mlmodeldetails": "Goyal19, YFCC100M", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "47.46%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 47.46, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27676, "rank": 349, "method": "Jigsaw ResNet-50 (ImageNet-1K, 2K permutations)", "mlmodel": {"last_updated": "2021-02-27T09:27:33.194000+0000", "id": 900, "url": "/model/jigsaw-resnet-50-imagenet-1k-2k-permutations"}, "method_short": "Jigsaw ResNet-50 ", "method_details": "ImageNet-1K, 2K permutations", "mlmodel_short": "Jigsaw ResNet-50 ", "mlmodeldetails": "ImageNet-1K, 2K permutations", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "46.73%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 46.73, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27685, "rank": 350, "method": "Jigsaw ResNet-50 (Goyal19, ImageNet-1K)", "mlmodel": {"last_updated": "2021-02-27T09:27:23.567000+0000", "id": 866, "url": "/model/jigsaw-resnet-50-goyal19-imagenet-1k"}, "method_short": "Jigsaw ResNet-50 ", "method_details": "Goyal19, ImageNet-1K", "mlmodel_short": "Jigsaw ResNet-50 ", "mlmodeldetails": "Goyal19, ImageNet-1K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "46.58%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 46.58, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27682, "rank": 351, "method": "Jigsaw ResNet-50 (ImageNet-22K, 2K permutations)", "mlmodel": {"last_updated": "2021-02-27T09:27:36.191000+0000", "id": 897, "url": "/model/jigsaw-resnet-50-imagenet-22k-2k-permutations"}, "method_short": "Jigsaw ResNet-50 ", "method_details": "ImageNet-22K, 2K permutations", "mlmodel_short": "Jigsaw ResNet-50 ", "mlmodeldetails": "ImageNet-22K, 2K permutations", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "44.84%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 44.84, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27663, "rank": 352, "method": "Colorization ResNet-50 (Goyal19, ImageNet-1K)", "mlmodel": {"last_updated": "2021-02-27T09:26:55.793000+0000", "id": 910, "url": "/model/colorization-resnet-50-goyal19-imagenet-1k"}, "method_short": "Colorization ResNet-50 ", "method_details": "Goyal19, ImageNet-1K", "mlmodel_short": "Colorization ResNet-50 ", "mlmodeldetails": "Goyal19, ImageNet-1K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "40.11%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 40.11, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [{"id": 3, "name": "ResNet", "color": "#2771D3"}], "reports": []}, {"table_id": 116, "row_id": 27694, "rank": 353, "method": "RotNet AlexNet (ImageNet-1K)", "mlmodel": {"last_updated": "2021-02-27T09:28:20.591000+0000", "id": 869, "url": "/model/rotnet-alexnet-imagenet-1k"}, "method_short": "RotNet AlexNet ", "method_details": "ImageNet-1K", "mlmodel_short": "RotNet AlexNet ", "mlmodeldetails": "ImageNet-1K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "39.51%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 39.51, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27651, "rank": 354, "method": "DeepCluster AlexNet (ImageNet-1K)", "mlmodel": {"last_updated": "2021-02-27T09:27:04.421000+0000", "id": 876, "url": "/model/deepcluster-alexnet-imagenet-1k"}, "method_short": "DeepCluster AlexNet ", "method_details": "ImageNet-1K", "mlmodel_short": "DeepCluster AlexNet ", "mlmodeldetails": "ImageNet-1K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "37.88%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 37.88, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27654, "rank": 355, "method": "Jigsaw AlexNet (Goyal19, ImageNet-22K)", "mlmodel": {"last_updated": "2021-02-27T09:27:18.469000+0000", "id": 893, "url": "/model/jigsaw-alexnet-goyal19-imagenet-22k"}, "method_short": "Jigsaw AlexNet ", "method_details": "Goyal19, ImageNet-22K", "mlmodel_short": "Jigsaw AlexNet ", "mlmodeldetails": "Goyal19, ImageNet-22K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "37.5%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 37.5, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27671, "rank": 356, "method": "Jigsaw AlexNet (Goyal19, YFCC100M)", "mlmodel": {"last_updated": "2021-02-27T09:27:20.856000+0000", "id": 889, "url": "/model/jigsaw-alexnet-goyal19-yfcc100m"}, "method_short": "Jigsaw AlexNet ", "method_details": "Goyal19, YFCC100M", "mlmodel_short": "Jigsaw AlexNet ", "mlmodeldetails": "Goyal19, YFCC100M", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "37.01%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 37.01, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 27690, "rank": 357, "method": "Jigsaw AlexNet (Goyal19, ImageNet-1K)", "mlmodel": {"last_updated": "2021-02-27T09:27:16.120000+0000", "id": 894, "url": "/model/jigsaw-alexnet-goyal19-imagenet-1k"}, "method_short": "Jigsaw AlexNet ", "method_details": "Goyal19, ImageNet-1K", "mlmodel_short": "Jigsaw AlexNet ", "mlmodeldetails": "Goyal19, ImageNet-1K", "evaluation_date": null, "metrics": {"Top 1 Accuracy": "34.82%", "Top 5 Accuracy": null}, "raw_metrics": {"Top 1 Accuracy": 34.82, "Top 5 Accuracy": null}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25767, "rank": 358, "method": "inception_v4", "mlmodel": {"last_updated": "2021-02-14T14:13:46.532000+0000", "id": 259, "url": "/model/inception-v4-1"}, "method_short": "inception_v4", "method_details": null, "mlmodel_short": "inception_v4", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "1.01%", "Top 5 Accuracy": "16.85%"}, "raw_metrics": {"Top 1 Accuracy": 1.01, "Top 5 Accuracy": 16.85}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25725, "rank": 359, "method": "ens_adv_inception_resnet_v2", "mlmodel": {"last_updated": "2021-02-14T14:11:49.928000+0000", "id": 111, "url": "/model/ens-adv-inception-resnet-v2"}, "method_short": "ens_adv_inception_resnet_v2", "method_details": null, "mlmodel_short": "ens_adv_inception_resnet_v2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "1.0%", "Top 5 Accuracy": "17.32%"}, "raw_metrics": {"Top 1 Accuracy": 1.0, "Top 5 Accuracy": 17.32}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25789, "rank": 360, "method": "pnasnet5large", "mlmodel": {"last_updated": "2021-02-14T14:14:41.820000+0000", "id": 113, "url": "/model/pnasnet5large"}, "method_short": "pnasnet5large", "method_details": null, "mlmodel_short": "pnasnet5large", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "0.98%", "Top 5 Accuracy": "18.58%"}, "raw_metrics": {"Top 1 Accuracy": 0.98, "Top 5 Accuracy": 18.58}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}, {"table_id": 116, "row_id": 25765, "rank": 361, "method": "inception_resnet_v2", "mlmodel": {"last_updated": "2021-02-14T14:13:41.341000+0000", "id": 252, "url": "/model/inception-resnet-v2-1"}, "method_short": "inception_resnet_v2", "method_details": null, "mlmodel_short": "inception_resnet_v2", "mlmodeldetails": null, "evaluation_date": null, "metrics": {"Top 1 Accuracy": "0.95%", "Top 5 Accuracy": "17.29%"}, "raw_metrics": {"Top 1 Accuracy": 0.95, "Top 5 Accuracy": 17.29}, "uses_additional_data": false, "paper": {"id": null, "title": null, "url": null, "published": null, "code": false, "review_url": null}, "external_source_url": null, "tags": [], "reports": []}]</script>
    <script id="dataset-details" type="application/json">[{"name": "ImageNet", "fullName": "", "url": "/dataset/imagenet", "description": "The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.\nThe publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.\nILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., \u201cthere are cars in this image\u201d but \u201cthere are no tigers,\u201d and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., \u201cthere is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels\u201d.\nThe ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.", "imagePath": "https://production-media.paperswithcode.com/datasets/ImageNet-0000000008-f2e87edd_Y0fT5zg.jpg", "iconName": "images", "color": "#A395C6"}]</script>
    <script id="sota-page-details" type="application/json">{"task_main_area_name": "Computer Vision", "task_name": "Image Classification", "dataset_name": "ImageNet", "description": "", "mirror_url": null, "has_competition_entries": false}</script>

    <script type="application/javascript">
            let evaluationChartData = JSON.parse(
                document.getElementById("evaluation-chart-data").textContent
            );
            let evaluationTableMetrics = JSON.parse(
                document.getElementById("evaluation-table-metrics").textContent
            );
            let evaluationTableData = JSON.parse(
                document.getElementById("evaluation-table-data").textContent
            );
            let communityChartData = JSON.parse(
                document.getElementById("community-chart-data").textContent
            );
            let communityTableMetrics = JSON.parse(
                document.getElementById("community-table-metrics").textContent
            );
            let communityTableData = JSON.parse(
                document.getElementById("community-table-data").textContent
            );
            let datasetDetails = JSON.parse(
                document.getElementById("dataset-details").textContent
            );
            let sotaPageDetails = JSON.parse(
                document.getElementById("sota-page-details").textContent
            );
            // Containers
            let sotaPageContainer = document.getElementById("sota-page");

            // Breadcrumbs
            let breadcrumbs = [
                {
                    title: "Browse",
                    url: "/sota"
                },
                
                {
                    title: sotaPageDetails.task_main_area_name,
                    url: "/area/computer-vision"
                },
                
                {
                    title: sotaPageDetails.task_name,
                    url: "/task/image-classification"
                },
                
                {
                    title: sotaPageDetails.dataset_name + " dataset",
                    url: "/dataset/imagenet"
                }
                
            ];

            let highlight = (
                
                    null
                
            );

            function datasetsSearchUrl(query) {
                return "/datasets?q="+encodeURIComponent(query);
            }

            function newDatasetUrl(datasetName) {
                return "/contribute/dataset/new?name="+encodeURIComponent(datasetName);
            }

            const SOTA_AUTOCOMPLETE_PAPER_URL = "/sota/autocomplete/paper";
            const VIEW_PAPER_URL = "/paper/PAPER_SLUG";
    </script>
    <!-- End SOTA Table Generation -->



</div>





<div class="footer">
  <div class="footer-contact">
    <span class="footer-contact-item">Contact us on:</span>

    <a class="footer-contact-item" href="mailto:hello@paperswithcode.com">
    <span class=" icon-wrapper icon-ion" data-name="mail"><svg xmlns="http://www.w3.org/2000/svg" width="512" height="512" viewBox="0 0 512 512"><path d="M424 80H88a56.06 56.06 0 0 0-56 56v240a56.06 56.06 0 0 0 56 56h336a56.06 56.06 0 0 0 56-56V136a56.06 56.06 0 0 0-56-56zm-14.18 92.63l-144 112a16 16 0 0 1-19.64 0l-144-112a16 16 0 1 1 19.64-25.26L256 251.73l134.18-104.36a16 16 0 0 1 19.64 25.26z"/></svg></span> hello@paperswithcode.com
    </a>.
    <span class="footer-contact-item">
        Papers With Code is a free resource with all data licensed under <a rel="noreferrer" href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA</a>.
    </span>
  </div>

  <div class="footer-links">
      <a href="/site/terms">Terms</a>
      <a href="/site/data-policy">Data policy</a>
      <a href="/site/cookies-policy">Cookies policy</a>
      <a href="/about#team" class="fair-logo"> from
          <img src="data:image/png;base64,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">
      </a>
  </div>
</div>



<script>
  // MathJax
  window.MathJax = {
    tex: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"],
      ],
    },
  };

  const mathjaxScript = document.createElement("script");
  mathjaxScript.src = "https://production-assets.paperswithcode.com/static/js/mathjax/tex-chtml.js";
  document.head.appendChild(mathjaxScript);
</script>


<script src="https://production-assets.paperswithcode.com/perf/766.4af6b88b.js" defer></script><script src="https://production-assets.paperswithcode.com/perf/2.6da00df7.js" defer></script><script src="https://production-assets.paperswithcode.com/perf/351.a22a9607.js" defer></script><script src="https://production-assets.paperswithcode.com/perf/452.d3ecdfa4.js" defer></script><script src="https://production-assets.paperswithcode.com/perf/553.357efc0e.js" defer></script><script src="https://production-assets.paperswithcode.com/perf/sota.table.040f2c99.js" defer></script>



</body>
</html>


